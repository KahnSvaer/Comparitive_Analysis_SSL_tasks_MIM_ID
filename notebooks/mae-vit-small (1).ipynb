{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0c5789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:45:54.722926Z",
     "iopub.status.busy": "2025-05-27T19:45:54.722716Z",
     "iopub.status.idle": "2025-05-27T19:45:54.726178Z",
     "shell.execute_reply": "2025-05-27T19:45:54.725701Z"
    },
    "papermill": {
     "duration": 0.008408,
     "end_time": "2025-05-27T19:45:54.727261",
     "exception": false,
     "start_time": "2025-05-27T19:45:54.718853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing MAE model over the dataset\n",
    "\n",
    "# Reference for code : https://github.com/facebookresearch/mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec347de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:45:54.733416Z",
     "iopub.status.busy": "2025-05-27T19:45:54.733189Z",
     "iopub.status.idle": "2025-05-27T19:45:59.927869Z",
     "shell.execute_reply": "2025-05-27T19:45:59.927123Z"
    },
    "papermill": {
     "duration": 5.199164,
     "end_time": "2025-05-27T19:45:59.929309",
     "exception": false,
     "start_time": "2025-05-27T19:45:54.730145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/': Device or resource busy\r\n"
     ]
    }
   ],
   "source": [
    "# Code for clearing memory\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "!rm -rf /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b06e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:45:59.936488Z",
     "iopub.status.busy": "2025-05-27T19:45:59.935973Z",
     "iopub.status.idle": "2025-05-27T19:45:59.966142Z",
     "shell.execute_reply": "2025-05-27T19:45:59.965425Z"
    },
    "papermill": {
     "duration": 0.035243,
     "end_time": "2025-05-27T19:45:59.967257",
     "exception": false,
     "start_time": "2025-05-27T19:45:59.932014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util/misc.py\n",
    "\n",
    "import builtins\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        setup_for_distributed(is_master=True)  # hack\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == float('inf'):\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def save_model(args, epoch, model, optimizer, loss_scaler):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    if loss_scaler is not None:\n",
    "        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            to_save = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'scaler': loss_scaler.state_dict(),\n",
    "                'args': args,\n",
    "            }\n",
    "\n",
    "            save_on_master(to_save, checkpoint_path)\n",
    "    else:\n",
    "        client_state = {'epoch': epoch}\n",
    "        model.save_checkpoint(save_dir=args.output_dir, tag=\"checkpoint-%s\" % epoch_name, client_state=client_state)\n",
    "\n",
    "\n",
    "def load_model(args, model, optimizer, loss_scaler):\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint and not (hasattr(args, 'eval') and args.eval):\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            args.start_epoch = checkpoint['epoch'] + 1\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "\n",
    "def all_reduce_mean(x):\n",
    "    world_size = get_world_size()\n",
    "    if world_size > 1:\n",
    "        x_reduce = torch.tensor(x).cuda()\n",
    "        dist.all_reduce(x_reduce)\n",
    "        x_reduce /= world_size\n",
    "        return x_reduce.item()\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb00eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:45:59.973008Z",
     "iopub.status.busy": "2025-05-27T19:45:59.972762Z",
     "iopub.status.idle": "2025-05-27T19:45:59.976893Z",
     "shell.execute_reply": "2025-05-27T19:45:59.976393Z"
    },
    "papermill": {
     "duration": 0.008286,
     "end_time": "2025-05-27T19:45:59.978001",
     "exception": false,
     "start_time": "2025-05-27T19:45:59.969715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util/lr_sched.py\n",
    "\n",
    "import math\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < args.warmup_epochs:\n",
    "        lr = args.lr * epoch / args.warmup_epochs \n",
    "    else:\n",
    "        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf7566b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:45:59.983831Z",
     "iopub.status.busy": "2025-05-27T19:45:59.983635Z",
     "iopub.status.idle": "2025-05-27T19:45:59.992078Z",
     "shell.execute_reply": "2025-05-27T19:45:59.991544Z"
    },
    "papermill": {
     "duration": 0.012671,
     "end_time": "2025-05-27T19:45:59.993177",
     "exception": false,
     "start_time": "2025-05-27T19:45:59.980506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util/pos_embed.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca072fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:45:59.998962Z",
     "iopub.status.busy": "2025-05-27T19:45:59.998770Z",
     "iopub.status.idle": "2025-05-27T19:46:09.283707Z",
     "shell.execute_reply": "2025-05-27T19:46:09.283140Z"
    },
    "papermill": {
     "duration": 9.289559,
     "end_time": "2025-05-27T19:46:09.285137",
     "exception": false,
     "start_time": "2025-05-27T19:45:59.995578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models_mae.py\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "# from util.pos_embed import get_2d_sincos_pos_embed\n",
    "\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss # Adding mean to balance out DataParallel\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "\n",
    "def mae_vit_small_patch16_dec512d8b(**kwargs):  # Main Test Done in this\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6,  # corrected for small model\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# set recommended archs\n",
    "\n",
    "available_models = {\n",
    "    'mae_vit_small_patch16' : mae_vit_small_patch16_dec512d8b, # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_base_patch16' : mae_vit_base_patch16_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_large_patch16' : mae_vit_large_patch16_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_huge_patch14' : mae_vit_huge_patch14_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fa8b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T19:46:09.291288Z",
     "iopub.status.busy": "2025-05-27T19:46:09.290963Z",
     "iopub.status.idle": "2025-05-27T19:46:09.299447Z",
     "shell.execute_reply": "2025-05-27T19:46:09.298923Z"
    },
    "papermill": {
     "duration": 0.01263,
     "end_time": "2025-05-27T19:46:09.300452",
     "exception": false,
     "start_time": "2025-05-27T19:46:09.287822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# engine_pretrain.py\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "\n",
    "# import util.misc as misc\n",
    "# import util.lr_sched as lr_sched\n",
    "\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler,\n",
    "                    log_writer=None,\n",
    "                    args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 20\n",
    "\n",
    "    accum_iter = args.accum_iter\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "\n",
    "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
    "\n",
    "        loss = loss.mean()\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
    "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=lr)\n",
    "\n",
    "        loss_value_reduce = all_reduce_mean(loss_value)\n",
    "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
    "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
    "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
    "\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ae7dee",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-27T19:46:09.306367Z",
     "iopub.status.busy": "2025-05-27T19:46:09.306145Z",
     "iopub.status.idle": "2025-05-28T00:44:02.630233Z",
     "shell.execute_reply": "2025-05-28T00:44:02.629341Z"
    },
    "papermill": {
     "duration": 17873.328783,
     "end_time": "2025-05-28T00:44:02.631659",
     "exception": false,
     "start_time": "2025-05-27T19:46:09.302876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 19:46:11.476550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748375171.707154      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748375171.767916      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with args:\n",
      " {\n",
      "  \"batch_size\": 256,\n",
      "  \"epochs\": 100,\n",
      "  \"accum_iter\": 1,\n",
      "  \"model\": \"mae_vit_small_patch16\",\n",
      "  \"input_size\": 224,\n",
      "  \"mask_ratio\": 0.75,\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"weight_decay\": 0.05,\n",
      "  \"lr\": null,\n",
      "  \"blr\": 0.001,\n",
      "  \"min_lr\": 0.0,\n",
      "  \"warmup_epochs\": 10,\n",
      "  \"data_path\": \"/kaggle/input/ssl-dataset/ssl_dataset/train.X1\",\n",
      "  \"output_dir\": \"/kaggle/working/checkpoints\",\n",
      "  \"log_dir\": \"/kaggle/working/logs\",\n",
      "  \"device\": \"cuda\",\n",
      "  \"seed\": 0,\n",
      "  \"resume\": \"\",\n",
      "  \"start_epoch\": 0,\n",
      "  \"num_workers\": 4,\n",
      "  \"pin_mem\": true,\n",
      "  \"world_size\": 1,\n",
      "  \"local_rank\": -1,\n",
      "  \"dist_on_itp\": false,\n",
      "  \"dist_url\": \"env://\"\n",
      "}\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 32500\n",
      "    Root location: /kaggle/input/ssl-dataset/ssl_dataset/train.X1\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Using DataParallel with 2 GPUs.\n",
      "Model = DataParallel(\n",
      "  (module): MaskedAutoencoderViT(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (decoder_embed): Linear(in_features=384, out_features=512, bias=True)\n",
      "    (decoder_blocks): ModuleList(\n",
      "      (0-7): 8 x Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      "base lr: 1.00e-03\n",
      "actual lr: 1.00e-03\n",
      "effective batch size: 256\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Start training for 100 epochs\n",
      "log_dir: /kaggle/working/logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3157160990.py:229: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_19/3798565144.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/126]  eta: 0:21:49  lr: 0.000000  loss: 2.0152 (2.0152)  time: 10.3941  data: 4.7563  max mem: 5726\n",
      "Epoch: [0]  [ 20/126]  eta: 0:03:03  lr: 0.000016  loss: 1.7293 (1.7959)  time: 1.2962  data: 0.0003  max mem: 6104\n",
      "Epoch: [0]  [ 40/126]  eta: 0:02:10  lr: 0.000032  loss: 1.3138 (1.5705)  time: 1.3042  data: 0.0004  max mem: 6104\n",
      "Epoch: [0]  [ 60/126]  eta: 0:01:36  lr: 0.000048  loss: 1.1185 (1.4245)  time: 1.3183  data: 0.0003  max mem: 6104\n",
      "Epoch: [0]  [ 80/126]  eta: 0:01:05  lr: 0.000063  loss: 0.9802 (1.3158)  time: 1.3510  data: 0.0003  max mem: 6104\n",
      "Epoch: [0]  [100/126]  eta: 0:00:36  lr: 0.000079  loss: 0.9140 (1.2370)  time: 1.3908  data: 0.0003  max mem: 6104\n",
      "Epoch: [0]  [120/126]  eta: 0:00:08  lr: 0.000095  loss: 0.8790 (1.1775)  time: 1.4124  data: 0.0004  max mem: 6104\n",
      "Epoch: [0]  [125/126]  eta: 0:00:01  lr: 0.000099  loss: 0.8813 (1.1655)  time: 1.3943  data: 0.0003  max mem: 6104\n",
      "Epoch: [0] Total time: 0:02:58 (1.4186 s / it)\n",
      "Averaged stats: lr: 0.000099  loss: 0.8813 (1.1655)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [1]  [  0/126]  eta: 0:10:29  lr: 0.000100  loss: 0.8694 (0.8694)  time: 4.9987  data: 3.4609  max mem: 6104\n",
      "Epoch: [1]  [ 20/126]  eta: 0:02:45  lr: 0.000116  loss: 0.8597 (0.8461)  time: 1.3940  data: 0.0004  max mem: 6104\n",
      "Epoch: [1]  [ 40/126]  eta: 0:02:07  lr: 0.000132  loss: 0.7369 (0.7995)  time: 1.3930  data: 0.0002  max mem: 6104\n",
      "Epoch: [1]  [ 60/126]  eta: 0:01:35  lr: 0.000148  loss: 0.6692 (0.7581)  time: 1.3806  data: 0.0002  max mem: 6104\n",
      "Epoch: [1]  [ 80/126]  eta: 0:01:06  lr: 0.000163  loss: 0.6585 (0.7380)  time: 1.4171  data: 0.0002  max mem: 6104\n",
      "Epoch: [1]  [100/126]  eta: 0:00:37  lr: 0.000179  loss: 0.5999 (0.7114)  time: 1.3869  data: 0.0002  max mem: 6104\n",
      "Epoch: [1]  [120/126]  eta: 0:00:08  lr: 0.000195  loss: 0.5659 (0.6871)  time: 1.3781  data: 0.0003  max mem: 6104\n",
      "Epoch: [1]  [125/126]  eta: 0:00:01  lr: 0.000199  loss: 0.5555 (0.6820)  time: 1.3716  data: 0.0003  max mem: 6104\n",
      "Epoch: [1] Total time: 0:02:58 (1.4196 s / it)\n",
      "Averaged stats: lr: 0.000199  loss: 0.5555 (0.6820)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [2]  [  0/126]  eta: 0:10:28  lr: 0.000200  loss: 0.5546 (0.5546)  time: 4.9869  data: 3.4251  max mem: 6104\n",
      "Epoch: [2]  [ 20/126]  eta: 0:02:46  lr: 0.000216  loss: 0.5315 (0.5385)  time: 1.4032  data: 0.0003  max mem: 6104\n",
      "Epoch: [2]  [ 40/126]  eta: 0:02:07  lr: 0.000232  loss: 0.5200 (0.5303)  time: 1.3785  data: 0.0003  max mem: 6104\n",
      "Epoch: [2]  [ 60/126]  eta: 0:01:35  lr: 0.000248  loss: 0.5170 (0.5278)  time: 1.3715  data: 0.0003  max mem: 6104\n",
      "Epoch: [2]  [ 80/126]  eta: 0:01:05  lr: 0.000263  loss: 0.5091 (0.5235)  time: 1.3731  data: 0.0003  max mem: 6104\n",
      "Epoch: [2]  [100/126]  eta: 0:00:36  lr: 0.000279  loss: 0.4913 (0.5178)  time: 1.3760  data: 0.0003  max mem: 6104\n",
      "Epoch: [2]  [120/126]  eta: 0:00:08  lr: 0.000295  loss: 0.4766 (0.5115)  time: 1.3697  data: 0.0003  max mem: 6104\n",
      "Epoch: [2]  [125/126]  eta: 0:00:01  lr: 0.000299  loss: 0.4767 (0.5103)  time: 1.3590  data: 0.0003  max mem: 6104\n",
      "Epoch: [2] Total time: 0:02:57 (1.4066 s / it)\n",
      "Averaged stats: lr: 0.000299  loss: 0.4767 (0.5103)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [3]  [  0/126]  eta: 0:10:24  lr: 0.000300  loss: 0.4805 (0.4805)  time: 4.9542  data: 3.3610  max mem: 6104\n",
      "Epoch: [3]  [ 20/126]  eta: 0:02:49  lr: 0.000316  loss: 0.4724 (0.4781)  time: 1.4294  data: 0.0004  max mem: 6104\n",
      "Epoch: [3]  [ 40/126]  eta: 0:02:08  lr: 0.000332  loss: 0.4630 (0.4718)  time: 1.3792  data: 0.0003  max mem: 6104\n",
      "Epoch: [3]  [ 60/126]  eta: 0:01:35  lr: 0.000348  loss: 0.4662 (0.4713)  time: 1.3675  data: 0.0003  max mem: 6104\n",
      "Epoch: [3]  [ 80/126]  eta: 0:01:05  lr: 0.000363  loss: 0.4498 (0.4661)  time: 1.3737  data: 0.0003  max mem: 6104\n",
      "Epoch: [3]  [100/126]  eta: 0:00:36  lr: 0.000379  loss: 0.4492 (0.4647)  time: 1.3721  data: 0.0002  max mem: 6104\n",
      "Epoch: [3]  [120/126]  eta: 0:00:08  lr: 0.000395  loss: 0.4412 (0.4616)  time: 1.3711  data: 0.0004  max mem: 6104\n",
      "Epoch: [3]  [125/126]  eta: 0:00:01  lr: 0.000399  loss: 0.4390 (0.4603)  time: 1.3630  data: 0.0003  max mem: 6104\n",
      "Epoch: [3] Total time: 0:02:57 (1.4099 s / it)\n",
      "Averaged stats: lr: 0.000399  loss: 0.4390 (0.4603)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [4]  [  0/126]  eta: 0:09:39  lr: 0.000400  loss: 0.4292 (0.4292)  time: 4.5954  data: 3.0118  max mem: 6104\n",
      "Epoch: [4]  [ 20/126]  eta: 0:02:44  lr: 0.000416  loss: 0.4347 (0.4394)  time: 1.4007  data: 0.0004  max mem: 6104\n",
      "Epoch: [4]  [ 40/126]  eta: 0:02:06  lr: 0.000432  loss: 0.4354 (0.4368)  time: 1.3794  data: 0.0002  max mem: 6104\n",
      "Epoch: [4]  [ 60/126]  eta: 0:01:34  lr: 0.000448  loss: 0.4310 (0.4357)  time: 1.3657  data: 0.0003  max mem: 6104\n",
      "Epoch: [4]  [ 80/126]  eta: 0:01:05  lr: 0.000463  loss: 0.4279 (0.4349)  time: 1.3846  data: 0.0002  max mem: 6104\n",
      "Epoch: [4]  [100/126]  eta: 0:00:36  lr: 0.000479  loss: 0.4227 (0.4328)  time: 1.3755  data: 0.0003  max mem: 6104\n",
      "Epoch: [4]  [120/126]  eta: 0:00:08  lr: 0.000495  loss: 0.4138 (0.4298)  time: 1.3680  data: 0.0004  max mem: 6104\n",
      "Epoch: [4]  [125/126]  eta: 0:00:01  lr: 0.000499  loss: 0.4096 (0.4287)  time: 1.3560  data: 0.0004  max mem: 6104\n",
      "Epoch: [4] Total time: 0:02:56 (1.4046 s / it)\n",
      "Averaged stats: lr: 0.000499  loss: 0.4096 (0.4287)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [5]  [  0/126]  eta: 0:10:25  lr: 0.000500  loss: 0.3969 (0.3969)  time: 4.9639  data: 3.3702  max mem: 6104\n",
      "Epoch: [5]  [ 20/126]  eta: 0:02:47  lr: 0.000516  loss: 0.4194 (0.4188)  time: 1.4070  data: 0.0005  max mem: 6104\n",
      "Epoch: [5]  [ 40/126]  eta: 0:02:07  lr: 0.000532  loss: 0.4104 (0.4162)  time: 1.3745  data: 0.0003  max mem: 6104\n",
      "Epoch: [5]  [ 60/126]  eta: 0:01:35  lr: 0.000548  loss: 0.4200 (0.4168)  time: 1.3729  data: 0.0003  max mem: 6104\n",
      "Epoch: [5]  [ 80/126]  eta: 0:01:05  lr: 0.000563  loss: 0.4082 (0.4147)  time: 1.3806  data: 0.0003  max mem: 6104\n",
      "Epoch: [5]  [100/126]  eta: 0:00:36  lr: 0.000579  loss: 0.4149 (0.4149)  time: 1.3732  data: 0.0002  max mem: 6104\n",
      "Epoch: [5]  [120/126]  eta: 0:00:08  lr: 0.000595  loss: 0.4051 (0.4137)  time: 1.4027  data: 0.0004  max mem: 6104\n",
      "Epoch: [5]  [125/126]  eta: 0:00:01  lr: 0.000599  loss: 0.4088 (0.4138)  time: 1.3962  data: 0.0004  max mem: 6104\n",
      "Epoch: [5] Total time: 0:02:58 (1.4130 s / it)\n",
      "Averaged stats: lr: 0.000599  loss: 0.4088 (0.4138)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [6]  [  0/126]  eta: 0:10:18  lr: 0.000600  loss: 0.4074 (0.4074)  time: 4.9082  data: 3.3098  max mem: 6104\n",
      "Epoch: [6]  [ 20/126]  eta: 0:02:46  lr: 0.000616  loss: 0.4059 (0.4025)  time: 1.4028  data: 0.0004  max mem: 6104\n",
      "Epoch: [6]  [ 40/126]  eta: 0:02:06  lr: 0.000632  loss: 0.4137 (0.4054)  time: 1.3743  data: 0.0002  max mem: 6104\n",
      "Epoch: [6]  [ 60/126]  eta: 0:01:35  lr: 0.000648  loss: 0.3925 (0.4022)  time: 1.3762  data: 0.0002  max mem: 6104\n",
      "Epoch: [6]  [ 80/126]  eta: 0:01:05  lr: 0.000663  loss: 0.4022 (0.4030)  time: 1.3763  data: 0.0002  max mem: 6104\n",
      "Epoch: [6]  [100/126]  eta: 0:00:36  lr: 0.000679  loss: 0.4006 (0.4038)  time: 1.3754  data: 0.0003  max mem: 6104\n",
      "Epoch: [6]  [120/126]  eta: 0:00:08  lr: 0.000695  loss: 0.3936 (0.4020)  time: 1.3705  data: 0.0004  max mem: 6104\n",
      "Epoch: [6]  [125/126]  eta: 0:00:01  lr: 0.000699  loss: 0.3911 (0.4016)  time: 1.3641  data: 0.0003  max mem: 6104\n",
      "Epoch: [6] Total time: 0:02:57 (1.4070 s / it)\n",
      "Averaged stats: lr: 0.000699  loss: 0.3911 (0.4016)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [7]  [  0/126]  eta: 0:10:45  lr: 0.000700  loss: 0.3655 (0.3655)  time: 5.1220  data: 3.5389  max mem: 6104\n",
      "Epoch: [7]  [ 20/126]  eta: 0:02:48  lr: 0.000716  loss: 0.3979 (0.3917)  time: 1.4102  data: 0.0003  max mem: 6104\n",
      "Epoch: [7]  [ 40/126]  eta: 0:02:07  lr: 0.000732  loss: 0.3849 (0.3893)  time: 1.3704  data: 0.0003  max mem: 6104\n",
      "Epoch: [7]  [ 60/126]  eta: 0:01:36  lr: 0.000748  loss: 0.3910 (0.3896)  time: 1.4066  data: 0.0003  max mem: 6104\n",
      "Epoch: [7]  [ 80/126]  eta: 0:01:06  lr: 0.000763  loss: 0.3860 (0.3891)  time: 1.3826  data: 0.0003  max mem: 6104\n",
      "Epoch: [7]  [100/126]  eta: 0:00:37  lr: 0.000779  loss: 0.3942 (0.3892)  time: 1.3729  data: 0.0002  max mem: 6104\n",
      "Epoch: [7]  [120/126]  eta: 0:00:08  lr: 0.000795  loss: 0.3746 (0.3873)  time: 1.3685  data: 0.0004  max mem: 6104\n",
      "Epoch: [7]  [125/126]  eta: 0:00:01  lr: 0.000799  loss: 0.3764 (0.3868)  time: 1.3607  data: 0.0004  max mem: 6104\n",
      "Epoch: [7] Total time: 0:02:58 (1.4141 s / it)\n",
      "Averaged stats: lr: 0.000799  loss: 0.3764 (0.3868)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [8]  [  0/126]  eta: 0:10:23  lr: 0.000800  loss: 0.3593 (0.3593)  time: 4.9459  data: 3.4107  max mem: 6104\n",
      "Epoch: [8]  [ 20/126]  eta: 0:02:47  lr: 0.000816  loss: 0.3774 (0.3785)  time: 1.4088  data: 0.0004  max mem: 6104\n",
      "Epoch: [8]  [ 40/126]  eta: 0:02:07  lr: 0.000832  loss: 0.3745 (0.3785)  time: 1.3816  data: 0.0003  max mem: 6104\n",
      "Epoch: [8]  [ 60/126]  eta: 0:01:35  lr: 0.000848  loss: 0.3809 (0.3797)  time: 1.3702  data: 0.0003  max mem: 6104\n",
      "Epoch: [8]  [ 80/126]  eta: 0:01:05  lr: 0.000863  loss: 0.3760 (0.3784)  time: 1.3746  data: 0.0003  max mem: 6104\n",
      "Epoch: [8]  [100/126]  eta: 0:00:36  lr: 0.000879  loss: 0.3776 (0.3781)  time: 1.3886  data: 0.0003  max mem: 6104\n",
      "Epoch: [8]  [120/126]  eta: 0:00:08  lr: 0.000895  loss: 0.3736 (0.3778)  time: 1.3745  data: 0.0004  max mem: 6104\n",
      "Epoch: [8]  [125/126]  eta: 0:00:01  lr: 0.000899  loss: 0.3749 (0.3779)  time: 1.3663  data: 0.0004  max mem: 6104\n",
      "Epoch: [8] Total time: 0:02:57 (1.4111 s / it)\n",
      "Averaged stats: lr: 0.000899  loss: 0.3749 (0.3779)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [9]  [  0/126]  eta: 0:10:07  lr: 0.000900  loss: 0.3761 (0.3761)  time: 4.8231  data: 3.1591  max mem: 6104\n",
      "Epoch: [9]  [ 20/126]  eta: 0:02:46  lr: 0.000916  loss: 0.3722 (0.3733)  time: 1.4049  data: 0.0005  max mem: 6104\n",
      "Epoch: [9]  [ 40/126]  eta: 0:02:06  lr: 0.000932  loss: 0.3690 (0.3731)  time: 1.3685  data: 0.0002  max mem: 6104\n",
      "Epoch: [9]  [ 60/126]  eta: 0:01:34  lr: 0.000948  loss: 0.3710 (0.3727)  time: 1.3704  data: 0.0003  max mem: 6104\n",
      "Epoch: [9]  [ 80/126]  eta: 0:01:05  lr: 0.000963  loss: 0.3727 (0.3736)  time: 1.3741  data: 0.0003  max mem: 6104\n",
      "Epoch: [9]  [100/126]  eta: 0:00:36  lr: 0.000979  loss: 0.3638 (0.3716)  time: 1.3824  data: 0.0004  max mem: 6104\n",
      "Epoch: [9]  [120/126]  eta: 0:00:08  lr: 0.000995  loss: 0.3650 (0.3702)  time: 1.3754  data: 0.0005  max mem: 6104\n",
      "Epoch: [9]  [125/126]  eta: 0:00:01  lr: 0.000999  loss: 0.3575 (0.3694)  time: 1.3656  data: 0.0004  max mem: 6104\n",
      "Epoch: [9] Total time: 0:02:57 (1.4060 s / it)\n",
      "Averaged stats: lr: 0.000999  loss: 0.3575 (0.3694)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [10]  [  0/126]  eta: 0:09:38  lr: 0.001000  loss: 0.3661 (0.3661)  time: 4.5887  data: 3.0200  max mem: 6104\n",
      "Epoch: [10]  [ 20/126]  eta: 0:02:49  lr: 0.001000  loss: 0.3631 (0.3666)  time: 1.4479  data: 0.0004  max mem: 6104\n",
      "Epoch: [10]  [ 40/126]  eta: 0:02:08  lr: 0.001000  loss: 0.3608 (0.3648)  time: 1.3807  data: 0.0003  max mem: 6104\n",
      "Epoch: [10]  [ 60/126]  eta: 0:01:35  lr: 0.001000  loss: 0.3616 (0.3637)  time: 1.3675  data: 0.0003  max mem: 6104\n",
      "Epoch: [10]  [ 80/126]  eta: 0:01:05  lr: 0.001000  loss: 0.3521 (0.3613)  time: 1.3769  data: 0.0003  max mem: 6104\n",
      "Epoch: [10]  [100/126]  eta: 0:00:36  lr: 0.001000  loss: 0.3599 (0.3610)  time: 1.3679  data: 0.0002  max mem: 6104\n",
      "Epoch: [10]  [120/126]  eta: 0:00:08  lr: 0.001000  loss: 0.3528 (0.3601)  time: 1.3670  data: 0.0003  max mem: 6104\n",
      "Epoch: [10]  [125/126]  eta: 0:00:01  lr: 0.001000  loss: 0.3592 (0.3606)  time: 1.3591  data: 0.0003  max mem: 6104\n",
      "Epoch: [10] Total time: 0:02:57 (1.4097 s / it)\n",
      "Averaged stats: lr: 0.001000  loss: 0.3592 (0.3606)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [11]  [  0/126]  eta: 0:10:07  lr: 0.001000  loss: 0.3355 (0.3355)  time: 4.8199  data: 3.1775  max mem: 6104\n",
      "Epoch: [11]  [ 20/126]  eta: 0:02:46  lr: 0.001000  loss: 0.3517 (0.3545)  time: 1.4066  data: 0.0004  max mem: 6104\n",
      "Epoch: [11]  [ 40/126]  eta: 0:02:06  lr: 0.000999  loss: 0.3543 (0.3556)  time: 1.3751  data: 0.0003  max mem: 6104\n",
      "Epoch: [11]  [ 60/126]  eta: 0:01:36  lr: 0.000999  loss: 0.3543 (0.3560)  time: 1.4210  data: 0.0003  max mem: 6104\n",
      "Epoch: [11]  [ 80/126]  eta: 0:01:06  lr: 0.000999  loss: 0.3520 (0.3543)  time: 1.3783  data: 0.0002  max mem: 6104\n",
      "Epoch: [11]  [100/126]  eta: 0:00:37  lr: 0.000999  loss: 0.3541 (0.3543)  time: 1.3678  data: 0.0002  max mem: 6104\n",
      "Epoch: [11]  [120/126]  eta: 0:00:08  lr: 0.000999  loss: 0.3537 (0.3543)  time: 1.3621  data: 0.0003  max mem: 6104\n",
      "Epoch: [11]  [125/126]  eta: 0:00:01  lr: 0.000999  loss: 0.3557 (0.3544)  time: 1.3543  data: 0.0003  max mem: 6104\n",
      "Epoch: [11] Total time: 0:02:57 (1.4114 s / it)\n",
      "Averaged stats: lr: 0.000999  loss: 0.3557 (0.3544)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [12]  [  0/126]  eta: 0:09:40  lr: 0.000999  loss: 0.3539 (0.3539)  time: 4.6088  data: 3.0090  max mem: 6104\n",
      "Epoch: [12]  [ 20/126]  eta: 0:02:45  lr: 0.000999  loss: 0.3446 (0.3477)  time: 1.4065  data: 0.0004  max mem: 6104\n",
      "Epoch: [12]  [ 40/126]  eta: 0:02:06  lr: 0.000998  loss: 0.3539 (0.3493)  time: 1.3761  data: 0.0003  max mem: 6104\n",
      "Epoch: [12]  [ 60/126]  eta: 0:01:34  lr: 0.000998  loss: 0.3553 (0.3502)  time: 1.3677  data: 0.0002  max mem: 6104\n",
      "Epoch: [12]  [ 80/126]  eta: 0:01:05  lr: 0.000998  loss: 0.3472 (0.3502)  time: 1.3699  data: 0.0002  max mem: 6104\n",
      "Epoch: [12]  [100/126]  eta: 0:00:36  lr: 0.000998  loss: 0.3433 (0.3496)  time: 1.3706  data: 0.0003  max mem: 6104\n",
      "Epoch: [12]  [120/126]  eta: 0:00:08  lr: 0.000997  loss: 0.3445 (0.3491)  time: 1.4136  data: 0.0004  max mem: 6104\n",
      "Epoch: [12]  [125/126]  eta: 0:00:01  lr: 0.000997  loss: 0.3427 (0.3489)  time: 1.4011  data: 0.0003  max mem: 6104\n",
      "Epoch: [12] Total time: 0:02:57 (1.4091 s / it)\n",
      "Averaged stats: lr: 0.000997  loss: 0.3427 (0.3489)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [13]  [  0/126]  eta: 0:10:12  lr: 0.000997  loss: 0.3151 (0.3151)  time: 4.8611  data: 3.2562  max mem: 6104\n",
      "Epoch: [13]  [ 20/126]  eta: 0:02:46  lr: 0.000997  loss: 0.3413 (0.3457)  time: 1.4092  data: 0.0004  max mem: 6104\n",
      "Epoch: [13]  [ 40/126]  eta: 0:02:06  lr: 0.000997  loss: 0.3411 (0.3450)  time: 1.3690  data: 0.0003  max mem: 6104\n",
      "Epoch: [13]  [ 60/126]  eta: 0:01:34  lr: 0.000996  loss: 0.3378 (0.3435)  time: 1.3666  data: 0.0003  max mem: 6104\n",
      "Epoch: [13]  [ 80/126]  eta: 0:01:05  lr: 0.000996  loss: 0.3467 (0.3435)  time: 1.3773  data: 0.0003  max mem: 6104\n",
      "Epoch: [13]  [100/126]  eta: 0:00:36  lr: 0.000996  loss: 0.3491 (0.3448)  time: 1.3688  data: 0.0003  max mem: 6104\n",
      "Epoch: [13]  [120/126]  eta: 0:00:08  lr: 0.000995  loss: 0.3412 (0.3443)  time: 1.3651  data: 0.0004  max mem: 6104\n",
      "Epoch: [13]  [125/126]  eta: 0:00:01  lr: 0.000995  loss: 0.3473 (0.3444)  time: 1.3556  data: 0.0003  max mem: 6104\n",
      "Epoch: [13] Total time: 0:02:56 (1.4029 s / it)\n",
      "Averaged stats: lr: 0.000995  loss: 0.3473 (0.3444)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [14]  [  0/126]  eta: 0:10:30  lr: 0.000995  loss: 0.3708 (0.3708)  time: 5.0007  data: 3.4539  max mem: 6104\n",
      "Epoch: [14]  [ 20/126]  eta: 0:02:47  lr: 0.000995  loss: 0.3409 (0.3443)  time: 1.4093  data: 0.0003  max mem: 6104\n",
      "Epoch: [14]  [ 40/126]  eta: 0:02:07  lr: 0.000994  loss: 0.3341 (0.3414)  time: 1.3711  data: 0.0002  max mem: 6104\n",
      "Epoch: [14]  [ 60/126]  eta: 0:01:35  lr: 0.000994  loss: 0.3435 (0.3429)  time: 1.3894  data: 0.0003  max mem: 6104\n",
      "Epoch: [14]  [ 80/126]  eta: 0:01:05  lr: 0.000993  loss: 0.3384 (0.3421)  time: 1.3777  data: 0.0003  max mem: 6104\n",
      "Epoch: [14]  [100/126]  eta: 0:00:36  lr: 0.000993  loss: 0.3348 (0.3410)  time: 1.3696  data: 0.0002  max mem: 6104\n",
      "Epoch: [14]  [120/126]  eta: 0:00:08  lr: 0.000993  loss: 0.3343 (0.3398)  time: 1.3666  data: 0.0004  max mem: 6104\n",
      "Epoch: [14]  [125/126]  eta: 0:00:01  lr: 0.000992  loss: 0.3365 (0.3398)  time: 1.3577  data: 0.0003  max mem: 6104\n",
      "Epoch: [14] Total time: 0:02:57 (1.4092 s / it)\n",
      "Averaged stats: lr: 0.000992  loss: 0.3365 (0.3398)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [15]  [  0/126]  eta: 0:10:55  lr: 0.000992  loss: 0.3367 (0.3367)  time: 5.2009  data: 3.6801  max mem: 6104\n",
      "Epoch: [15]  [ 20/126]  eta: 0:02:48  lr: 0.000992  loss: 0.3397 (0.3366)  time: 1.4045  data: 0.0005  max mem: 6104\n",
      "Epoch: [15]  [ 40/126]  eta: 0:02:07  lr: 0.000991  loss: 0.3362 (0.3352)  time: 1.3701  data: 0.0003  max mem: 6104\n",
      "Epoch: [15]  [ 60/126]  eta: 0:01:35  lr: 0.000991  loss: 0.3411 (0.3374)  time: 1.3706  data: 0.0003  max mem: 6104\n",
      "Epoch: [15]  [ 80/126]  eta: 0:01:05  lr: 0.000990  loss: 0.3384 (0.3377)  time: 1.3703  data: 0.0002  max mem: 6104\n",
      "Epoch: [15]  [100/126]  eta: 0:00:36  lr: 0.000990  loss: 0.3328 (0.3375)  time: 1.3967  data: 0.0003  max mem: 6104\n",
      "Epoch: [15]  [120/126]  eta: 0:00:08  lr: 0.000989  loss: 0.3334 (0.3371)  time: 1.3709  data: 0.0004  max mem: 6104\n",
      "Epoch: [15]  [125/126]  eta: 0:00:01  lr: 0.000989  loss: 0.3346 (0.3374)  time: 1.3660  data: 0.0004  max mem: 6104\n",
      "Epoch: [15] Total time: 0:02:57 (1.4105 s / it)\n",
      "Averaged stats: lr: 0.000989  loss: 0.3346 (0.3374)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [16]  [  0/126]  eta: 0:10:02  lr: 0.000989  loss: 0.3588 (0.3588)  time: 4.7827  data: 3.1628  max mem: 6104\n",
      "Epoch: [16]  [ 20/126]  eta: 0:02:46  lr: 0.000988  loss: 0.3331 (0.3370)  time: 1.4073  data: 0.0004  max mem: 6104\n",
      "Epoch: [16]  [ 40/126]  eta: 0:02:06  lr: 0.000988  loss: 0.3374 (0.3384)  time: 1.3732  data: 0.0003  max mem: 6104\n",
      "Epoch: [16]  [ 60/126]  eta: 0:01:35  lr: 0.000987  loss: 0.3288 (0.3363)  time: 1.3762  data: 0.0003  max mem: 6104\n",
      "Epoch: [16]  [ 80/126]  eta: 0:01:05  lr: 0.000987  loss: 0.3364 (0.3363)  time: 1.3742  data: 0.0003  max mem: 6104\n",
      "Epoch: [16]  [100/126]  eta: 0:00:36  lr: 0.000986  loss: 0.3305 (0.3354)  time: 1.3648  data: 0.0002  max mem: 6104\n",
      "Epoch: [16]  [120/126]  eta: 0:00:08  lr: 0.000985  loss: 0.3311 (0.3349)  time: 1.3663  data: 0.0003  max mem: 6104\n",
      "Epoch: [16]  [125/126]  eta: 0:00:01  lr: 0.000985  loss: 0.3382 (0.3350)  time: 1.3566  data: 0.0003  max mem: 6104\n",
      "Epoch: [16] Total time: 0:02:56 (1.4036 s / it)\n",
      "Averaged stats: lr: 0.000985  loss: 0.3382 (0.3350)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [17]  [  0/126]  eta: 0:11:06  lr: 0.000985  loss: 0.3303 (0.3303)  time: 5.2892  data: 3.6653  max mem: 6104\n",
      "Epoch: [17]  [ 20/126]  eta: 0:02:48  lr: 0.000984  loss: 0.3340 (0.3337)  time: 1.4034  data: 0.0004  max mem: 6104\n",
      "Epoch: [17]  [ 40/126]  eta: 0:02:09  lr: 0.000984  loss: 0.3378 (0.3336)  time: 1.4193  data: 0.0002  max mem: 6104\n",
      "Epoch: [17]  [ 60/126]  eta: 0:01:36  lr: 0.000983  loss: 0.3321 (0.3336)  time: 1.3696  data: 0.0002  max mem: 6104\n",
      "Epoch: [17]  [ 80/126]  eta: 0:01:06  lr: 0.000982  loss: 0.3274 (0.3325)  time: 1.3776  data: 0.0003  max mem: 6104\n",
      "Epoch: [17]  [100/126]  eta: 0:00:37  lr: 0.000982  loss: 0.3333 (0.3319)  time: 1.3746  data: 0.0003  max mem: 6104\n",
      "Epoch: [17]  [120/126]  eta: 0:00:08  lr: 0.000981  loss: 0.3356 (0.3325)  time: 1.3755  data: 0.0003  max mem: 6104\n",
      "Epoch: [17]  [125/126]  eta: 0:00:01  lr: 0.000981  loss: 0.3380 (0.3327)  time: 1.3681  data: 0.0003  max mem: 6104\n",
      "Epoch: [17] Total time: 0:02:58 (1.4169 s / it)\n",
      "Averaged stats: lr: 0.000981  loss: 0.3380 (0.3327)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [18]  [  0/126]  eta: 0:09:45  lr: 0.000981  loss: 0.3507 (0.3507)  time: 4.6494  data: 2.9564  max mem: 6104\n",
      "Epoch: [18]  [ 20/126]  eta: 0:02:44  lr: 0.000980  loss: 0.3329 (0.3304)  time: 1.4011  data: 0.0004  max mem: 6104\n",
      "Epoch: [18]  [ 40/126]  eta: 0:02:06  lr: 0.000979  loss: 0.3318 (0.3320)  time: 1.3815  data: 0.0003  max mem: 6104\n",
      "Epoch: [18]  [ 60/126]  eta: 0:01:34  lr: 0.000978  loss: 0.3311 (0.3320)  time: 1.3683  data: 0.0002  max mem: 6104\n",
      "Epoch: [18]  [ 80/126]  eta: 0:01:05  lr: 0.000977  loss: 0.3324 (0.3320)  time: 1.4133  data: 0.0003  max mem: 6104\n",
      "Epoch: [18]  [100/126]  eta: 0:00:36  lr: 0.000977  loss: 0.3246 (0.3316)  time: 1.3727  data: 0.0003  max mem: 6104\n",
      "Epoch: [18]  [120/126]  eta: 0:00:08  lr: 0.000976  loss: 0.3302 (0.3313)  time: 1.3716  data: 0.0004  max mem: 6104\n",
      "Epoch: [18]  [125/126]  eta: 0:00:01  lr: 0.000976  loss: 0.3298 (0.3310)  time: 1.3635  data: 0.0003  max mem: 6104\n",
      "Epoch: [18] Total time: 0:02:57 (1.4105 s / it)\n",
      "Averaged stats: lr: 0.000976  loss: 0.3298 (0.3310)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [19]  [  0/126]  eta: 0:10:55  lr: 0.000976  loss: 0.3334 (0.3334)  time: 5.2057  data: 3.6249  max mem: 6104\n",
      "Epoch: [19]  [ 20/126]  eta: 0:02:47  lr: 0.000975  loss: 0.3247 (0.3281)  time: 1.3979  data: 0.0003  max mem: 6104\n",
      "Epoch: [19]  [ 40/126]  eta: 0:02:07  lr: 0.000974  loss: 0.3311 (0.3287)  time: 1.3765  data: 0.0003  max mem: 6104\n",
      "Epoch: [19]  [ 60/126]  eta: 0:01:35  lr: 0.000973  loss: 0.3311 (0.3296)  time: 1.3698  data: 0.0003  max mem: 6104\n",
      "Epoch: [19]  [ 80/126]  eta: 0:01:05  lr: 0.000972  loss: 0.3274 (0.3295)  time: 1.3724  data: 0.0002  max mem: 6104\n",
      "Epoch: [19]  [100/126]  eta: 0:00:36  lr: 0.000971  loss: 0.3258 (0.3291)  time: 1.3734  data: 0.0003  max mem: 6104\n",
      "Epoch: [19]  [120/126]  eta: 0:00:08  lr: 0.000970  loss: 0.3233 (0.3287)  time: 1.3762  data: 0.0003  max mem: 6104\n",
      "Epoch: [19]  [125/126]  eta: 0:00:01  lr: 0.000970  loss: 0.3269 (0.3285)  time: 1.3658  data: 0.0003  max mem: 6104\n",
      "Epoch: [19] Total time: 0:02:57 (1.4074 s / it)\n",
      "Averaged stats: lr: 0.000970  loss: 0.3269 (0.3285)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [20]  [  0/126]  eta: 0:11:11  lr: 0.000970  loss: 0.3303 (0.3303)  time: 5.3306  data: 3.7214  max mem: 6104\n",
      "Epoch: [20]  [ 20/126]  eta: 0:02:52  lr: 0.000969  loss: 0.3303 (0.3322)  time: 1.4412  data: 0.0003  max mem: 6104\n",
      "Epoch: [20]  [ 40/126]  eta: 0:02:09  lr: 0.000968  loss: 0.3327 (0.3322)  time: 1.3701  data: 0.0002  max mem: 6104\n",
      "Epoch: [20]  [ 60/126]  eta: 0:01:36  lr: 0.000967  loss: 0.3284 (0.3306)  time: 1.3699  data: 0.0003  max mem: 6104\n",
      "Epoch: [20]  [ 80/126]  eta: 0:01:06  lr: 0.000966  loss: 0.3251 (0.3297)  time: 1.3729  data: 0.0002  max mem: 6104\n",
      "Epoch: [20]  [100/126]  eta: 0:00:37  lr: 0.000965  loss: 0.3250 (0.3290)  time: 1.3715  data: 0.0003  max mem: 6104\n",
      "Epoch: [20]  [120/126]  eta: 0:00:08  lr: 0.000964  loss: 0.3229 (0.3284)  time: 1.3686  data: 0.0004  max mem: 6104\n",
      "Epoch: [20]  [125/126]  eta: 0:00:01  lr: 0.000964  loss: 0.3222 (0.3281)  time: 1.3591  data: 0.0004  max mem: 6104\n",
      "Epoch: [20] Total time: 0:02:58 (1.4131 s / it)\n",
      "Averaged stats: lr: 0.000964  loss: 0.3222 (0.3281)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [21]  [  0/126]  eta: 0:10:39  lr: 0.000964  loss: 0.3188 (0.3188)  time: 5.0746  data: 3.5395  max mem: 6104\n",
      "Epoch: [21]  [ 20/126]  eta: 0:02:46  lr: 0.000963  loss: 0.3290 (0.3289)  time: 1.3991  data: 0.0006  max mem: 6104\n",
      "Epoch: [21]  [ 40/126]  eta: 0:02:07  lr: 0.000961  loss: 0.3251 (0.3285)  time: 1.3800  data: 0.0002  max mem: 6104\n",
      "Epoch: [21]  [ 60/126]  eta: 0:01:35  lr: 0.000960  loss: 0.3268 (0.3273)  time: 1.3840  data: 0.0002  max mem: 6104\n",
      "Epoch: [21]  [ 80/126]  eta: 0:01:05  lr: 0.000959  loss: 0.3232 (0.3264)  time: 1.3758  data: 0.0002  max mem: 6104\n",
      "Epoch: [21]  [100/126]  eta: 0:00:36  lr: 0.000958  loss: 0.3250 (0.3268)  time: 1.3777  data: 0.0002  max mem: 6104\n",
      "Epoch: [21]  [120/126]  eta: 0:00:08  lr: 0.000957  loss: 0.3303 (0.3272)  time: 1.3650  data: 0.0003  max mem: 6104\n",
      "Epoch: [21]  [125/126]  eta: 0:00:01  lr: 0.000957  loss: 0.3330 (0.3272)  time: 1.3581  data: 0.0003  max mem: 6104\n",
      "Epoch: [21] Total time: 0:02:57 (1.4089 s / it)\n",
      "Averaged stats: lr: 0.000957  loss: 0.3330 (0.3272)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [22]  [  0/126]  eta: 0:10:16  lr: 0.000957  loss: 0.3354 (0.3354)  time: 4.8941  data: 3.3805  max mem: 6104\n",
      "Epoch: [22]  [ 20/126]  eta: 0:02:47  lr: 0.000956  loss: 0.3214 (0.3214)  time: 1.4102  data: 0.0004  max mem: 6104\n",
      "Epoch: [22]  [ 40/126]  eta: 0:02:07  lr: 0.000954  loss: 0.3219 (0.3218)  time: 1.3758  data: 0.0003  max mem: 6104\n",
      "Epoch: [22]  [ 60/126]  eta: 0:01:35  lr: 0.000953  loss: 0.3282 (0.3236)  time: 1.3634  data: 0.0003  max mem: 6104\n",
      "Epoch: [22]  [ 80/126]  eta: 0:01:05  lr: 0.000952  loss: 0.3222 (0.3237)  time: 1.3767  data: 0.0002  max mem: 6104\n",
      "Epoch: [22]  [100/126]  eta: 0:00:36  lr: 0.000951  loss: 0.3282 (0.3248)  time: 1.4126  data: 0.0002  max mem: 6104\n",
      "Epoch: [22]  [120/126]  eta: 0:00:08  lr: 0.000950  loss: 0.3224 (0.3246)  time: 1.3684  data: 0.0005  max mem: 6104\n",
      "Epoch: [22]  [125/126]  eta: 0:00:01  lr: 0.000949  loss: 0.3224 (0.3245)  time: 1.3553  data: 0.0004  max mem: 6104\n",
      "Epoch: [22] Total time: 0:02:57 (1.4115 s / it)\n",
      "Averaged stats: lr: 0.000949  loss: 0.3224 (0.3245)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [23]  [  0/126]  eta: 0:11:21  lr: 0.000949  loss: 0.3188 (0.3188)  time: 5.4053  data: 3.7743  max mem: 6104\n",
      "Epoch: [23]  [ 20/126]  eta: 0:02:48  lr: 0.000948  loss: 0.3268 (0.3278)  time: 1.4020  data: 0.0004  max mem: 6104\n",
      "Epoch: [23]  [ 40/126]  eta: 0:02:07  lr: 0.000947  loss: 0.3163 (0.3237)  time: 1.3748  data: 0.0003  max mem: 6104\n",
      "Epoch: [23]  [ 60/126]  eta: 0:01:35  lr: 0.000946  loss: 0.3238 (0.3230)  time: 1.3640  data: 0.0003  max mem: 6104\n",
      "Epoch: [23]  [ 80/126]  eta: 0:01:05  lr: 0.000944  loss: 0.3200 (0.3224)  time: 1.3745  data: 0.0003  max mem: 6104\n",
      "Epoch: [23]  [100/126]  eta: 0:00:36  lr: 0.000943  loss: 0.3191 (0.3226)  time: 1.3777  data: 0.0003  max mem: 6104\n",
      "Epoch: [23]  [120/126]  eta: 0:00:08  lr: 0.000942  loss: 0.3206 (0.3228)  time: 1.3692  data: 0.0004  max mem: 6104\n",
      "Epoch: [23]  [125/126]  eta: 0:00:01  lr: 0.000942  loss: 0.3206 (0.3229)  time: 1.3640  data: 0.0004  max mem: 6104\n",
      "Epoch: [23] Total time: 0:02:57 (1.4085 s / it)\n",
      "Averaged stats: lr: 0.000942  loss: 0.3206 (0.3229)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [24]  [  0/126]  eta: 0:09:48  lr: 0.000941  loss: 0.3140 (0.3140)  time: 4.6711  data: 3.0672  max mem: 6104\n",
      "Epoch: [24]  [ 20/126]  eta: 0:02:49  lr: 0.000940  loss: 0.3189 (0.3216)  time: 1.4415  data: 0.0004  max mem: 6104\n",
      "Epoch: [24]  [ 40/126]  eta: 0:02:07  lr: 0.000939  loss: 0.3195 (0.3197)  time: 1.3758  data: 0.0003  max mem: 6104\n",
      "Epoch: [24]  [ 60/126]  eta: 0:01:35  lr: 0.000938  loss: 0.3218 (0.3207)  time: 1.3693  data: 0.0003  max mem: 6104\n",
      "Epoch: [24]  [ 80/126]  eta: 0:01:05  lr: 0.000936  loss: 0.3243 (0.3217)  time: 1.3684  data: 0.0002  max mem: 6104\n",
      "Epoch: [24]  [100/126]  eta: 0:00:36  lr: 0.000935  loss: 0.3219 (0.3217)  time: 1.3783  data: 0.0003  max mem: 6104\n",
      "Epoch: [24]  [120/126]  eta: 0:00:08  lr: 0.000933  loss: 0.3236 (0.3219)  time: 1.3704  data: 0.0004  max mem: 6104\n",
      "Epoch: [24]  [125/126]  eta: 0:00:01  lr: 0.000933  loss: 0.3236 (0.3222)  time: 1.3600  data: 0.0003  max mem: 6104\n",
      "Epoch: [24] Total time: 0:02:57 (1.4093 s / it)\n",
      "Averaged stats: lr: 0.000933  loss: 0.3236 (0.3222)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [25]  [  0/126]  eta: 0:10:32  lr: 0.000933  loss: 0.3072 (0.3072)  time: 5.0169  data: 3.4305  max mem: 6104\n",
      "Epoch: [25]  [ 20/126]  eta: 0:02:46  lr: 0.000932  loss: 0.3257 (0.3229)  time: 1.3999  data: 0.0003  max mem: 6104\n",
      "Epoch: [25]  [ 40/126]  eta: 0:02:08  lr: 0.000930  loss: 0.3224 (0.3221)  time: 1.4144  data: 0.0003  max mem: 6104\n",
      "Epoch: [25]  [ 60/126]  eta: 0:01:35  lr: 0.000929  loss: 0.3160 (0.3208)  time: 1.3694  data: 0.0003  max mem: 6104\n",
      "Epoch: [25]  [ 80/126]  eta: 0:01:06  lr: 0.000927  loss: 0.3229 (0.3219)  time: 1.3778  data: 0.0003  max mem: 6104\n",
      "Epoch: [25]  [100/126]  eta: 0:00:36  lr: 0.000926  loss: 0.3169 (0.3213)  time: 1.3728  data: 0.0003  max mem: 6104\n",
      "Epoch: [25]  [120/126]  eta: 0:00:08  lr: 0.000924  loss: 0.3205 (0.3213)  time: 1.3739  data: 0.0003  max mem: 6104\n",
      "Epoch: [25]  [125/126]  eta: 0:00:01  lr: 0.000924  loss: 0.3203 (0.3212)  time: 1.3644  data: 0.0003  max mem: 6104\n",
      "Epoch: [25] Total time: 0:02:58 (1.4129 s / it)\n",
      "Averaged stats: lr: 0.000924  loss: 0.3203 (0.3212)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [26]  [  0/126]  eta: 0:09:43  lr: 0.000924  loss: 0.3225 (0.3225)  time: 4.6346  data: 3.0771  max mem: 6104\n",
      "Epoch: [26]  [ 20/126]  eta: 0:02:45  lr: 0.000923  loss: 0.3184 (0.3191)  time: 1.4116  data: 0.0004  max mem: 6104\n",
      "Epoch: [26]  [ 40/126]  eta: 0:02:06  lr: 0.000921  loss: 0.3218 (0.3194)  time: 1.3725  data: 0.0002  max mem: 6104\n",
      "Epoch: [26]  [ 60/126]  eta: 0:01:35  lr: 0.000920  loss: 0.3165 (0.3181)  time: 1.3751  data: 0.0003  max mem: 6104\n",
      "Epoch: [26]  [ 80/126]  eta: 0:01:05  lr: 0.000918  loss: 0.3243 (0.3193)  time: 1.3776  data: 0.0003  max mem: 6104\n",
      "Epoch: [26]  [100/126]  eta: 0:00:36  lr: 0.000917  loss: 0.3153 (0.3195)  time: 1.4045  data: 0.0003  max mem: 6104\n",
      "Epoch: [26]  [120/126]  eta: 0:00:08  lr: 0.000915  loss: 0.3253 (0.3204)  time: 1.3747  data: 0.0003  max mem: 6104\n",
      "Epoch: [26]  [125/126]  eta: 0:00:01  lr: 0.000915  loss: 0.3209 (0.3202)  time: 1.3628  data: 0.0003  max mem: 6104\n",
      "Epoch: [26] Total time: 0:02:57 (1.4111 s / it)\n",
      "Averaged stats: lr: 0.000915  loss: 0.3209 (0.3202)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [27]  [  0/126]  eta: 0:11:39  lr: 0.000915  loss: 0.3121 (0.3121)  time: 5.5478  data: 4.0225  max mem: 6104\n",
      "Epoch: [27]  [ 20/126]  eta: 0:02:49  lr: 0.000913  loss: 0.3145 (0.3190)  time: 1.3985  data: 0.0003  max mem: 6104\n",
      "Epoch: [27]  [ 40/126]  eta: 0:02:07  lr: 0.000911  loss: 0.3200 (0.3198)  time: 1.3681  data: 0.0002  max mem: 6104\n",
      "Epoch: [27]  [ 60/126]  eta: 0:01:35  lr: 0.000910  loss: 0.3207 (0.3199)  time: 1.3668  data: 0.0003  max mem: 6104\n",
      "Epoch: [27]  [ 80/126]  eta: 0:01:05  lr: 0.000908  loss: 0.3164 (0.3194)  time: 1.3806  data: 0.0003  max mem: 6104\n",
      "Epoch: [27]  [100/126]  eta: 0:00:36  lr: 0.000907  loss: 0.3125 (0.3186)  time: 1.3728  data: 0.0002  max mem: 6104\n",
      "Epoch: [27]  [120/126]  eta: 0:00:08  lr: 0.000905  loss: 0.3205 (0.3190)  time: 1.3745  data: 0.0004  max mem: 6104\n",
      "Epoch: [27]  [125/126]  eta: 0:00:01  lr: 0.000905  loss: 0.3143 (0.3186)  time: 1.3672  data: 0.0003  max mem: 6104\n",
      "Epoch: [27] Total time: 0:02:57 (1.4096 s / it)\n",
      "Averaged stats: lr: 0.000905  loss: 0.3143 (0.3186)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [28]  [  0/126]  eta: 0:10:25  lr: 0.000905  loss: 0.3233 (0.3233)  time: 4.9676  data: 3.4442  max mem: 6104\n",
      "Epoch: [28]  [ 20/126]  eta: 0:02:45  lr: 0.000903  loss: 0.3159 (0.3174)  time: 1.3938  data: 0.0003  max mem: 6104\n",
      "Epoch: [28]  [ 40/126]  eta: 0:02:08  lr: 0.000901  loss: 0.3182 (0.3171)  time: 1.4128  data: 0.0003  max mem: 6104\n",
      "Epoch: [28]  [ 60/126]  eta: 0:01:35  lr: 0.000900  loss: 0.3203 (0.3186)  time: 1.3677  data: 0.0003  max mem: 6104\n",
      "Epoch: [28]  [ 80/126]  eta: 0:01:05  lr: 0.000898  loss: 0.3197 (0.3190)  time: 1.3762  data: 0.0002  max mem: 6104\n",
      "Epoch: [28]  [100/126]  eta: 0:00:36  lr: 0.000896  loss: 0.3177 (0.3187)  time: 1.3704  data: 0.0003  max mem: 6104\n",
      "Epoch: [28]  [120/126]  eta: 0:00:08  lr: 0.000895  loss: 0.3176 (0.3188)  time: 1.3715  data: 0.0003  max mem: 6104\n",
      "Epoch: [28]  [125/126]  eta: 0:00:01  lr: 0.000894  loss: 0.3124 (0.3185)  time: 1.3638  data: 0.0003  max mem: 6104\n",
      "Epoch: [28] Total time: 0:02:57 (1.4102 s / it)\n",
      "Averaged stats: lr: 0.000894  loss: 0.3124 (0.3185)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [29]  [  0/126]  eta: 0:11:09  lr: 0.000894  loss: 0.3128 (0.3128)  time: 5.3150  data: 3.6947  max mem: 6104\n",
      "Epoch: [29]  [ 20/126]  eta: 0:02:47  lr: 0.000892  loss: 0.3198 (0.3189)  time: 1.3971  data: 0.0003  max mem: 6104\n",
      "Epoch: [29]  [ 40/126]  eta: 0:02:07  lr: 0.000891  loss: 0.3175 (0.3173)  time: 1.3704  data: 0.0002  max mem: 6104\n",
      "Epoch: [29]  [ 60/126]  eta: 0:01:35  lr: 0.000889  loss: 0.3144 (0.3170)  time: 1.3795  data: 0.0003  max mem: 6104\n",
      "Epoch: [29]  [ 80/126]  eta: 0:01:05  lr: 0.000887  loss: 0.3158 (0.3176)  time: 1.3781  data: 0.0003  max mem: 6104\n",
      "Epoch: [29]  [100/126]  eta: 0:00:37  lr: 0.000885  loss: 0.3122 (0.3170)  time: 1.4063  data: 0.0003  max mem: 6104\n",
      "Epoch: [29]  [120/126]  eta: 0:00:08  lr: 0.000884  loss: 0.3164 (0.3175)  time: 1.3760  data: 0.0004  max mem: 6104\n",
      "Epoch: [29]  [125/126]  eta: 0:00:01  lr: 0.000883  loss: 0.3164 (0.3175)  time: 1.3697  data: 0.0004  max mem: 6104\n",
      "Epoch: [29] Total time: 0:02:58 (1.4152 s / it)\n",
      "Averaged stats: lr: 0.000883  loss: 0.3164 (0.3175)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [30]  [  0/126]  eta: 0:09:54  lr: 0.000883  loss: 0.3113 (0.3113)  time: 4.7173  data: 3.1218  max mem: 6104\n",
      "Epoch: [30]  [ 20/126]  eta: 0:02:45  lr: 0.000881  loss: 0.3187 (0.3234)  time: 1.4035  data: 0.0003  max mem: 6104\n",
      "Epoch: [30]  [ 40/126]  eta: 0:02:06  lr: 0.000879  loss: 0.3096 (0.3186)  time: 1.3818  data: 0.0003  max mem: 6104\n",
      "Epoch: [30]  [ 60/126]  eta: 0:01:34  lr: 0.000878  loss: 0.3132 (0.3179)  time: 1.3626  data: 0.0002  max mem: 6104\n",
      "Epoch: [30]  [ 80/126]  eta: 0:01:05  lr: 0.000876  loss: 0.3154 (0.3175)  time: 1.3728  data: 0.0002  max mem: 6104\n",
      "Epoch: [30]  [100/126]  eta: 0:00:36  lr: 0.000874  loss: 0.3121 (0.3171)  time: 1.3695  data: 0.0003  max mem: 6104\n",
      "Epoch: [30]  [120/126]  eta: 0:00:08  lr: 0.000872  loss: 0.3148 (0.3173)  time: 1.3690  data: 0.0004  max mem: 6104\n",
      "Epoch: [30]  [125/126]  eta: 0:00:01  lr: 0.000872  loss: 0.3142 (0.3171)  time: 1.3732  data: 0.0004  max mem: 6104\n",
      "Epoch: [30] Total time: 0:02:57 (1.4050 s / it)\n",
      "Averaged stats: lr: 0.000872  loss: 0.3142 (0.3171)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [31]  [  0/126]  eta: 0:10:17  lr: 0.000872  loss: 0.3003 (0.3003)  time: 4.9034  data: 3.3923  max mem: 6104\n",
      "Epoch: [31]  [ 20/126]  eta: 0:02:45  lr: 0.000870  loss: 0.3182 (0.3162)  time: 1.3991  data: 0.0005  max mem: 6104\n",
      "Epoch: [31]  [ 40/126]  eta: 0:02:06  lr: 0.000868  loss: 0.3126 (0.3148)  time: 1.3720  data: 0.0003  max mem: 6104\n",
      "Epoch: [31]  [ 60/126]  eta: 0:01:34  lr: 0.000866  loss: 0.3205 (0.3163)  time: 1.3655  data: 0.0003  max mem: 6104\n",
      "Epoch: [31]  [ 80/126]  eta: 0:01:05  lr: 0.000864  loss: 0.3181 (0.3163)  time: 1.3760  data: 0.0003  max mem: 6104\n",
      "Epoch: [31]  [100/126]  eta: 0:00:36  lr: 0.000862  loss: 0.3176 (0.3162)  time: 1.3699  data: 0.0002  max mem: 6104\n",
      "Epoch: [31]  [120/126]  eta: 0:00:08  lr: 0.000860  loss: 0.3096 (0.3155)  time: 1.3724  data: 0.0003  max mem: 6104\n",
      "Epoch: [31]  [125/126]  eta: 0:00:01  lr: 0.000860  loss: 0.3130 (0.3154)  time: 1.3615  data: 0.0003  max mem: 6104\n",
      "Epoch: [31] Total time: 0:02:56 (1.4034 s / it)\n",
      "Averaged stats: lr: 0.000860  loss: 0.3130 (0.3154)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [32]  [  0/126]  eta: 0:10:13  lr: 0.000860  loss: 0.3117 (0.3117)  time: 4.8688  data: 3.2156  max mem: 6104\n",
      "Epoch: [32]  [ 20/126]  eta: 0:02:46  lr: 0.000858  loss: 0.3081 (0.3147)  time: 1.4090  data: 0.0004  max mem: 6104\n",
      "Epoch: [32]  [ 40/126]  eta: 0:02:07  lr: 0.000856  loss: 0.3143 (0.3165)  time: 1.3764  data: 0.0003  max mem: 6104\n",
      "Epoch: [32]  [ 60/126]  eta: 0:01:36  lr: 0.000854  loss: 0.3114 (0.3156)  time: 1.4087  data: 0.0003  max mem: 6104\n",
      "Epoch: [32]  [ 80/126]  eta: 0:01:06  lr: 0.000852  loss: 0.3144 (0.3152)  time: 1.3796  data: 0.0003  max mem: 6104\n",
      "Epoch: [32]  [100/126]  eta: 0:00:37  lr: 0.000850  loss: 0.3141 (0.3147)  time: 1.3749  data: 0.0002  max mem: 6104\n",
      "Epoch: [32]  [120/126]  eta: 0:00:08  lr: 0.000848  loss: 0.3107 (0.3146)  time: 1.3753  data: 0.0003  max mem: 6104\n",
      "Epoch: [32]  [125/126]  eta: 0:00:01  lr: 0.000847  loss: 0.3104 (0.3143)  time: 1.3713  data: 0.0003  max mem: 6104\n",
      "Epoch: [32] Total time: 0:02:58 (1.4146 s / it)\n",
      "Averaged stats: lr: 0.000847  loss: 0.3104 (0.3143)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [33]  [  0/126]  eta: 0:10:14  lr: 0.000847  loss: 0.3258 (0.3258)  time: 4.8752  data: 3.3111  max mem: 6104\n",
      "Epoch: [33]  [ 20/126]  eta: 0:02:46  lr: 0.000845  loss: 0.3154 (0.3174)  time: 1.4102  data: 0.0004  max mem: 6104\n",
      "Epoch: [33]  [ 40/126]  eta: 0:02:06  lr: 0.000843  loss: 0.3147 (0.3170)  time: 1.3709  data: 0.0003  max mem: 6104\n",
      "Epoch: [33]  [ 60/126]  eta: 0:01:35  lr: 0.000841  loss: 0.3162 (0.3165)  time: 1.3713  data: 0.0003  max mem: 6104\n",
      "Epoch: [33]  [ 80/126]  eta: 0:01:05  lr: 0.000839  loss: 0.3092 (0.3158)  time: 1.3803  data: 0.0003  max mem: 6104\n",
      "Epoch: [33]  [100/126]  eta: 0:00:36  lr: 0.000837  loss: 0.3088 (0.3146)  time: 1.3769  data: 0.0003  max mem: 6104\n",
      "Epoch: [33]  [120/126]  eta: 0:00:08  lr: 0.000835  loss: 0.3080 (0.3141)  time: 1.3988  data: 0.0003  max mem: 6104\n",
      "Epoch: [33]  [125/126]  eta: 0:00:01  lr: 0.000835  loss: 0.3078 (0.3141)  time: 1.3689  data: 0.0003  max mem: 6104\n",
      "Epoch: [33] Total time: 0:02:57 (1.4122 s / it)\n",
      "Averaged stats: lr: 0.000835  loss: 0.3078 (0.3141)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [34]  [  0/126]  eta: 0:10:40  lr: 0.000835  loss: 0.3165 (0.3165)  time: 5.0849  data: 3.3486  max mem: 6104\n",
      "Epoch: [34]  [ 20/126]  eta: 0:02:48  lr: 0.000833  loss: 0.3146 (0.3153)  time: 1.4151  data: 0.0004  max mem: 6104\n",
      "Epoch: [34]  [ 40/126]  eta: 0:02:07  lr: 0.000830  loss: 0.3136 (0.3143)  time: 1.3791  data: 0.0003  max mem: 6104\n",
      "Epoch: [34]  [ 60/126]  eta: 0:01:35  lr: 0.000828  loss: 0.3091 (0.3125)  time: 1.3722  data: 0.0003  max mem: 6104\n",
      "Epoch: [34]  [ 80/126]  eta: 0:01:05  lr: 0.000826  loss: 0.3121 (0.3124)  time: 1.3786  data: 0.0002  max mem: 6104\n",
      "Epoch: [34]  [100/126]  eta: 0:00:36  lr: 0.000824  loss: 0.3085 (0.3124)  time: 1.3815  data: 0.0002  max mem: 6104\n",
      "Epoch: [34]  [120/126]  eta: 0:00:08  lr: 0.000822  loss: 0.3147 (0.3127)  time: 1.3753  data: 0.0005  max mem: 6104\n",
      "Epoch: [34]  [125/126]  eta: 0:00:01  lr: 0.000821  loss: 0.3084 (0.3128)  time: 1.3688  data: 0.0004  max mem: 6104\n",
      "Epoch: [34] Total time: 0:02:57 (1.4126 s / it)\n",
      "Averaged stats: lr: 0.000821  loss: 0.3084 (0.3128)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [35]  [  0/126]  eta: 0:09:42  lr: 0.000821  loss: 0.3174 (0.3174)  time: 4.6261  data: 3.0310  max mem: 6104\n",
      "Epoch: [35]  [ 20/126]  eta: 0:02:48  lr: 0.000819  loss: 0.3159 (0.3130)  time: 1.4367  data: 0.0005  max mem: 6104\n",
      "Epoch: [35]  [ 40/126]  eta: 0:02:07  lr: 0.000817  loss: 0.3199 (0.3153)  time: 1.3755  data: 0.0003  max mem: 6104\n",
      "Epoch: [35]  [ 60/126]  eta: 0:01:35  lr: 0.000815  loss: 0.3078 (0.3132)  time: 1.3715  data: 0.0003  max mem: 6104\n",
      "Epoch: [35]  [ 80/126]  eta: 0:01:05  lr: 0.000813  loss: 0.3129 (0.3130)  time: 1.3762  data: 0.0002  max mem: 6104\n",
      "Epoch: [35]  [100/126]  eta: 0:00:36  lr: 0.000811  loss: 0.3139 (0.3132)  time: 1.3775  data: 0.0003  max mem: 6104\n",
      "Epoch: [35]  [120/126]  eta: 0:00:08  lr: 0.000808  loss: 0.3075 (0.3128)  time: 1.3741  data: 0.0003  max mem: 6104\n",
      "Epoch: [35]  [125/126]  eta: 0:00:01  lr: 0.000808  loss: 0.3075 (0.3127)  time: 1.3666  data: 0.0003  max mem: 6104\n",
      "Epoch: [35] Total time: 0:02:57 (1.4106 s / it)\n",
      "Averaged stats: lr: 0.000808  loss: 0.3075 (0.3127)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [36]  [  0/126]  eta: 0:10:51  lr: 0.000808  loss: 0.3180 (0.3180)  time: 5.1726  data: 3.5324  max mem: 6104\n",
      "Epoch: [36]  [ 20/126]  eta: 0:02:48  lr: 0.000806  loss: 0.3086 (0.3095)  time: 1.4099  data: 0.0004  max mem: 6104\n",
      "Epoch: [36]  [ 40/126]  eta: 0:02:07  lr: 0.000803  loss: 0.3090 (0.3105)  time: 1.3757  data: 0.0002  max mem: 6104\n",
      "Epoch: [36]  [ 60/126]  eta: 0:01:36  lr: 0.000801  loss: 0.3099 (0.3115)  time: 1.4063  data: 0.0002  max mem: 6104\n",
      "Epoch: [36]  [ 80/126]  eta: 0:01:06  lr: 0.000799  loss: 0.3043 (0.3108)  time: 1.3803  data: 0.0002  max mem: 6104\n",
      "Epoch: [36]  [100/126]  eta: 0:00:37  lr: 0.000797  loss: 0.3168 (0.3118)  time: 1.3726  data: 0.0002  max mem: 6104\n",
      "Epoch: [36]  [120/126]  eta: 0:00:08  lr: 0.000795  loss: 0.3107 (0.3119)  time: 1.3700  data: 0.0004  max mem: 6104\n",
      "Epoch: [36]  [125/126]  eta: 0:00:01  lr: 0.000794  loss: 0.3107 (0.3120)  time: 1.3644  data: 0.0004  max mem: 6104\n",
      "Epoch: [36] Total time: 0:02:58 (1.4152 s / it)\n",
      "Averaged stats: lr: 0.000794  loss: 0.3107 (0.3120)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [37]  [  0/126]  eta: 0:10:11  lr: 0.000794  loss: 0.3080 (0.3080)  time: 4.8562  data: 3.2786  max mem: 6104\n",
      "Epoch: [37]  [ 20/126]  eta: 0:02:46  lr: 0.000792  loss: 0.3117 (0.3107)  time: 1.4080  data: 0.0004  max mem: 6104\n",
      "Epoch: [37]  [ 40/126]  eta: 0:02:06  lr: 0.000789  loss: 0.3074 (0.3094)  time: 1.3759  data: 0.0003  max mem: 6104\n",
      "Epoch: [37]  [ 60/126]  eta: 0:01:35  lr: 0.000787  loss: 0.3102 (0.3104)  time: 1.3739  data: 0.0003  max mem: 6104\n",
      "Epoch: [37]  [ 80/126]  eta: 0:01:05  lr: 0.000785  loss: 0.3096 (0.3101)  time: 1.3734  data: 0.0003  max mem: 6104\n",
      "Epoch: [37]  [100/126]  eta: 0:00:36  lr: 0.000783  loss: 0.3156 (0.3111)  time: 1.3775  data: 0.0003  max mem: 6104\n",
      "Epoch: [37]  [120/126]  eta: 0:00:08  lr: 0.000780  loss: 0.3057 (0.3107)  time: 1.3713  data: 0.0003  max mem: 6104\n",
      "Epoch: [37]  [125/126]  eta: 0:00:01  lr: 0.000780  loss: 0.3083 (0.3107)  time: 1.3618  data: 0.0003  max mem: 6104\n",
      "Epoch: [37] Total time: 0:02:57 (1.4071 s / it)\n",
      "Averaged stats: lr: 0.000780  loss: 0.3083 (0.3107)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [38]  [  0/126]  eta: 0:13:42  lr: 0.000780  loss: 0.2951 (0.2951)  time: 6.5294  data: 4.1690  max mem: 6104\n",
      "Epoch: [38]  [ 20/126]  eta: 0:02:54  lr: 0.000777  loss: 0.3062 (0.3079)  time: 1.3982  data: 0.0003  max mem: 6104\n",
      "Epoch: [38]  [ 40/126]  eta: 0:02:10  lr: 0.000775  loss: 0.3066 (0.3059)  time: 1.3814  data: 0.0003  max mem: 6104\n",
      "Epoch: [38]  [ 60/126]  eta: 0:01:36  lr: 0.000773  loss: 0.3098 (0.3076)  time: 1.3701  data: 0.0003  max mem: 6104\n",
      "Epoch: [38]  [ 80/126]  eta: 0:01:06  lr: 0.000770  loss: 0.3131 (0.3088)  time: 1.3833  data: 0.0002  max mem: 6104\n",
      "Epoch: [38]  [100/126]  eta: 0:00:37  lr: 0.000768  loss: 0.3181 (0.3103)  time: 1.3770  data: 0.0003  max mem: 6104\n",
      "Epoch: [38]  [120/126]  eta: 0:00:08  lr: 0.000766  loss: 0.3059 (0.3106)  time: 1.3674  data: 0.0004  max mem: 6104\n",
      "Epoch: [38]  [125/126]  eta: 0:00:01  lr: 0.000765  loss: 0.3103 (0.3105)  time: 1.3616  data: 0.0004  max mem: 6104\n",
      "Epoch: [38] Total time: 0:02:58 (1.4202 s / it)\n",
      "Averaged stats: lr: 0.000765  loss: 0.3103 (0.3105)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [39]  [  0/126]  eta: 0:10:07  lr: 0.000765  loss: 0.3004 (0.3004)  time: 4.8253  data: 3.1387  max mem: 6104\n",
      "Epoch: [39]  [ 20/126]  eta: 0:02:46  lr: 0.000763  loss: 0.3115 (0.3108)  time: 1.4104  data: 0.0004  max mem: 6104\n",
      "Epoch: [39]  [ 40/126]  eta: 0:02:07  lr: 0.000760  loss: 0.3142 (0.3112)  time: 1.3797  data: 0.0002  max mem: 6104\n",
      "Epoch: [39]  [ 60/126]  eta: 0:01:36  lr: 0.000758  loss: 0.3074 (0.3100)  time: 1.4124  data: 0.0003  max mem: 6104\n",
      "Epoch: [39]  [ 80/126]  eta: 0:01:06  lr: 0.000755  loss: 0.3107 (0.3106)  time: 1.3746  data: 0.0002  max mem: 6104\n",
      "Epoch: [39]  [100/126]  eta: 0:00:37  lr: 0.000753  loss: 0.3102 (0.3102)  time: 1.3776  data: 0.0003  max mem: 6104\n",
      "Epoch: [39]  [120/126]  eta: 0:00:08  lr: 0.000751  loss: 0.3022 (0.3097)  time: 1.3738  data: 0.0005  max mem: 6104\n",
      "Epoch: [39]  [125/126]  eta: 0:00:01  lr: 0.000750  loss: 0.3022 (0.3096)  time: 1.3625  data: 0.0004  max mem: 6104\n",
      "Epoch: [39] Total time: 0:02:58 (1.4146 s / it)\n",
      "Averaged stats: lr: 0.000750  loss: 0.3022 (0.3096)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [40]  [  0/126]  eta: 0:10:18  lr: 0.000750  loss: 0.3212 (0.3212)  time: 4.9099  data: 3.2964  max mem: 6104\n",
      "Epoch: [40]  [ 20/126]  eta: 0:02:45  lr: 0.000748  loss: 0.3071 (0.3099)  time: 1.3947  data: 0.0005  max mem: 6104\n",
      "Epoch: [40]  [ 40/126]  eta: 0:02:06  lr: 0.000745  loss: 0.3109 (0.3103)  time: 1.3761  data: 0.0003  max mem: 6104\n",
      "Epoch: [40]  [ 60/126]  eta: 0:01:34  lr: 0.000743  loss: 0.3101 (0.3097)  time: 1.3733  data: 0.0002  max mem: 6104\n",
      "Epoch: [40]  [ 80/126]  eta: 0:01:05  lr: 0.000740  loss: 0.3090 (0.3091)  time: 1.3810  data: 0.0002  max mem: 6104\n",
      "Epoch: [40]  [100/126]  eta: 0:00:36  lr: 0.000738  loss: 0.3102 (0.3089)  time: 1.3761  data: 0.0003  max mem: 6104\n",
      "Epoch: [40]  [120/126]  eta: 0:00:08  lr: 0.000735  loss: 0.3046 (0.3085)  time: 1.4092  data: 0.0004  max mem: 6104\n",
      "Epoch: [40]  [125/126]  eta: 0:00:01  lr: 0.000735  loss: 0.3041 (0.3087)  time: 1.4008  data: 0.0003  max mem: 6104\n",
      "Epoch: [40] Total time: 0:02:57 (1.4126 s / it)\n",
      "Averaged stats: lr: 0.000735  loss: 0.3041 (0.3087)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [41]  [  0/126]  eta: 0:10:18  lr: 0.000735  loss: 0.3093 (0.3093)  time: 4.9082  data: 3.3651  max mem: 6104\n",
      "Epoch: [41]  [ 20/126]  eta: 0:02:47  lr: 0.000732  loss: 0.3030 (0.3038)  time: 1.4097  data: 0.0003  max mem: 6104\n",
      "Epoch: [41]  [ 40/126]  eta: 0:02:06  lr: 0.000730  loss: 0.3080 (0.3078)  time: 1.3704  data: 0.0003  max mem: 6104\n",
      "Epoch: [41]  [ 60/126]  eta: 0:01:35  lr: 0.000727  loss: 0.3077 (0.3078)  time: 1.3733  data: 0.0002  max mem: 6104\n",
      "Epoch: [41]  [ 80/126]  eta: 0:01:05  lr: 0.000725  loss: 0.3099 (0.3089)  time: 1.3734  data: 0.0002  max mem: 6104\n",
      "Epoch: [41]  [100/126]  eta: 0:00:36  lr: 0.000722  loss: 0.3088 (0.3091)  time: 1.3740  data: 0.0003  max mem: 6104\n",
      "Epoch: [41]  [120/126]  eta: 0:00:08  lr: 0.000720  loss: 0.3036 (0.3087)  time: 1.3814  data: 0.0004  max mem: 6104\n",
      "Epoch: [41]  [125/126]  eta: 0:00:01  lr: 0.000719  loss: 0.3076 (0.3087)  time: 1.3685  data: 0.0004  max mem: 6104\n",
      "Epoch: [41] Total time: 0:02:57 (1.4076 s / it)\n",
      "Averaged stats: lr: 0.000719  loss: 0.3076 (0.3087)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [42]  [  0/126]  eta: 0:11:49  lr: 0.000719  loss: 0.3114 (0.3114)  time: 5.6315  data: 4.1356  max mem: 6104\n",
      "Epoch: [42]  [ 20/126]  eta: 0:02:48  lr: 0.000717  loss: 0.3081 (0.3068)  time: 1.3897  data: 0.0004  max mem: 6104\n",
      "Epoch: [42]  [ 40/126]  eta: 0:02:08  lr: 0.000714  loss: 0.3059 (0.3072)  time: 1.3830  data: 0.0003  max mem: 6104\n",
      "Epoch: [42]  [ 60/126]  eta: 0:01:36  lr: 0.000712  loss: 0.3051 (0.3070)  time: 1.3935  data: 0.0002  max mem: 6104\n",
      "Epoch: [42]  [ 80/126]  eta: 0:01:06  lr: 0.000709  loss: 0.3106 (0.3078)  time: 1.3805  data: 0.0003  max mem: 6104\n",
      "Epoch: [42]  [100/126]  eta: 0:00:37  lr: 0.000707  loss: 0.3088 (0.3080)  time: 1.3756  data: 0.0003  max mem: 6104\n",
      "Epoch: [42]  [120/126]  eta: 0:00:08  lr: 0.000704  loss: 0.3069 (0.3082)  time: 1.3670  data: 0.0003  max mem: 6104\n",
      "Epoch: [42]  [125/126]  eta: 0:00:01  lr: 0.000703  loss: 0.3116 (0.3085)  time: 1.3597  data: 0.0003  max mem: 6104\n",
      "Epoch: [42] Total time: 0:02:58 (1.4148 s / it)\n",
      "Averaged stats: lr: 0.000703  loss: 0.3116 (0.3085)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [43]  [  0/126]  eta: 0:10:07  lr: 0.000703  loss: 0.3260 (0.3260)  time: 4.8207  data: 3.2140  max mem: 6104\n",
      "Epoch: [43]  [ 20/126]  eta: 0:02:45  lr: 0.000701  loss: 0.3146 (0.3142)  time: 1.4020  data: 0.0003  max mem: 6104\n",
      "Epoch: [43]  [ 40/126]  eta: 0:02:06  lr: 0.000698  loss: 0.3031 (0.3082)  time: 1.3807  data: 0.0002  max mem: 6104\n",
      "Epoch: [43]  [ 60/126]  eta: 0:01:35  lr: 0.000696  loss: 0.3064 (0.3075)  time: 1.3778  data: 0.0003  max mem: 6104\n",
      "Epoch: [43]  [ 80/126]  eta: 0:01:05  lr: 0.000693  loss: 0.3050 (0.3069)  time: 1.3656  data: 0.0002  max mem: 6104\n",
      "Epoch: [43]  [100/126]  eta: 0:00:36  lr: 0.000691  loss: 0.3069 (0.3068)  time: 1.3898  data: 0.0002  max mem: 6104\n",
      "Epoch: [43]  [120/126]  eta: 0:00:08  lr: 0.000688  loss: 0.3111 (0.3074)  time: 1.3733  data: 0.0004  max mem: 6104\n",
      "Epoch: [43]  [125/126]  eta: 0:00:01  lr: 0.000687  loss: 0.3116 (0.3077)  time: 1.3597  data: 0.0004  max mem: 6104\n",
      "Epoch: [43] Total time: 0:02:57 (1.4079 s / it)\n",
      "Averaged stats: lr: 0.000687  loss: 0.3116 (0.3077)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [44]  [  0/126]  eta: 0:11:23  lr: 0.000687  loss: 0.3325 (0.3325)  time: 5.4228  data: 3.8005  max mem: 6104\n",
      "Epoch: [44]  [ 20/126]  eta: 0:02:49  lr: 0.000685  loss: 0.3113 (0.3128)  time: 1.4077  data: 0.0003  max mem: 6104\n",
      "Epoch: [44]  [ 40/126]  eta: 0:02:08  lr: 0.000682  loss: 0.3042 (0.3093)  time: 1.3815  data: 0.0003  max mem: 6104\n",
      "Epoch: [44]  [ 60/126]  eta: 0:01:35  lr: 0.000680  loss: 0.3012 (0.3071)  time: 1.3691  data: 0.0002  max mem: 6104\n",
      "Epoch: [44]  [ 80/126]  eta: 0:01:05  lr: 0.000677  loss: 0.3119 (0.3082)  time: 1.3742  data: 0.0003  max mem: 6104\n",
      "Epoch: [44]  [100/126]  eta: 0:00:36  lr: 0.000674  loss: 0.3003 (0.3070)  time: 1.3762  data: 0.0003  max mem: 6104\n",
      "Epoch: [44]  [120/126]  eta: 0:00:08  lr: 0.000672  loss: 0.3020 (0.3066)  time: 1.3714  data: 0.0004  max mem: 6104\n",
      "Epoch: [44]  [125/126]  eta: 0:00:01  lr: 0.000671  loss: 0.3024 (0.3069)  time: 1.3634  data: 0.0004  max mem: 6104\n",
      "Epoch: [44] Total time: 0:02:57 (1.4115 s / it)\n",
      "Averaged stats: lr: 0.000671  loss: 0.3024 (0.3069)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [45]  [  0/126]  eta: 0:10:19  lr: 0.000671  loss: 0.3106 (0.3106)  time: 4.9172  data: 3.3675  max mem: 6104\n",
      "Epoch: [45]  [ 20/126]  eta: 0:02:49  lr: 0.000668  loss: 0.3059 (0.3075)  time: 1.4310  data: 0.0003  max mem: 6104\n",
      "Epoch: [45]  [ 40/126]  eta: 0:02:08  lr: 0.000666  loss: 0.3105 (0.3080)  time: 1.3793  data: 0.0003  max mem: 6104\n",
      "Epoch: [45]  [ 60/126]  eta: 0:01:35  lr: 0.000663  loss: 0.3047 (0.3073)  time: 1.3635  data: 0.0002  max mem: 6104\n",
      "Epoch: [45]  [ 80/126]  eta: 0:01:05  lr: 0.000661  loss: 0.3048 (0.3072)  time: 1.3746  data: 0.0003  max mem: 6104\n",
      "Epoch: [45]  [100/126]  eta: 0:00:36  lr: 0.000658  loss: 0.3018 (0.3067)  time: 1.3785  data: 0.0004  max mem: 6104\n",
      "Epoch: [45]  [120/126]  eta: 0:00:08  lr: 0.000655  loss: 0.3052 (0.3066)  time: 1.3696  data: 0.0003  max mem: 6104\n",
      "Epoch: [45]  [125/126]  eta: 0:00:01  lr: 0.000655  loss: 0.3106 (0.3068)  time: 1.3613  data: 0.0003  max mem: 6104\n",
      "Epoch: [45] Total time: 0:02:57 (1.4104 s / it)\n",
      "Averaged stats: lr: 0.000655  loss: 0.3106 (0.3068)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [46]  [  0/126]  eta: 0:10:45  lr: 0.000655  loss: 0.3139 (0.3139)  time: 5.1212  data: 3.3813  max mem: 6104\n",
      "Epoch: [46]  [ 20/126]  eta: 0:02:49  lr: 0.000652  loss: 0.3052 (0.3035)  time: 1.4235  data: 0.0004  max mem: 6104\n",
      "Epoch: [46]  [ 40/126]  eta: 0:02:09  lr: 0.000649  loss: 0.3038 (0.3048)  time: 1.4149  data: 0.0003  max mem: 6104\n",
      "Epoch: [46]  [ 60/126]  eta: 0:01:36  lr: 0.000647  loss: 0.3085 (0.3057)  time: 1.3711  data: 0.0004  max mem: 6104\n",
      "Epoch: [46]  [ 80/126]  eta: 0:01:06  lr: 0.000644  loss: 0.3049 (0.3059)  time: 1.3809  data: 0.0003  max mem: 6104\n",
      "Epoch: [46]  [100/126]  eta: 0:00:37  lr: 0.000641  loss: 0.3043 (0.3055)  time: 1.3779  data: 0.0003  max mem: 6104\n",
      "Epoch: [46]  [120/126]  eta: 0:00:08  lr: 0.000639  loss: 0.3074 (0.3057)  time: 1.3740  data: 0.0004  max mem: 6104\n",
      "Epoch: [46]  [125/126]  eta: 0:00:01  lr: 0.000638  loss: 0.3069 (0.3054)  time: 1.3683  data: 0.0003  max mem: 6104\n",
      "Epoch: [46] Total time: 0:02:58 (1.4194 s / it)\n",
      "Averaged stats: lr: 0.000638  loss: 0.3069 (0.3054)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [47]  [  0/126]  eta: 0:10:26  lr: 0.000638  loss: 0.2952 (0.2952)  time: 4.9744  data: 3.3798  max mem: 6104\n",
      "Epoch: [47]  [ 20/126]  eta: 0:02:48  lr: 0.000635  loss: 0.3037 (0.3037)  time: 1.4191  data: 0.0003  max mem: 6104\n",
      "Epoch: [47]  [ 40/126]  eta: 0:02:07  lr: 0.000632  loss: 0.3041 (0.3047)  time: 1.3826  data: 0.0003  max mem: 6104\n",
      "Epoch: [47]  [ 60/126]  eta: 0:01:35  lr: 0.000630  loss: 0.3027 (0.3045)  time: 1.3716  data: 0.0003  max mem: 6104\n",
      "Epoch: [47]  [ 80/126]  eta: 0:01:05  lr: 0.000627  loss: 0.3016 (0.3041)  time: 1.3766  data: 0.0003  max mem: 6104\n",
      "Epoch: [47]  [100/126]  eta: 0:00:37  lr: 0.000624  loss: 0.3094 (0.3047)  time: 1.3937  data: 0.0003  max mem: 6104\n",
      "Epoch: [47]  [120/126]  eta: 0:00:08  lr: 0.000622  loss: 0.3059 (0.3049)  time: 1.3780  data: 0.0004  max mem: 6104\n",
      "Epoch: [47]  [125/126]  eta: 0:00:01  lr: 0.000621  loss: 0.3058 (0.3049)  time: 1.3676  data: 0.0003  max mem: 6104\n",
      "Epoch: [47] Total time: 0:02:58 (1.4149 s / it)\n",
      "Averaged stats: lr: 0.000621  loss: 0.3058 (0.3049)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [48]  [  0/126]  eta: 0:11:41  lr: 0.000621  loss: 0.3022 (0.3022)  time: 5.5669  data: 4.0002  max mem: 6104\n",
      "Epoch: [48]  [ 20/126]  eta: 0:02:49  lr: 0.000618  loss: 0.3004 (0.3035)  time: 1.4012  data: 0.0004  max mem: 6104\n",
      "Epoch: [48]  [ 40/126]  eta: 0:02:08  lr: 0.000616  loss: 0.3060 (0.3046)  time: 1.3854  data: 0.0002  max mem: 6104\n",
      "Epoch: [48]  [ 60/126]  eta: 0:01:36  lr: 0.000613  loss: 0.3071 (0.3056)  time: 1.3791  data: 0.0003  max mem: 6104\n",
      "Epoch: [48]  [ 80/126]  eta: 0:01:06  lr: 0.000610  loss: 0.3053 (0.3053)  time: 1.3824  data: 0.0002  max mem: 6104\n",
      "Epoch: [48]  [100/126]  eta: 0:00:37  lr: 0.000607  loss: 0.3019 (0.3050)  time: 1.3750  data: 0.0002  max mem: 6104\n",
      "Epoch: [48]  [120/126]  eta: 0:00:08  lr: 0.000605  loss: 0.3009 (0.3047)  time: 1.3783  data: 0.0005  max mem: 6104\n",
      "Epoch: [48]  [125/126]  eta: 0:00:01  lr: 0.000604  loss: 0.3021 (0.3048)  time: 1.3732  data: 0.0005  max mem: 6104\n",
      "Epoch: [48] Total time: 0:02:58 (1.4165 s / it)\n",
      "Averaged stats: lr: 0.000604  loss: 0.3021 (0.3048)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [49]  [  0/126]  eta: 0:10:15  lr: 0.000604  loss: 0.3003 (0.3003)  time: 4.8883  data: 3.2499  max mem: 6104\n",
      "Epoch: [49]  [ 20/126]  eta: 0:02:50  lr: 0.000601  loss: 0.3000 (0.3041)  time: 1.4452  data: 0.0004  max mem: 6104\n",
      "Epoch: [49]  [ 40/126]  eta: 0:02:08  lr: 0.000599  loss: 0.3066 (0.3039)  time: 1.3777  data: 0.0002  max mem: 6104\n",
      "Epoch: [49]  [ 60/126]  eta: 0:01:36  lr: 0.000596  loss: 0.2969 (0.3024)  time: 1.3774  data: 0.0003  max mem: 6104\n",
      "Epoch: [49]  [ 80/126]  eta: 0:01:06  lr: 0.000593  loss: 0.3051 (0.3031)  time: 1.3801  data: 0.0003  max mem: 6104\n",
      "Epoch: [49]  [100/126]  eta: 0:00:37  lr: 0.000590  loss: 0.3100 (0.3044)  time: 1.3752  data: 0.0003  max mem: 6104\n",
      "Epoch: [49]  [120/126]  eta: 0:00:08  lr: 0.000588  loss: 0.3011 (0.3043)  time: 1.3771  data: 0.0004  max mem: 6104\n",
      "Epoch: [49]  [125/126]  eta: 0:00:01  lr: 0.000587  loss: 0.3052 (0.3046)  time: 1.3672  data: 0.0003  max mem: 6104\n",
      "Epoch: [49] Total time: 0:02:58 (1.4162 s / it)\n",
      "Averaged stats: lr: 0.000587  loss: 0.3052 (0.3046)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [50]  [  0/126]  eta: 0:11:39  lr: 0.000587  loss: 0.2959 (0.2959)  time: 5.5522  data: 3.9640  max mem: 6104\n",
      "Epoch: [50]  [ 20/126]  eta: 0:02:50  lr: 0.000584  loss: 0.3051 (0.3052)  time: 1.4135  data: 0.0004  max mem: 6104\n",
      "Epoch: [50]  [ 40/126]  eta: 0:02:10  lr: 0.000581  loss: 0.3098 (0.3065)  time: 1.4192  data: 0.0003  max mem: 6104\n",
      "Epoch: [50]  [ 60/126]  eta: 0:01:37  lr: 0.000579  loss: 0.3011 (0.3049)  time: 1.3815  data: 0.0002  max mem: 6104\n",
      "Epoch: [50]  [ 80/126]  eta: 0:01:06  lr: 0.000576  loss: 0.3012 (0.3043)  time: 1.3790  data: 0.0003  max mem: 6104\n",
      "Epoch: [50]  [100/126]  eta: 0:00:37  lr: 0.000573  loss: 0.3016 (0.3040)  time: 1.3765  data: 0.0002  max mem: 6104\n",
      "Epoch: [50]  [120/126]  eta: 0:00:08  lr: 0.000570  loss: 0.3033 (0.3039)  time: 1.3803  data: 0.0004  max mem: 6104\n",
      "Epoch: [50]  [125/126]  eta: 0:00:01  lr: 0.000570  loss: 0.3024 (0.3035)  time: 1.3723  data: 0.0004  max mem: 6104\n",
      "Epoch: [50] Total time: 0:02:59 (1.4240 s / it)\n",
      "Averaged stats: lr: 0.000570  loss: 0.3024 (0.3035)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [51]  [  0/126]  eta: 0:10:25  lr: 0.000570  loss: 0.2880 (0.2880)  time: 4.9634  data: 3.4355  max mem: 6104\n",
      "Epoch: [51]  [ 20/126]  eta: 0:02:47  lr: 0.000567  loss: 0.3014 (0.3011)  time: 1.4085  data: 0.0004  max mem: 6104\n",
      "Epoch: [51]  [ 40/126]  eta: 0:02:07  lr: 0.000564  loss: 0.3091 (0.3040)  time: 1.3797  data: 0.0003  max mem: 6104\n",
      "Epoch: [51]  [ 60/126]  eta: 0:01:35  lr: 0.000561  loss: 0.3055 (0.3046)  time: 1.3752  data: 0.0002  max mem: 6104\n",
      "Epoch: [51]  [ 80/126]  eta: 0:01:05  lr: 0.000559  loss: 0.3071 (0.3044)  time: 1.3850  data: 0.0002  max mem: 6104\n",
      "Epoch: [51]  [100/126]  eta: 0:00:37  lr: 0.000556  loss: 0.3086 (0.3050)  time: 1.4156  data: 0.0003  max mem: 6104\n",
      "Epoch: [51]  [120/126]  eta: 0:00:08  lr: 0.000553  loss: 0.2995 (0.3046)  time: 1.3794  data: 0.0004  max mem: 6104\n",
      "Epoch: [51]  [125/126]  eta: 0:00:01  lr: 0.000552  loss: 0.2961 (0.3045)  time: 1.3725  data: 0.0003  max mem: 6104\n",
      "Epoch: [51] Total time: 0:02:58 (1.4189 s / it)\n",
      "Averaged stats: lr: 0.000552  loss: 0.2961 (0.3045)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [52]  [  0/126]  eta: 0:10:33  lr: 0.000552  loss: 0.3064 (0.3064)  time: 5.0258  data: 3.4278  max mem: 6104\n",
      "Epoch: [52]  [ 20/126]  eta: 0:02:47  lr: 0.000550  loss: 0.3067 (0.3046)  time: 1.4103  data: 0.0004  max mem: 6104\n",
      "Epoch: [52]  [ 40/126]  eta: 0:02:07  lr: 0.000547  loss: 0.2966 (0.3003)  time: 1.3739  data: 0.0003  max mem: 6104\n",
      "Epoch: [52]  [ 60/126]  eta: 0:01:35  lr: 0.000544  loss: 0.3067 (0.3027)  time: 1.3777  data: 0.0003  max mem: 6104\n",
      "Epoch: [52]  [ 80/126]  eta: 0:01:05  lr: 0.000541  loss: 0.2971 (0.3015)  time: 1.3822  data: 0.0003  max mem: 6104\n",
      "Epoch: [52]  [100/126]  eta: 0:00:36  lr: 0.000538  loss: 0.3030 (0.3021)  time: 1.3750  data: 0.0002  max mem: 6104\n",
      "Epoch: [52]  [120/126]  eta: 0:00:08  lr: 0.000536  loss: 0.3020 (0.3023)  time: 1.3766  data: 0.0005  max mem: 6104\n",
      "Epoch: [52]  [125/126]  eta: 0:00:01  lr: 0.000535  loss: 0.2970 (0.3025)  time: 1.3706  data: 0.0004  max mem: 6104\n",
      "Epoch: [52] Total time: 0:02:57 (1.4111 s / it)\n",
      "Averaged stats: lr: 0.000535  loss: 0.2970 (0.3025)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [53]  [  0/126]  eta: 0:10:09  lr: 0.000535  loss: 0.3285 (0.3285)  time: 4.8404  data: 3.2099  max mem: 6104\n",
      "Epoch: [53]  [ 20/126]  eta: 0:02:46  lr: 0.000532  loss: 0.2991 (0.3016)  time: 1.4107  data: 0.0004  max mem: 6104\n",
      "Epoch: [53]  [ 40/126]  eta: 0:02:08  lr: 0.000529  loss: 0.3023 (0.3028)  time: 1.4172  data: 0.0003  max mem: 6104\n",
      "Epoch: [53]  [ 60/126]  eta: 0:01:36  lr: 0.000527  loss: 0.3005 (0.3031)  time: 1.3827  data: 0.0003  max mem: 6104\n",
      "Epoch: [53]  [ 80/126]  eta: 0:01:06  lr: 0.000524  loss: 0.3032 (0.3030)  time: 1.3804  data: 0.0002  max mem: 6104\n",
      "Epoch: [53]  [100/126]  eta: 0:00:37  lr: 0.000521  loss: 0.3010 (0.3028)  time: 1.3731  data: 0.0003  max mem: 6104\n",
      "Epoch: [53]  [120/126]  eta: 0:00:08  lr: 0.000518  loss: 0.3000 (0.3025)  time: 1.3824  data: 0.0004  max mem: 6104\n",
      "Epoch: [53]  [125/126]  eta: 0:00:01  lr: 0.000518  loss: 0.3033 (0.3025)  time: 1.3674  data: 0.0004  max mem: 6104\n",
      "Epoch: [53] Total time: 0:02:58 (1.4176 s / it)\n",
      "Averaged stats: lr: 0.000518  loss: 0.3033 (0.3025)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [54]  [  0/126]  eta: 0:10:13  lr: 0.000517  loss: 0.2958 (0.2958)  time: 4.8717  data: 3.2716  max mem: 6104\n",
      "Epoch: [54]  [ 20/126]  eta: 0:02:47  lr: 0.000515  loss: 0.3042 (0.3051)  time: 1.4110  data: 0.0004  max mem: 6104\n",
      "Epoch: [54]  [ 40/126]  eta: 0:02:07  lr: 0.000512  loss: 0.2969 (0.3030)  time: 1.3803  data: 0.0003  max mem: 6104\n",
      "Epoch: [54]  [ 60/126]  eta: 0:01:35  lr: 0.000509  loss: 0.3059 (0.3040)  time: 1.3753  data: 0.0002  max mem: 6104\n",
      "Epoch: [54]  [ 80/126]  eta: 0:01:06  lr: 0.000506  loss: 0.3023 (0.3035)  time: 1.4164  data: 0.0003  max mem: 6104\n",
      "Epoch: [54]  [100/126]  eta: 0:00:37  lr: 0.000504  loss: 0.3010 (0.3029)  time: 1.3741  data: 0.0002  max mem: 6104\n",
      "Epoch: [54]  [120/126]  eta: 0:00:08  lr: 0.000501  loss: 0.2947 (0.3019)  time: 1.3707  data: 0.0004  max mem: 6104\n",
      "Epoch: [54]  [125/126]  eta: 0:00:01  lr: 0.000500  loss: 0.3046 (0.3020)  time: 1.3639  data: 0.0003  max mem: 6104\n",
      "Epoch: [54] Total time: 0:02:58 (1.4149 s / it)\n",
      "Averaged stats: lr: 0.000500  loss: 0.3046 (0.3020)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [55]  [  0/126]  eta: 0:10:34  lr: 0.000500  loss: 0.2955 (0.2955)  time: 5.0360  data: 3.4811  max mem: 6104\n",
      "Epoch: [55]  [ 20/126]  eta: 0:02:47  lr: 0.000497  loss: 0.2981 (0.2990)  time: 1.4046  data: 0.0004  max mem: 6104\n",
      "Epoch: [55]  [ 40/126]  eta: 0:02:07  lr: 0.000494  loss: 0.3037 (0.3023)  time: 1.3829  data: 0.0002  max mem: 6104\n",
      "Epoch: [55]  [ 60/126]  eta: 0:01:35  lr: 0.000492  loss: 0.2981 (0.3016)  time: 1.3749  data: 0.0002  max mem: 6104\n",
      "Epoch: [55]  [ 80/126]  eta: 0:01:05  lr: 0.000489  loss: 0.3016 (0.3015)  time: 1.3841  data: 0.0003  max mem: 6104\n",
      "Epoch: [55]  [100/126]  eta: 0:00:36  lr: 0.000486  loss: 0.2991 (0.3011)  time: 1.3689  data: 0.0002  max mem: 6104\n",
      "Epoch: [55]  [120/126]  eta: 0:00:08  lr: 0.000483  loss: 0.2990 (0.3012)  time: 1.3857  data: 0.0004  max mem: 6104\n",
      "Epoch: [55]  [125/126]  eta: 0:00:01  lr: 0.000483  loss: 0.2977 (0.3013)  time: 1.3802  data: 0.0003  max mem: 6104\n",
      "Epoch: [55] Total time: 0:02:57 (1.4124 s / it)\n",
      "Averaged stats: lr: 0.000483  loss: 0.2977 (0.3013)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [56]  [  0/126]  eta: 0:10:39  lr: 0.000483  loss: 0.3215 (0.3215)  time: 5.0757  data: 3.4732  max mem: 6104\n",
      "Epoch: [56]  [ 20/126]  eta: 0:02:47  lr: 0.000480  loss: 0.3049 (0.3055)  time: 1.4085  data: 0.0004  max mem: 6104\n",
      "Epoch: [56]  [ 40/126]  eta: 0:02:07  lr: 0.000477  loss: 0.2992 (0.3026)  time: 1.3734  data: 0.0002  max mem: 6104\n",
      "Epoch: [56]  [ 60/126]  eta: 0:01:35  lr: 0.000474  loss: 0.3024 (0.3024)  time: 1.3757  data: 0.0003  max mem: 6104\n",
      "Epoch: [56]  [ 80/126]  eta: 0:01:05  lr: 0.000471  loss: 0.3052 (0.3031)  time: 1.3813  data: 0.0003  max mem: 6104\n",
      "Epoch: [56]  [100/126]  eta: 0:00:36  lr: 0.000469  loss: 0.2995 (0.3020)  time: 1.3777  data: 0.0002  max mem: 6104\n",
      "Epoch: [56]  [120/126]  eta: 0:00:08  lr: 0.000466  loss: 0.2998 (0.3018)  time: 1.3776  data: 0.0003  max mem: 6104\n",
      "Epoch: [56]  [125/126]  eta: 0:00:01  lr: 0.000465  loss: 0.2953 (0.3016)  time: 1.3719  data: 0.0003  max mem: 6104\n",
      "Epoch: [56] Total time: 0:02:57 (1.4113 s / it)\n",
      "Averaged stats: lr: 0.000465  loss: 0.2953 (0.3016)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [57]  [  0/126]  eta: 0:10:06  lr: 0.000465  loss: 0.2950 (0.2950)  time: 4.8171  data: 3.2428  max mem: 6104\n",
      "Epoch: [57]  [ 20/126]  eta: 0:02:47  lr: 0.000462  loss: 0.2989 (0.2989)  time: 1.4144  data: 0.0004  max mem: 6104\n",
      "Epoch: [57]  [ 40/126]  eta: 0:02:07  lr: 0.000460  loss: 0.3064 (0.3013)  time: 1.3791  data: 0.0003  max mem: 6104\n",
      "Epoch: [57]  [ 60/126]  eta: 0:01:35  lr: 0.000457  loss: 0.2948 (0.3005)  time: 1.4011  data: 0.0003  max mem: 6104\n",
      "Epoch: [57]  [ 80/126]  eta: 0:01:06  lr: 0.000454  loss: 0.3017 (0.3012)  time: 1.3783  data: 0.0004  max mem: 6104\n",
      "Epoch: [57]  [100/126]  eta: 0:00:37  lr: 0.000451  loss: 0.2962 (0.3005)  time: 1.3750  data: 0.0003  max mem: 6104\n",
      "Epoch: [57]  [120/126]  eta: 0:00:08  lr: 0.000449  loss: 0.2997 (0.3006)  time: 1.3710  data: 0.0004  max mem: 6104\n",
      "Epoch: [57]  [125/126]  eta: 0:00:01  lr: 0.000448  loss: 0.2997 (0.3008)  time: 1.3664  data: 0.0004  max mem: 6104\n",
      "Epoch: [57] Total time: 0:02:58 (1.4134 s / it)\n",
      "Averaged stats: lr: 0.000448  loss: 0.2997 (0.3008)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [58]  [  0/126]  eta: 0:10:57  lr: 0.000448  loss: 0.3012 (0.3012)  time: 5.2214  data: 3.6727  max mem: 6104\n",
      "Epoch: [58]  [ 20/126]  eta: 0:02:47  lr: 0.000445  loss: 0.2987 (0.3025)  time: 1.4023  data: 0.0003  max mem: 6104\n",
      "Epoch: [58]  [ 40/126]  eta: 0:02:07  lr: 0.000442  loss: 0.2995 (0.3004)  time: 1.3809  data: 0.0003  max mem: 6104\n",
      "Epoch: [58]  [ 60/126]  eta: 0:01:35  lr: 0.000439  loss: 0.3002 (0.3009)  time: 1.3744  data: 0.0003  max mem: 6104\n",
      "Epoch: [58]  [ 80/126]  eta: 0:01:05  lr: 0.000437  loss: 0.2993 (0.3007)  time: 1.3795  data: 0.0003  max mem: 6104\n",
      "Epoch: [58]  [100/126]  eta: 0:00:37  lr: 0.000434  loss: 0.3044 (0.3014)  time: 1.3955  data: 0.0003  max mem: 6104\n",
      "Epoch: [58]  [120/126]  eta: 0:00:08  lr: 0.000431  loss: 0.2942 (0.3003)  time: 1.3838  data: 0.0004  max mem: 6104\n",
      "Epoch: [58]  [125/126]  eta: 0:00:01  lr: 0.000431  loss: 0.2988 (0.3006)  time: 1.3802  data: 0.0003  max mem: 6104\n",
      "Epoch: [58] Total time: 0:02:58 (1.4162 s / it)\n",
      "Averaged stats: lr: 0.000431  loss: 0.2988 (0.3006)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [59]  [  0/126]  eta: 0:10:09  lr: 0.000430  loss: 0.2880 (0.2880)  time: 4.8335  data: 3.2123  max mem: 6104\n",
      "Epoch: [59]  [ 20/126]  eta: 0:02:46  lr: 0.000428  loss: 0.2953 (0.2974)  time: 1.4052  data: 0.0003  max mem: 6104\n",
      "Epoch: [59]  [ 40/126]  eta: 0:02:07  lr: 0.000425  loss: 0.3039 (0.3013)  time: 1.3816  data: 0.0003  max mem: 6104\n",
      "Epoch: [59]  [ 60/126]  eta: 0:01:35  lr: 0.000422  loss: 0.3034 (0.3025)  time: 1.3761  data: 0.0003  max mem: 6104\n",
      "Epoch: [59]  [ 80/126]  eta: 0:01:05  lr: 0.000419  loss: 0.2938 (0.3008)  time: 1.3794  data: 0.0003  max mem: 6104\n",
      "Epoch: [59]  [100/126]  eta: 0:00:36  lr: 0.000417  loss: 0.2938 (0.3000)  time: 1.3766  data: 0.0003  max mem: 6104\n",
      "Epoch: [59]  [120/126]  eta: 0:00:08  lr: 0.000414  loss: 0.3023 (0.3009)  time: 1.3727  data: 0.0003  max mem: 6104\n",
      "Epoch: [59]  [125/126]  eta: 0:00:01  lr: 0.000413  loss: 0.3000 (0.3006)  time: 1.3644  data: 0.0003  max mem: 6104\n",
      "Epoch: [59] Total time: 0:02:57 (1.4089 s / it)\n",
      "Averaged stats: lr: 0.000413  loss: 0.3000 (0.3006)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [60]  [  0/126]  eta: 0:10:35  lr: 0.000413  loss: 0.3089 (0.3089)  time: 5.0439  data: 3.5377  max mem: 6104\n",
      "Epoch: [60]  [ 20/126]  eta: 0:02:47  lr: 0.000410  loss: 0.3003 (0.3024)  time: 1.4025  data: 0.0004  max mem: 6104\n",
      "Epoch: [60]  [ 40/126]  eta: 0:02:08  lr: 0.000408  loss: 0.2949 (0.2993)  time: 1.4170  data: 0.0003  max mem: 6104\n",
      "Epoch: [60]  [ 60/126]  eta: 0:01:36  lr: 0.000405  loss: 0.2988 (0.3000)  time: 1.3788  data: 0.0003  max mem: 6104\n",
      "Epoch: [60]  [ 80/126]  eta: 0:01:06  lr: 0.000402  loss: 0.2952 (0.2987)  time: 1.3789  data: 0.0002  max mem: 6104\n",
      "Epoch: [60]  [100/126]  eta: 0:00:37  lr: 0.000400  loss: 0.2980 (0.2988)  time: 1.3828  data: 0.0002  max mem: 6104\n",
      "Epoch: [60]  [120/126]  eta: 0:00:08  lr: 0.000397  loss: 0.2984 (0.2993)  time: 1.3714  data: 0.0004  max mem: 6104\n",
      "Epoch: [60]  [125/126]  eta: 0:00:01  lr: 0.000396  loss: 0.2984 (0.2992)  time: 1.3670  data: 0.0003  max mem: 6104\n",
      "Epoch: [60] Total time: 0:02:58 (1.4170 s / it)\n",
      "Averaged stats: lr: 0.000396  loss: 0.2984 (0.2992)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [61]  [  0/126]  eta: 0:10:44  lr: 0.000396  loss: 0.2725 (0.2725)  time: 5.1142  data: 3.4823  max mem: 6104\n",
      "Epoch: [61]  [ 20/126]  eta: 0:02:48  lr: 0.000393  loss: 0.2968 (0.2926)  time: 1.4120  data: 0.0003  max mem: 6104\n",
      "Epoch: [61]  [ 40/126]  eta: 0:02:07  lr: 0.000391  loss: 0.3039 (0.2973)  time: 1.3768  data: 0.0002  max mem: 6104\n",
      "Epoch: [61]  [ 60/126]  eta: 0:01:35  lr: 0.000388  loss: 0.2959 (0.2984)  time: 1.3737  data: 0.0002  max mem: 6104\n",
      "Epoch: [61]  [ 80/126]  eta: 0:01:06  lr: 0.000385  loss: 0.3017 (0.2984)  time: 1.4237  data: 0.0002  max mem: 6104\n",
      "Epoch: [61]  [100/126]  eta: 0:00:37  lr: 0.000383  loss: 0.3005 (0.2988)  time: 1.3780  data: 0.0003  max mem: 6104\n",
      "Epoch: [61]  [120/126]  eta: 0:00:08  lr: 0.000380  loss: 0.2925 (0.2982)  time: 1.3756  data: 0.0003  max mem: 6104\n",
      "Epoch: [61]  [125/126]  eta: 0:00:01  lr: 0.000379  loss: 0.2925 (0.2982)  time: 1.3713  data: 0.0003  max mem: 6104\n",
      "Epoch: [61] Total time: 0:02:58 (1.4191 s / it)\n",
      "Averaged stats: lr: 0.000379  loss: 0.2925 (0.2982)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [62]  [  0/126]  eta: 0:10:17  lr: 0.000379  loss: 0.3008 (0.3008)  time: 4.9042  data: 3.3161  max mem: 6104\n",
      "Epoch: [62]  [ 20/126]  eta: 0:02:46  lr: 0.000376  loss: 0.2959 (0.2966)  time: 1.4049  data: 0.0004  max mem: 6104\n",
      "Epoch: [62]  [ 40/126]  eta: 0:02:07  lr: 0.000374  loss: 0.2983 (0.2983)  time: 1.3870  data: 0.0003  max mem: 6104\n",
      "Epoch: [62]  [ 60/126]  eta: 0:01:35  lr: 0.000371  loss: 0.2986 (0.2985)  time: 1.3736  data: 0.0003  max mem: 6104\n",
      "Epoch: [62]  [ 80/126]  eta: 0:01:05  lr: 0.000368  loss: 0.2975 (0.2986)  time: 1.3777  data: 0.0002  max mem: 6104\n",
      "Epoch: [62]  [100/126]  eta: 0:00:36  lr: 0.000366  loss: 0.2980 (0.2990)  time: 1.3783  data: 0.0003  max mem: 6104\n",
      "Epoch: [62]  [120/126]  eta: 0:00:08  lr: 0.000363  loss: 0.2944 (0.2984)  time: 1.4077  data: 0.0003  max mem: 6104\n",
      "Epoch: [62]  [125/126]  eta: 0:00:01  lr: 0.000362  loss: 0.2933 (0.2984)  time: 1.4011  data: 0.0003  max mem: 6104\n",
      "Epoch: [62] Total time: 0:02:58 (1.4153 s / it)\n",
      "Averaged stats: lr: 0.000362  loss: 0.2933 (0.2984)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [63]  [  0/126]  eta: 0:10:03  lr: 0.000362  loss: 0.3068 (0.3068)  time: 4.7894  data: 3.2008  max mem: 6104\n",
      "Epoch: [63]  [ 20/126]  eta: 0:02:46  lr: 0.000360  loss: 0.2893 (0.2920)  time: 1.4101  data: 0.0005  max mem: 6104\n",
      "Epoch: [63]  [ 40/126]  eta: 0:02:07  lr: 0.000357  loss: 0.2944 (0.2941)  time: 1.3803  data: 0.0004  max mem: 6104\n",
      "Epoch: [63]  [ 60/126]  eta: 0:01:35  lr: 0.000354  loss: 0.3001 (0.2974)  time: 1.3741  data: 0.0003  max mem: 6104\n",
      "Epoch: [63]  [ 80/126]  eta: 0:01:05  lr: 0.000352  loss: 0.2942 (0.2978)  time: 1.3784  data: 0.0003  max mem: 6104\n",
      "Epoch: [63]  [100/126]  eta: 0:00:36  lr: 0.000349  loss: 0.2967 (0.2978)  time: 1.3854  data: 0.0003  max mem: 6104\n",
      "Epoch: [63]  [120/126]  eta: 0:00:08  lr: 0.000346  loss: 0.2962 (0.2973)  time: 1.3722  data: 0.0004  max mem: 6104\n",
      "Epoch: [63]  [125/126]  eta: 0:00:01  lr: 0.000346  loss: 0.2975 (0.2970)  time: 1.3630  data: 0.0004  max mem: 6104\n",
      "Epoch: [63] Total time: 0:02:57 (1.4097 s / it)\n",
      "Averaged stats: lr: 0.000346  loss: 0.2975 (0.2970)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [64]  [  0/126]  eta: 0:10:27  lr: 0.000345  loss: 0.2902 (0.2902)  time: 4.9779  data: 3.4335  max mem: 6104\n",
      "Epoch: [64]  [ 20/126]  eta: 0:02:49  lr: 0.000343  loss: 0.3028 (0.2995)  time: 1.4324  data: 0.0003  max mem: 6104\n",
      "Epoch: [64]  [ 40/126]  eta: 0:02:08  lr: 0.000340  loss: 0.2943 (0.2967)  time: 1.3839  data: 0.0004  max mem: 6104\n",
      "Epoch: [64]  [ 60/126]  eta: 0:01:36  lr: 0.000338  loss: 0.3019 (0.2986)  time: 1.3722  data: 0.0003  max mem: 6104\n",
      "Epoch: [64]  [ 80/126]  eta: 0:01:06  lr: 0.000335  loss: 0.2906 (0.2971)  time: 1.3789  data: 0.0002  max mem: 6104\n",
      "Epoch: [64]  [100/126]  eta: 0:00:37  lr: 0.000332  loss: 0.3000 (0.2973)  time: 1.3821  data: 0.0003  max mem: 6104\n",
      "Epoch: [64]  [120/126]  eta: 0:00:08  lr: 0.000330  loss: 0.2956 (0.2977)  time: 1.3800  data: 0.0003  max mem: 6104\n",
      "Epoch: [64]  [125/126]  eta: 0:00:01  lr: 0.000329  loss: 0.2940 (0.2979)  time: 1.3709  data: 0.0003  max mem: 6104\n",
      "Epoch: [64] Total time: 0:02:58 (1.4162 s / it)\n",
      "Averaged stats: lr: 0.000329  loss: 0.2940 (0.2979)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [65]  [  0/126]  eta: 0:10:31  lr: 0.000329  loss: 0.3000 (0.3000)  time: 5.0090  data: 3.3608  max mem: 6104\n",
      "Epoch: [65]  [ 20/126]  eta: 0:02:46  lr: 0.000326  loss: 0.2933 (0.2937)  time: 1.3986  data: 0.0004  max mem: 6104\n",
      "Epoch: [65]  [ 40/126]  eta: 0:02:07  lr: 0.000324  loss: 0.2933 (0.2939)  time: 1.3830  data: 0.0003  max mem: 6104\n",
      "Epoch: [65]  [ 60/126]  eta: 0:01:35  lr: 0.000321  loss: 0.2976 (0.2954)  time: 1.3771  data: 0.0003  max mem: 6104\n",
      "Epoch: [65]  [ 80/126]  eta: 0:01:05  lr: 0.000319  loss: 0.2943 (0.2955)  time: 1.3767  data: 0.0003  max mem: 6104\n",
      "Epoch: [65]  [100/126]  eta: 0:00:37  lr: 0.000316  loss: 0.2965 (0.2959)  time: 1.4184  data: 0.0003  max mem: 6104\n",
      "Epoch: [65]  [120/126]  eta: 0:00:08  lr: 0.000313  loss: 0.2980 (0.2966)  time: 1.3797  data: 0.0003  max mem: 6104\n",
      "Epoch: [65]  [125/126]  eta: 0:00:01  lr: 0.000313  loss: 0.3031 (0.2968)  time: 1.3704  data: 0.0003  max mem: 6104\n",
      "Epoch: [65] Total time: 0:02:58 (1.4171 s / it)\n",
      "Averaged stats: lr: 0.000313  loss: 0.3031 (0.2968)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [66]  [  0/126]  eta: 0:10:09  lr: 0.000313  loss: 0.2761 (0.2761)  time: 4.8365  data: 3.2514  max mem: 6104\n",
      "Epoch: [66]  [ 20/126]  eta: 0:02:47  lr: 0.000310  loss: 0.3010 (0.2985)  time: 1.4155  data: 0.0005  max mem: 6104\n",
      "Epoch: [66]  [ 40/126]  eta: 0:02:07  lr: 0.000308  loss: 0.2949 (0.2972)  time: 1.3781  data: 0.0003  max mem: 6104\n",
      "Epoch: [66]  [ 60/126]  eta: 0:01:35  lr: 0.000305  loss: 0.2986 (0.2976)  time: 1.3739  data: 0.0003  max mem: 6104\n",
      "Epoch: [66]  [ 80/126]  eta: 0:01:05  lr: 0.000302  loss: 0.2915 (0.2966)  time: 1.3852  data: 0.0003  max mem: 6104\n",
      "Epoch: [66]  [100/126]  eta: 0:00:36  lr: 0.000300  loss: 0.2971 (0.2968)  time: 1.3843  data: 0.0003  max mem: 6104\n",
      "Epoch: [66]  [120/126]  eta: 0:00:08  lr: 0.000297  loss: 0.2998 (0.2976)  time: 1.3857  data: 0.0003  max mem: 6104\n",
      "Epoch: [66]  [125/126]  eta: 0:00:01  lr: 0.000297  loss: 0.2994 (0.2973)  time: 1.3767  data: 0.0003  max mem: 6104\n",
      "Epoch: [66] Total time: 0:02:58 (1.4141 s / it)\n",
      "Averaged stats: lr: 0.000297  loss: 0.2994 (0.2973)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [67]  [  0/126]  eta: 0:10:03  lr: 0.000297  loss: 0.3126 (0.3126)  time: 4.7881  data: 3.1444  max mem: 6104\n",
      "Epoch: [67]  [ 20/126]  eta: 0:02:49  lr: 0.000294  loss: 0.2912 (0.2946)  time: 1.4417  data: 0.0005  max mem: 6104\n",
      "Epoch: [67]  [ 40/126]  eta: 0:02:08  lr: 0.000292  loss: 0.2936 (0.2949)  time: 1.3833  data: 0.0003  max mem: 6104\n",
      "Epoch: [67]  [ 60/126]  eta: 0:01:36  lr: 0.000289  loss: 0.2909 (0.2944)  time: 1.3789  data: 0.0002  max mem: 6104\n",
      "Epoch: [67]  [ 80/126]  eta: 0:01:06  lr: 0.000287  loss: 0.2995 (0.2957)  time: 1.3812  data: 0.0003  max mem: 6104\n",
      "Epoch: [67]  [100/126]  eta: 0:00:37  lr: 0.000284  loss: 0.3026 (0.2964)  time: 1.3819  data: 0.0002  max mem: 6104\n",
      "Epoch: [67]  [120/126]  eta: 0:00:08  lr: 0.000282  loss: 0.2978 (0.2963)  time: 1.3783  data: 0.0005  max mem: 6104\n",
      "Epoch: [67]  [125/126]  eta: 0:00:01  lr: 0.000281  loss: 0.2964 (0.2963)  time: 1.3736  data: 0.0005  max mem: 6104\n",
      "Epoch: [67] Total time: 0:02:58 (1.4174 s / it)\n",
      "Averaged stats: lr: 0.000281  loss: 0.2964 (0.2963)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [68]  [  0/126]  eta: 0:10:12  lr: 0.000281  loss: 0.2908 (0.2908)  time: 4.8607  data: 3.3158  max mem: 6104\n",
      "Epoch: [68]  [ 20/126]  eta: 0:02:46  lr: 0.000278  loss: 0.2946 (0.2962)  time: 1.4078  data: 0.0003  max mem: 6104\n",
      "Epoch: [68]  [ 40/126]  eta: 0:02:07  lr: 0.000276  loss: 0.2967 (0.2982)  time: 1.3778  data: 0.0003  max mem: 6104\n",
      "Epoch: [68]  [ 60/126]  eta: 0:01:35  lr: 0.000273  loss: 0.2984 (0.2981)  time: 1.3791  data: 0.0002  max mem: 6104\n",
      "Epoch: [68]  [ 80/126]  eta: 0:01:05  lr: 0.000271  loss: 0.2961 (0.2976)  time: 1.3762  data: 0.0002  max mem: 6104\n",
      "Epoch: [68]  [100/126]  eta: 0:00:37  lr: 0.000268  loss: 0.2912 (0.2971)  time: 1.4189  data: 0.0003  max mem: 6104\n",
      "Epoch: [68]  [120/126]  eta: 0:00:08  lr: 0.000266  loss: 0.2920 (0.2968)  time: 1.3761  data: 0.0004  max mem: 6104\n",
      "Epoch: [68]  [125/126]  eta: 0:00:01  lr: 0.000265  loss: 0.2948 (0.2968)  time: 1.3655  data: 0.0003  max mem: 6104\n",
      "Epoch: [68] Total time: 0:02:58 (1.4162 s / it)\n",
      "Averaged stats: lr: 0.000265  loss: 0.2948 (0.2968)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [69]  [  0/126]  eta: 0:11:31  lr: 0.000265  loss: 0.3166 (0.3166)  time: 5.4919  data: 3.9243  max mem: 6104\n",
      "Epoch: [69]  [ 20/126]  eta: 0:02:49  lr: 0.000263  loss: 0.2948 (0.2964)  time: 1.4067  data: 0.0003  max mem: 6104\n",
      "Epoch: [69]  [ 40/126]  eta: 0:02:08  lr: 0.000260  loss: 0.2998 (0.2964)  time: 1.3846  data: 0.0003  max mem: 6104\n",
      "Epoch: [69]  [ 60/126]  eta: 0:01:36  lr: 0.000258  loss: 0.2949 (0.2957)  time: 1.3753  data: 0.0003  max mem: 6104\n",
      "Epoch: [69]  [ 80/126]  eta: 0:01:06  lr: 0.000256  loss: 0.2967 (0.2962)  time: 1.3881  data: 0.0003  max mem: 6104\n",
      "Epoch: [69]  [100/126]  eta: 0:00:37  lr: 0.000253  loss: 0.2951 (0.2965)  time: 1.3811  data: 0.0002  max mem: 6104\n",
      "Epoch: [69]  [120/126]  eta: 0:00:08  lr: 0.000251  loss: 0.2897 (0.2961)  time: 1.3752  data: 0.0004  max mem: 6104\n",
      "Epoch: [69]  [125/126]  eta: 0:00:01  lr: 0.000250  loss: 0.2940 (0.2961)  time: 1.3694  data: 0.0004  max mem: 6104\n",
      "Epoch: [69] Total time: 0:02:58 (1.4174 s / it)\n",
      "Averaged stats: lr: 0.000250  loss: 0.2940 (0.2961)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [70]  [  0/126]  eta: 0:10:10  lr: 0.000250  loss: 0.2922 (0.2922)  time: 4.8417  data: 3.2872  max mem: 6104\n",
      "Epoch: [70]  [ 20/126]  eta: 0:02:46  lr: 0.000248  loss: 0.2948 (0.2954)  time: 1.4089  data: 0.0003  max mem: 6104\n",
      "Epoch: [70]  [ 40/126]  eta: 0:02:07  lr: 0.000245  loss: 0.2936 (0.2954)  time: 1.3968  data: 0.0002  max mem: 6104\n",
      "Epoch: [70]  [ 60/126]  eta: 0:01:35  lr: 0.000243  loss: 0.2903 (0.2941)  time: 1.3727  data: 0.0003  max mem: 6104\n",
      "Epoch: [70]  [ 80/126]  eta: 0:01:05  lr: 0.000240  loss: 0.2963 (0.2937)  time: 1.3789  data: 0.0003  max mem: 6104\n",
      "Epoch: [70]  [100/126]  eta: 0:00:36  lr: 0.000238  loss: 0.2959 (0.2945)  time: 1.3734  data: 0.0002  max mem: 6104\n",
      "Epoch: [70]  [120/126]  eta: 0:00:08  lr: 0.000236  loss: 0.2963 (0.2953)  time: 1.3737  data: 0.0004  max mem: 6104\n",
      "Epoch: [70]  [125/126]  eta: 0:00:01  lr: 0.000235  loss: 0.2963 (0.2954)  time: 1.3687  data: 0.0003  max mem: 6104\n",
      "Epoch: [70] Total time: 0:02:57 (1.4113 s / it)\n",
      "Averaged stats: lr: 0.000235  loss: 0.2963 (0.2954)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [71]  [  0/126]  eta: 0:10:32  lr: 0.000235  loss: 0.2930 (0.2930)  time: 5.0191  data: 3.4740  max mem: 6104\n",
      "Epoch: [71]  [ 20/126]  eta: 0:02:47  lr: 0.000233  loss: 0.2950 (0.2967)  time: 1.4069  data: 0.0003  max mem: 6104\n",
      "Epoch: [71]  [ 40/126]  eta: 0:02:07  lr: 0.000230  loss: 0.2941 (0.2959)  time: 1.3759  data: 0.0003  max mem: 6104\n",
      "Epoch: [71]  [ 60/126]  eta: 0:01:35  lr: 0.000228  loss: 0.2944 (0.2960)  time: 1.3712  data: 0.0003  max mem: 6104\n",
      "Epoch: [71]  [ 80/126]  eta: 0:01:06  lr: 0.000226  loss: 0.2918 (0.2951)  time: 1.4273  data: 0.0003  max mem: 6104\n",
      "Epoch: [71]  [100/126]  eta: 0:00:37  lr: 0.000223  loss: 0.2947 (0.2956)  time: 1.3751  data: 0.0002  max mem: 6104\n",
      "Epoch: [71]  [120/126]  eta: 0:00:08  lr: 0.000221  loss: 0.2917 (0.2950)  time: 1.3738  data: 0.0003  max mem: 6104\n",
      "Epoch: [71]  [125/126]  eta: 0:00:01  lr: 0.000221  loss: 0.2917 (0.2949)  time: 1.3684  data: 0.0003  max mem: 6104\n",
      "Epoch: [71] Total time: 0:02:58 (1.4168 s / it)\n",
      "Averaged stats: lr: 0.000221  loss: 0.2917 (0.2949)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [72]  [  0/126]  eta: 0:10:34  lr: 0.000220  loss: 0.3126 (0.3126)  time: 5.0330  data: 3.4217  max mem: 6104\n",
      "Epoch: [72]  [ 20/126]  eta: 0:02:48  lr: 0.000218  loss: 0.2930 (0.2976)  time: 1.4153  data: 0.0005  max mem: 6104\n",
      "Epoch: [72]  [ 40/126]  eta: 0:02:07  lr: 0.000216  loss: 0.2923 (0.2960)  time: 1.3796  data: 0.0003  max mem: 6104\n",
      "Epoch: [72]  [ 60/126]  eta: 0:01:35  lr: 0.000214  loss: 0.2878 (0.2936)  time: 1.3774  data: 0.0003  max mem: 6104\n",
      "Epoch: [72]  [ 80/126]  eta: 0:01:05  lr: 0.000211  loss: 0.2939 (0.2937)  time: 1.3828  data: 0.0003  max mem: 6104\n",
      "Epoch: [72]  [100/126]  eta: 0:00:36  lr: 0.000209  loss: 0.2950 (0.2940)  time: 1.3757  data: 0.0003  max mem: 6104\n",
      "Epoch: [72]  [120/126]  eta: 0:00:08  lr: 0.000207  loss: 0.2904 (0.2939)  time: 1.3904  data: 0.0004  max mem: 6104\n",
      "Epoch: [72]  [125/126]  eta: 0:00:01  lr: 0.000206  loss: 0.2898 (0.2940)  time: 1.3857  data: 0.0004  max mem: 6104\n",
      "Epoch: [72] Total time: 0:02:58 (1.4157 s / it)\n",
      "Averaged stats: lr: 0.000206  loss: 0.2898 (0.2940)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [73]  [  0/126]  eta: 0:09:41  lr: 0.000206  loss: 0.3162 (0.3162)  time: 4.6150  data: 3.0099  max mem: 6104\n",
      "Epoch: [73]  [ 20/126]  eta: 0:02:45  lr: 0.000204  loss: 0.2931 (0.2934)  time: 1.4052  data: 0.0004  max mem: 6104\n",
      "Epoch: [73]  [ 40/126]  eta: 0:02:06  lr: 0.000202  loss: 0.2901 (0.2934)  time: 1.3798  data: 0.0002  max mem: 6104\n",
      "Epoch: [73]  [ 60/126]  eta: 0:01:35  lr: 0.000199  loss: 0.2928 (0.2933)  time: 1.3751  data: 0.0003  max mem: 6104\n",
      "Epoch: [73]  [ 80/126]  eta: 0:01:05  lr: 0.000197  loss: 0.2923 (0.2934)  time: 1.3850  data: 0.0002  max mem: 6104\n",
      "Epoch: [73]  [100/126]  eta: 0:00:36  lr: 0.000195  loss: 0.2919 (0.2930)  time: 1.3859  data: 0.0003  max mem: 6104\n",
      "Epoch: [73]  [120/126]  eta: 0:00:08  lr: 0.000193  loss: 0.2934 (0.2931)  time: 1.3792  data: 0.0004  max mem: 6104\n",
      "Epoch: [73]  [125/126]  eta: 0:00:01  lr: 0.000192  loss: 0.2940 (0.2933)  time: 1.3700  data: 0.0004  max mem: 6104\n",
      "Epoch: [73] Total time: 0:02:57 (1.4101 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 0.2940 (0.2933)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [74]  [  0/126]  eta: 0:10:28  lr: 0.000192  loss: 0.2951 (0.2951)  time: 4.9848  data: 3.4394  max mem: 6104\n",
      "Epoch: [74]  [ 20/126]  eta: 0:02:47  lr: 0.000190  loss: 0.2918 (0.2934)  time: 1.4131  data: 0.0005  max mem: 6104\n",
      "Epoch: [74]  [ 40/126]  eta: 0:02:08  lr: 0.000188  loss: 0.2966 (0.2951)  time: 1.4115  data: 0.0003  max mem: 6104\n",
      "Epoch: [74]  [ 60/126]  eta: 0:01:36  lr: 0.000186  loss: 0.2921 (0.2943)  time: 1.3740  data: 0.0003  max mem: 6104\n",
      "Epoch: [74]  [ 80/126]  eta: 0:01:06  lr: 0.000184  loss: 0.2958 (0.2942)  time: 1.3780  data: 0.0003  max mem: 6104\n",
      "Epoch: [74]  [100/126]  eta: 0:00:37  lr: 0.000181  loss: 0.2877 (0.2935)  time: 1.3825  data: 0.0003  max mem: 6104\n",
      "Epoch: [74]  [120/126]  eta: 0:00:08  lr: 0.000179  loss: 0.2956 (0.2938)  time: 1.3760  data: 0.0003  max mem: 6104\n",
      "Epoch: [74]  [125/126]  eta: 0:00:01  lr: 0.000179  loss: 0.2950 (0.2936)  time: 1.3678  data: 0.0003  max mem: 6104\n",
      "Epoch: [74] Total time: 0:02:58 (1.4175 s / it)\n",
      "Averaged stats: lr: 0.000179  loss: 0.2950 (0.2936)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [75]  [  0/126]  eta: 0:10:08  lr: 0.000179  loss: 0.2732 (0.2732)  time: 4.8317  data: 3.1324  max mem: 6104\n",
      "Epoch: [75]  [ 20/126]  eta: 0:02:47  lr: 0.000176  loss: 0.2894 (0.2887)  time: 1.4128  data: 0.0003  max mem: 6104\n",
      "Epoch: [75]  [ 40/126]  eta: 0:02:07  lr: 0.000174  loss: 0.2926 (0.2892)  time: 1.3798  data: 0.0003  max mem: 6104\n",
      "Epoch: [75]  [ 60/126]  eta: 0:01:35  lr: 0.000172  loss: 0.2986 (0.2921)  time: 1.3763  data: 0.0003  max mem: 6104\n",
      "Epoch: [75]  [ 80/126]  eta: 0:01:06  lr: 0.000170  loss: 0.2921 (0.2922)  time: 1.4188  data: 0.0003  max mem: 6104\n",
      "Epoch: [75]  [100/126]  eta: 0:00:37  lr: 0.000168  loss: 0.2982 (0.2930)  time: 1.3789  data: 0.0002  max mem: 6104\n",
      "Epoch: [75]  [120/126]  eta: 0:00:08  lr: 0.000166  loss: 0.3002 (0.2938)  time: 1.3726  data: 0.0004  max mem: 6104\n",
      "Epoch: [75]  [125/126]  eta: 0:00:01  lr: 0.000166  loss: 0.2978 (0.2934)  time: 1.3662  data: 0.0003  max mem: 6104\n",
      "Epoch: [75] Total time: 0:02:58 (1.4166 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 0.2978 (0.2934)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [76]  [  0/126]  eta: 0:10:55  lr: 0.000165  loss: 0.2998 (0.2998)  time: 5.2006  data: 3.6890  max mem: 6104\n",
      "Epoch: [76]  [ 20/126]  eta: 0:02:48  lr: 0.000163  loss: 0.2884 (0.2891)  time: 1.4135  data: 0.0003  max mem: 6104\n",
      "Epoch: [76]  [ 40/126]  eta: 0:02:07  lr: 0.000161  loss: 0.2942 (0.2916)  time: 1.3767  data: 0.0002  max mem: 6104\n",
      "Epoch: [76]  [ 60/126]  eta: 0:01:35  lr: 0.000159  loss: 0.2934 (0.2928)  time: 1.3741  data: 0.0003  max mem: 6104\n",
      "Epoch: [76]  [ 80/126]  eta: 0:01:05  lr: 0.000157  loss: 0.2893 (0.2926)  time: 1.3847  data: 0.0002  max mem: 6104\n",
      "Epoch: [76]  [100/126]  eta: 0:00:36  lr: 0.000155  loss: 0.2933 (0.2931)  time: 1.3775  data: 0.0003  max mem: 6104\n",
      "Epoch: [76]  [120/126]  eta: 0:00:08  lr: 0.000153  loss: 0.2954 (0.2934)  time: 1.3721  data: 0.0004  max mem: 6104\n",
      "Epoch: [76]  [125/126]  eta: 0:00:01  lr: 0.000153  loss: 0.2933 (0.2932)  time: 1.3972  data: 0.0004  max mem: 6104\n",
      "Epoch: [76] Total time: 0:02:58 (1.4180 s / it)\n",
      "Averaged stats: lr: 0.000153  loss: 0.2933 (0.2932)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [77]  [  0/126]  eta: 0:10:50  lr: 0.000153  loss: 0.3039 (0.3039)  time: 5.1660  data: 3.6237  max mem: 6104\n",
      "Epoch: [77]  [ 20/126]  eta: 0:02:48  lr: 0.000151  loss: 0.2896 (0.2934)  time: 1.4089  data: 0.0003  max mem: 6104\n",
      "Epoch: [77]  [ 40/126]  eta: 0:02:08  lr: 0.000149  loss: 0.2890 (0.2930)  time: 1.3846  data: 0.0002  max mem: 6104\n",
      "Epoch: [77]  [ 60/126]  eta: 0:01:35  lr: 0.000147  loss: 0.2898 (0.2939)  time: 1.3747  data: 0.0002  max mem: 6104\n",
      "Epoch: [77]  [ 80/126]  eta: 0:01:06  lr: 0.000145  loss: 0.2840 (0.2930)  time: 1.3858  data: 0.0003  max mem: 6104\n",
      "Epoch: [77]  [100/126]  eta: 0:00:37  lr: 0.000143  loss: 0.2927 (0.2930)  time: 1.3815  data: 0.0003  max mem: 6104\n",
      "Epoch: [77]  [120/126]  eta: 0:00:08  lr: 0.000141  loss: 0.2910 (0.2929)  time: 1.3731  data: 0.0004  max mem: 6104\n",
      "Epoch: [77]  [125/126]  eta: 0:00:01  lr: 0.000140  loss: 0.2878 (0.2927)  time: 1.3680  data: 0.0003  max mem: 6104\n",
      "Epoch: [77] Total time: 0:02:58 (1.4144 s / it)\n",
      "Averaged stats: lr: 0.000140  loss: 0.2878 (0.2927)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [78]  [  0/126]  eta: 0:11:28  lr: 0.000140  loss: 0.3068 (0.3068)  time: 5.4634  data: 3.8972  max mem: 6104\n",
      "Epoch: [78]  [ 20/126]  eta: 0:02:50  lr: 0.000138  loss: 0.2937 (0.2941)  time: 1.4109  data: 0.0005  max mem: 6104\n",
      "Epoch: [78]  [ 40/126]  eta: 0:02:08  lr: 0.000137  loss: 0.2884 (0.2921)  time: 1.3808  data: 0.0003  max mem: 6104\n",
      "Epoch: [78]  [ 60/126]  eta: 0:01:36  lr: 0.000135  loss: 0.2907 (0.2918)  time: 1.4140  data: 0.0003  max mem: 6104\n",
      "Epoch: [78]  [ 80/126]  eta: 0:01:06  lr: 0.000133  loss: 0.2944 (0.2928)  time: 1.3838  data: 0.0003  max mem: 6104\n",
      "Epoch: [78]  [100/126]  eta: 0:00:37  lr: 0.000131  loss: 0.2864 (0.2927)  time: 1.3786  data: 0.0003  max mem: 6104\n",
      "Epoch: [78]  [120/126]  eta: 0:00:08  lr: 0.000129  loss: 0.2873 (0.2926)  time: 1.3773  data: 0.0003  max mem: 6104\n",
      "Epoch: [78]  [125/126]  eta: 0:00:01  lr: 0.000129  loss: 0.2873 (0.2926)  time: 1.3696  data: 0.0003  max mem: 6104\n",
      "Epoch: [78] Total time: 0:02:59 (1.4227 s / it)\n",
      "Averaged stats: lr: 0.000129  loss: 0.2873 (0.2926)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [79]  [  0/126]  eta: 0:10:46  lr: 0.000128  loss: 0.3002 (0.3002)  time: 5.1324  data: 3.5283  max mem: 6104\n",
      "Epoch: [79]  [ 20/126]  eta: 0:02:48  lr: 0.000127  loss: 0.2847 (0.2873)  time: 1.4109  data: 0.0004  max mem: 6104\n",
      "Epoch: [79]  [ 40/126]  eta: 0:02:07  lr: 0.000125  loss: 0.2939 (0.2894)  time: 1.3828  data: 0.0003  max mem: 6104\n",
      "Epoch: [79]  [ 60/126]  eta: 0:01:35  lr: 0.000123  loss: 0.2926 (0.2904)  time: 1.3757  data: 0.0002  max mem: 6104\n",
      "Epoch: [79]  [ 80/126]  eta: 0:01:05  lr: 0.000121  loss: 0.2908 (0.2909)  time: 1.3802  data: 0.0003  max mem: 6104\n",
      "Epoch: [79]  [100/126]  eta: 0:00:36  lr: 0.000119  loss: 0.2901 (0.2910)  time: 1.3771  data: 0.0002  max mem: 6104\n",
      "Epoch: [79]  [120/126]  eta: 0:00:08  lr: 0.000118  loss: 0.2905 (0.2911)  time: 1.3994  data: 0.0003  max mem: 6104\n",
      "Epoch: [79]  [125/126]  eta: 0:00:01  lr: 0.000117  loss: 0.2987 (0.2918)  time: 1.3716  data: 0.0003  max mem: 6104\n",
      "Epoch: [79] Total time: 0:02:58 (1.4169 s / it)\n",
      "Averaged stats: lr: 0.000117  loss: 0.2987 (0.2918)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [80]  [  0/126]  eta: 0:10:05  lr: 0.000117  loss: 0.2915 (0.2915)  time: 4.8040  data: 3.1828  max mem: 6104\n",
      "Epoch: [80]  [ 20/126]  eta: 0:02:47  lr: 0.000115  loss: 0.2920 (0.2943)  time: 1.4172  data: 0.0004  max mem: 6104\n",
      "Epoch: [80]  [ 40/126]  eta: 0:02:07  lr: 0.000113  loss: 0.2943 (0.2944)  time: 1.3880  data: 0.0003  max mem: 6104\n",
      "Epoch: [80]  [ 60/126]  eta: 0:01:35  lr: 0.000112  loss: 0.2850 (0.2922)  time: 1.3852  data: 0.0004  max mem: 6104\n",
      "Epoch: [80]  [ 80/126]  eta: 0:01:06  lr: 0.000110  loss: 0.2876 (0.2913)  time: 1.3809  data: 0.0002  max mem: 6104\n",
      "Epoch: [80]  [100/126]  eta: 0:00:37  lr: 0.000108  loss: 0.2952 (0.2919)  time: 1.3800  data: 0.0003  max mem: 6104\n",
      "Epoch: [80]  [120/126]  eta: 0:00:08  lr: 0.000107  loss: 0.2898 (0.2922)  time: 1.3795  data: 0.0004  max mem: 6104\n",
      "Epoch: [80]  [125/126]  eta: 0:00:01  lr: 0.000106  loss: 0.2898 (0.2921)  time: 1.3727  data: 0.0004  max mem: 6104\n",
      "Epoch: [80] Total time: 0:02:58 (1.4153 s / it)\n",
      "Averaged stats: lr: 0.000106  loss: 0.2898 (0.2921)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [81]  [  0/126]  eta: 0:10:11  lr: 0.000106  loss: 0.2970 (0.2970)  time: 4.8501  data: 3.2848  max mem: 6104\n",
      "Epoch: [81]  [ 20/126]  eta: 0:02:51  lr: 0.000104  loss: 0.2899 (0.2914)  time: 1.4553  data: 0.0005  max mem: 6104\n",
      "Epoch: [81]  [ 40/126]  eta: 0:02:09  lr: 0.000103  loss: 0.2915 (0.2922)  time: 1.3858  data: 0.0003  max mem: 6104\n",
      "Epoch: [81]  [ 60/126]  eta: 0:01:36  lr: 0.000101  loss: 0.2924 (0.2930)  time: 1.3716  data: 0.0003  max mem: 6104\n",
      "Epoch: [81]  [ 80/126]  eta: 0:01:06  lr: 0.000099  loss: 0.2947 (0.2932)  time: 1.3871  data: 0.0003  max mem: 6104\n",
      "Epoch: [81]  [100/126]  eta: 0:00:37  lr: 0.000098  loss: 0.2873 (0.2923)  time: 1.3902  data: 0.0002  max mem: 6104\n",
      "Epoch: [81]  [120/126]  eta: 0:00:08  lr: 0.000096  loss: 0.2879 (0.2919)  time: 1.3696  data: 0.0004  max mem: 6104\n",
      "Epoch: [81]  [125/126]  eta: 0:00:01  lr: 0.000096  loss: 0.2886 (0.2918)  time: 1.3606  data: 0.0004  max mem: 6104\n",
      "Epoch: [81] Total time: 0:02:58 (1.4200 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.2886 (0.2918)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [82]  [  0/126]  eta: 0:10:22  lr: 0.000095  loss: 0.2931 (0.2931)  time: 4.9408  data: 3.3349  max mem: 6104\n",
      "Epoch: [82]  [ 20/126]  eta: 0:02:47  lr: 0.000094  loss: 0.2906 (0.2923)  time: 1.4116  data: 0.0003  max mem: 6104\n",
      "Epoch: [82]  [ 40/126]  eta: 0:02:07  lr: 0.000092  loss: 0.2893 (0.2914)  time: 1.3819  data: 0.0002  max mem: 6104\n",
      "Epoch: [82]  [ 60/126]  eta: 0:01:35  lr: 0.000091  loss: 0.2887 (0.2905)  time: 1.3728  data: 0.0003  max mem: 6104\n",
      "Epoch: [82]  [ 80/126]  eta: 0:01:06  lr: 0.000089  loss: 0.2835 (0.2892)  time: 1.4126  data: 0.0003  max mem: 6104\n",
      "Epoch: [82]  [100/126]  eta: 0:00:37  lr: 0.000088  loss: 0.2858 (0.2896)  time: 1.3809  data: 0.0002  max mem: 6104\n",
      "Epoch: [82]  [120/126]  eta: 0:00:08  lr: 0.000086  loss: 0.2924 (0.2908)  time: 1.3823  data: 0.0003  max mem: 6104\n",
      "Epoch: [82]  [125/126]  eta: 0:00:01  lr: 0.000086  loss: 0.2959 (0.2911)  time: 1.3754  data: 0.0003  max mem: 6104\n",
      "Epoch: [82] Total time: 0:02:58 (1.4178 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2959 (0.2911)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [83]  [  0/126]  eta: 0:09:57  lr: 0.000085  loss: 0.2875 (0.2875)  time: 4.7392  data: 3.1887  max mem: 6104\n",
      "Epoch: [83]  [ 20/126]  eta: 0:02:46  lr: 0.000084  loss: 0.2891 (0.2911)  time: 1.4094  data: 0.0004  max mem: 6104\n",
      "Epoch: [83]  [ 40/126]  eta: 0:02:07  lr: 0.000082  loss: 0.2906 (0.2912)  time: 1.3857  data: 0.0003  max mem: 6104\n",
      "Epoch: [83]  [ 60/126]  eta: 0:01:35  lr: 0.000081  loss: 0.2958 (0.2920)  time: 1.3700  data: 0.0003  max mem: 6104\n",
      "Epoch: [83]  [ 80/126]  eta: 0:01:05  lr: 0.000079  loss: 0.2903 (0.2918)  time: 1.3791  data: 0.0003  max mem: 6104\n",
      "Epoch: [83]  [100/126]  eta: 0:00:36  lr: 0.000078  loss: 0.2859 (0.2908)  time: 1.3795  data: 0.0003  max mem: 6104\n",
      "Epoch: [83]  [120/126]  eta: 0:00:08  lr: 0.000076  loss: 0.2909 (0.2911)  time: 1.3786  data: 0.0003  max mem: 6104\n",
      "Epoch: [83]  [125/126]  eta: 0:00:01  lr: 0.000076  loss: 0.2902 (0.2909)  time: 1.3676  data: 0.0003  max mem: 6104\n",
      "Epoch: [83] Total time: 0:02:57 (1.4099 s / it)\n",
      "Averaged stats: lr: 0.000076  loss: 0.2902 (0.2909)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [84]  [  0/126]  eta: 0:11:07  lr: 0.000076  loss: 0.2812 (0.2812)  time: 5.2978  data: 3.6542  max mem: 6104\n",
      "Epoch: [84]  [ 20/126]  eta: 0:02:47  lr: 0.000075  loss: 0.2922 (0.2930)  time: 1.3980  data: 0.0003  max mem: 6104\n",
      "Epoch: [84]  [ 40/126]  eta: 0:02:09  lr: 0.000073  loss: 0.2916 (0.2924)  time: 1.4155  data: 0.0003  max mem: 6104\n",
      "Epoch: [84]  [ 60/126]  eta: 0:01:36  lr: 0.000072  loss: 0.2887 (0.2920)  time: 1.3789  data: 0.0003  max mem: 6104\n",
      "Epoch: [84]  [ 80/126]  eta: 0:01:06  lr: 0.000070  loss: 0.2923 (0.2927)  time: 1.3733  data: 0.0003  max mem: 6104\n",
      "Epoch: [84]  [100/126]  eta: 0:00:37  lr: 0.000069  loss: 0.2871 (0.2921)  time: 1.3802  data: 0.0002  max mem: 6104\n",
      "Epoch: [84]  [120/126]  eta: 0:00:08  lr: 0.000067  loss: 0.2912 (0.2923)  time: 1.3731  data: 0.0005  max mem: 6104\n",
      "Epoch: [84]  [125/126]  eta: 0:00:01  lr: 0.000067  loss: 0.2884 (0.2921)  time: 1.3631  data: 0.0005  max mem: 6104\n",
      "Epoch: [84] Total time: 0:02:58 (1.4167 s / it)\n",
      "Averaged stats: lr: 0.000067  loss: 0.2884 (0.2921)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [85]  [  0/126]  eta: 0:10:20  lr: 0.000067  loss: 0.3009 (0.3009)  time: 4.9271  data: 3.3657  max mem: 6104\n",
      "Epoch: [85]  [ 20/126]  eta: 0:02:46  lr: 0.000066  loss: 0.2946 (0.2950)  time: 1.4060  data: 0.0003  max mem: 6104\n",
      "Epoch: [85]  [ 40/126]  eta: 0:02:07  lr: 0.000064  loss: 0.2856 (0.2924)  time: 1.3769  data: 0.0002  max mem: 6104\n",
      "Epoch: [85]  [ 60/126]  eta: 0:01:35  lr: 0.000063  loss: 0.2806 (0.2900)  time: 1.3697  data: 0.0002  max mem: 6104\n",
      "Epoch: [85]  [ 80/126]  eta: 0:01:05  lr: 0.000062  loss: 0.2954 (0.2917)  time: 1.3911  data: 0.0003  max mem: 6104\n",
      "Epoch: [85]  [100/126]  eta: 0:00:36  lr: 0.000060  loss: 0.2956 (0.2926)  time: 1.3735  data: 0.0003  max mem: 6104\n",
      "Epoch: [85]  [120/126]  eta: 0:00:08  lr: 0.000059  loss: 0.2887 (0.2916)  time: 1.3773  data: 0.0004  max mem: 6104\n",
      "Epoch: [85]  [125/126]  eta: 0:00:01  lr: 0.000059  loss: 0.2891 (0.2914)  time: 1.3700  data: 0.0003  max mem: 6104\n",
      "Epoch: [85] Total time: 0:02:57 (1.4105 s / it)\n",
      "Averaged stats: lr: 0.000059  loss: 0.2891 (0.2914)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [86]  [  0/126]  eta: 0:11:59  lr: 0.000059  loss: 0.2942 (0.2942)  time: 5.7114  data: 4.1728  max mem: 6104\n",
      "Epoch: [86]  [ 20/126]  eta: 0:02:50  lr: 0.000057  loss: 0.2905 (0.2897)  time: 1.4020  data: 0.0005  max mem: 6104\n",
      "Epoch: [86]  [ 40/126]  eta: 0:02:08  lr: 0.000056  loss: 0.2886 (0.2901)  time: 1.3748  data: 0.0003  max mem: 6104\n",
      "Epoch: [86]  [ 60/126]  eta: 0:01:36  lr: 0.000055  loss: 0.2851 (0.2890)  time: 1.3773  data: 0.0003  max mem: 6104\n",
      "Epoch: [86]  [ 80/126]  eta: 0:01:06  lr: 0.000053  loss: 0.2952 (0.2904)  time: 1.3827  data: 0.0002  max mem: 6104\n",
      "Epoch: [86]  [100/126]  eta: 0:00:37  lr: 0.000052  loss: 0.2882 (0.2903)  time: 1.3781  data: 0.0003  max mem: 6104\n",
      "Epoch: [86]  [120/126]  eta: 0:00:08  lr: 0.000051  loss: 0.2923 (0.2906)  time: 1.3782  data: 0.0003  max mem: 6104\n",
      "Epoch: [86]  [125/126]  eta: 0:00:01  lr: 0.000051  loss: 0.2923 (0.2908)  time: 1.3704  data: 0.0003  max mem: 6104\n",
      "Epoch: [86] Total time: 0:02:58 (1.4162 s / it)\n",
      "Averaged stats: lr: 0.000051  loss: 0.2923 (0.2908)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [87]  [  0/126]  eta: 0:10:14  lr: 0.000051  loss: 0.2815 (0.2815)  time: 4.8740  data: 3.2677  max mem: 6104\n",
      "Epoch: [87]  [ 20/126]  eta: 0:02:46  lr: 0.000049  loss: 0.2922 (0.2929)  time: 1.4105  data: 0.0004  max mem: 6104\n",
      "Epoch: [87]  [ 40/126]  eta: 0:02:08  lr: 0.000048  loss: 0.2876 (0.2911)  time: 1.4131  data: 0.0003  max mem: 6104\n",
      "Epoch: [87]  [ 60/126]  eta: 0:01:36  lr: 0.000047  loss: 0.2922 (0.2919)  time: 1.3765  data: 0.0003  max mem: 6104\n",
      "Epoch: [87]  [ 80/126]  eta: 0:01:06  lr: 0.000046  loss: 0.2849 (0.2915)  time: 1.3812  data: 0.0002  max mem: 6104\n",
      "Epoch: [87]  [100/126]  eta: 0:00:37  lr: 0.000045  loss: 0.2870 (0.2909)  time: 1.3749  data: 0.0003  max mem: 6104\n",
      "Epoch: [87]  [120/126]  eta: 0:00:08  lr: 0.000044  loss: 0.2899 (0.2906)  time: 1.3777  data: 0.0003  max mem: 6104\n",
      "Epoch: [87]  [125/126]  eta: 0:00:01  lr: 0.000043  loss: 0.2872 (0.2904)  time: 1.3653  data: 0.0003  max mem: 6104\n",
      "Epoch: [87] Total time: 0:02:58 (1.4163 s / it)\n",
      "Averaged stats: lr: 0.000043  loss: 0.2872 (0.2904)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [88]  [  0/126]  eta: 0:11:51  lr: 0.000043  loss: 0.2862 (0.2862)  time: 5.6451  data: 4.1109  max mem: 6104\n",
      "Epoch: [88]  [ 20/126]  eta: 0:02:50  lr: 0.000042  loss: 0.2943 (0.2939)  time: 1.4088  data: 0.0003  max mem: 6104\n",
      "Epoch: [88]  [ 40/126]  eta: 0:02:08  lr: 0.000041  loss: 0.2899 (0.2924)  time: 1.3747  data: 0.0002  max mem: 6104\n",
      "Epoch: [88]  [ 60/126]  eta: 0:01:36  lr: 0.000040  loss: 0.2897 (0.2910)  time: 1.3778  data: 0.0002  max mem: 6104\n",
      "Epoch: [88]  [ 80/126]  eta: 0:01:06  lr: 0.000039  loss: 0.2880 (0.2900)  time: 1.4004  data: 0.0003  max mem: 6104\n",
      "Epoch: [88]  [100/126]  eta: 0:00:37  lr: 0.000038  loss: 0.2895 (0.2899)  time: 1.3753  data: 0.0003  max mem: 6104\n",
      "Epoch: [88]  [120/126]  eta: 0:00:08  lr: 0.000037  loss: 0.2905 (0.2901)  time: 1.3783  data: 0.0004  max mem: 6104\n",
      "Epoch: [88]  [125/126]  eta: 0:00:01  lr: 0.000036  loss: 0.2898 (0.2901)  time: 1.3725  data: 0.0003  max mem: 6104\n",
      "Epoch: [88] Total time: 0:02:58 (1.4195 s / it)\n",
      "Averaged stats: lr: 0.000036  loss: 0.2898 (0.2901)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [89]  [  0/126]  eta: 0:10:49  lr: 0.000036  loss: 0.2755 (0.2755)  time: 5.1570  data: 3.5084  max mem: 6104\n",
      "Epoch: [89]  [ 20/126]  eta: 0:02:48  lr: 0.000035  loss: 0.2880 (0.2897)  time: 1.4079  data: 0.0006  max mem: 6104\n",
      "Epoch: [89]  [ 40/126]  eta: 0:02:07  lr: 0.000034  loss: 0.2903 (0.2902)  time: 1.3806  data: 0.0003  max mem: 6104\n",
      "Epoch: [89]  [ 60/126]  eta: 0:01:35  lr: 0.000033  loss: 0.2929 (0.2907)  time: 1.3706  data: 0.0003  max mem: 6104\n",
      "Epoch: [89]  [ 80/126]  eta: 0:01:05  lr: 0.000032  loss: 0.2887 (0.2906)  time: 1.3881  data: 0.0003  max mem: 6104\n",
      "Epoch: [89]  [100/126]  eta: 0:00:36  lr: 0.000031  loss: 0.2897 (0.2901)  time: 1.3726  data: 0.0002  max mem: 6104\n",
      "Epoch: [89]  [120/126]  eta: 0:00:08  lr: 0.000030  loss: 0.2872 (0.2898)  time: 1.3966  data: 0.0004  max mem: 6104\n",
      "Epoch: [89]  [125/126]  eta: 0:00:01  lr: 0.000030  loss: 0.2888 (0.2901)  time: 1.3857  data: 0.0004  max mem: 6104\n",
      "Epoch: [89] Total time: 0:02:58 (1.4162 s / it)\n",
      "Averaged stats: lr: 0.000030  loss: 0.2888 (0.2901)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [90]  [  0/126]  eta: 0:11:02  lr: 0.000030  loss: 0.3015 (0.3015)  time: 5.2543  data: 3.7153  max mem: 6104\n",
      "Epoch: [90]  [ 20/126]  eta: 0:02:48  lr: 0.000029  loss: 0.2878 (0.2863)  time: 1.4062  data: 0.0003  max mem: 6104\n",
      "Epoch: [90]  [ 40/126]  eta: 0:02:07  lr: 0.000028  loss: 0.2883 (0.2881)  time: 1.3811  data: 0.0003  max mem: 6104\n",
      "Epoch: [90]  [ 60/126]  eta: 0:01:35  lr: 0.000027  loss: 0.2860 (0.2879)  time: 1.3723  data: 0.0002  max mem: 6104\n",
      "Epoch: [90]  [ 80/126]  eta: 0:01:05  lr: 0.000026  loss: 0.2925 (0.2889)  time: 1.3819  data: 0.0003  max mem: 6104\n",
      "Epoch: [90]  [100/126]  eta: 0:00:36  lr: 0.000026  loss: 0.2901 (0.2894)  time: 1.3750  data: 0.0003  max mem: 6104\n",
      "Epoch: [90]  [120/126]  eta: 0:00:08  lr: 0.000025  loss: 0.2954 (0.2902)  time: 1.3796  data: 0.0004  max mem: 6104\n",
      "Epoch: [90]  [125/126]  eta: 0:00:01  lr: 0.000025  loss: 0.2954 (0.2903)  time: 1.3741  data: 0.0003  max mem: 6104\n",
      "Epoch: [90] Total time: 0:02:58 (1.4132 s / it)\n",
      "Averaged stats: lr: 0.000025  loss: 0.2954 (0.2903)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [91]  [  0/126]  eta: 0:10:33  lr: 0.000024  loss: 0.2830 (0.2830)  time: 5.0243  data: 3.4963  max mem: 6104\n",
      "Epoch: [91]  [ 20/126]  eta: 0:02:48  lr: 0.000024  loss: 0.2878 (0.2892)  time: 1.4196  data: 0.0004  max mem: 6104\n",
      "Epoch: [91]  [ 40/126]  eta: 0:02:09  lr: 0.000023  loss: 0.2851 (0.2881)  time: 1.4093  data: 0.0003  max mem: 6104\n",
      "Epoch: [91]  [ 60/126]  eta: 0:01:36  lr: 0.000022  loss: 0.2938 (0.2895)  time: 1.3742  data: 0.0003  max mem: 6104\n",
      "Epoch: [91]  [ 80/126]  eta: 0:01:06  lr: 0.000021  loss: 0.2920 (0.2901)  time: 1.3814  data: 0.0002  max mem: 6104\n",
      "Epoch: [91]  [100/126]  eta: 0:00:37  lr: 0.000020  loss: 0.2898 (0.2905)  time: 1.3798  data: 0.0003  max mem: 6104\n",
      "Epoch: [91]  [120/126]  eta: 0:00:08  lr: 0.000020  loss: 0.2882 (0.2898)  time: 1.3748  data: 0.0003  max mem: 6104\n",
      "Epoch: [91]  [125/126]  eta: 0:00:01  lr: 0.000019  loss: 0.2866 (0.2897)  time: 1.3672  data: 0.0003  max mem: 6104\n",
      "Epoch: [91] Total time: 0:02:58 (1.4182 s / it)\n",
      "Averaged stats: lr: 0.000019  loss: 0.2866 (0.2897)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [92]  [  0/126]  eta: 0:10:05  lr: 0.000019  loss: 0.3044 (0.3044)  time: 4.8034  data: 3.2332  max mem: 6104\n",
      "Epoch: [92]  [ 20/126]  eta: 0:02:46  lr: 0.000019  loss: 0.2899 (0.2917)  time: 1.4119  data: 0.0006  max mem: 6104\n",
      "Epoch: [92]  [ 40/126]  eta: 0:02:07  lr: 0.000018  loss: 0.2892 (0.2896)  time: 1.3765  data: 0.0003  max mem: 6104\n",
      "Epoch: [92]  [ 60/126]  eta: 0:01:35  lr: 0.000017  loss: 0.2932 (0.2903)  time: 1.3783  data: 0.0002  max mem: 6104\n",
      "Epoch: [92]  [ 80/126]  eta: 0:01:05  lr: 0.000016  loss: 0.2888 (0.2904)  time: 1.3809  data: 0.0003  max mem: 6104\n",
      "Epoch: [92]  [100/126]  eta: 0:00:37  lr: 0.000016  loss: 0.2887 (0.2901)  time: 1.4185  data: 0.0003  max mem: 6104\n",
      "Epoch: [92]  [120/126]  eta: 0:00:08  lr: 0.000015  loss: 0.2867 (0.2900)  time: 1.3806  data: 0.0003  max mem: 6104\n",
      "Epoch: [92]  [125/126]  eta: 0:00:01  lr: 0.000015  loss: 0.2863 (0.2898)  time: 1.3736  data: 0.0003  max mem: 6104\n",
      "Epoch: [92] Total time: 0:02:58 (1.4177 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.2863 (0.2898)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [93]  [  0/126]  eta: 0:10:32  lr: 0.000015  loss: 0.2781 (0.2781)  time: 5.0221  data: 3.4789  max mem: 6104\n",
      "Epoch: [93]  [ 20/126]  eta: 0:02:47  lr: 0.000014  loss: 0.2867 (0.2903)  time: 1.4118  data: 0.0003  max mem: 6104\n",
      "Epoch: [93]  [ 40/126]  eta: 0:02:07  lr: 0.000014  loss: 0.2933 (0.2923)  time: 1.3844  data: 0.0003  max mem: 6104\n",
      "Epoch: [93]  [ 60/126]  eta: 0:01:35  lr: 0.000013  loss: 0.2883 (0.2910)  time: 1.3800  data: 0.0003  max mem: 6104\n",
      "Epoch: [93]  [ 80/126]  eta: 0:01:05  lr: 0.000012  loss: 0.2840 (0.2903)  time: 1.3824  data: 0.0003  max mem: 6104\n",
      "Epoch: [93]  [100/126]  eta: 0:00:36  lr: 0.000012  loss: 0.2879 (0.2900)  time: 1.3756  data: 0.0003  max mem: 6104\n",
      "Epoch: [93]  [120/126]  eta: 0:00:08  lr: 0.000011  loss: 0.2887 (0.2902)  time: 1.3749  data: 0.0005  max mem: 6104\n",
      "Epoch: [93]  [125/126]  eta: 0:00:01  lr: 0.000011  loss: 0.2907 (0.2903)  time: 1.3690  data: 0.0005  max mem: 6104\n",
      "Epoch: [93] Total time: 0:02:58 (1.4136 s / it)\n",
      "Averaged stats: lr: 0.000011  loss: 0.2907 (0.2903)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [94]  [  0/126]  eta: 0:09:56  lr: 0.000011  loss: 0.2905 (0.2905)  time: 4.7345  data: 3.1609  max mem: 6104\n",
      "Epoch: [94]  [ 20/126]  eta: 0:02:46  lr: 0.000010  loss: 0.2940 (0.2922)  time: 1.4110  data: 0.0003  max mem: 6104\n",
      "Epoch: [94]  [ 40/126]  eta: 0:02:08  lr: 0.000010  loss: 0.2865 (0.2903)  time: 1.4114  data: 0.0003  max mem: 6104\n",
      "Epoch: [94]  [ 60/126]  eta: 0:01:35  lr: 0.000009  loss: 0.2894 (0.2904)  time: 1.3755  data: 0.0003  max mem: 6104\n",
      "Epoch: [94]  [ 80/126]  eta: 0:01:06  lr: 0.000009  loss: 0.2848 (0.2898)  time: 1.3813  data: 0.0003  max mem: 6104\n",
      "Epoch: [94]  [100/126]  eta: 0:00:37  lr: 0.000008  loss: 0.2881 (0.2895)  time: 1.3818  data: 0.0002  max mem: 6104\n",
      "Epoch: [94]  [120/126]  eta: 0:00:08  lr: 0.000008  loss: 0.2886 (0.2896)  time: 1.3788  data: 0.0004  max mem: 6104\n",
      "Epoch: [94]  [125/126]  eta: 0:00:01  lr: 0.000008  loss: 0.2886 (0.2898)  time: 1.3723  data: 0.0003  max mem: 6104\n",
      "Epoch: [94] Total time: 0:02:58 (1.4161 s / it)\n",
      "Averaged stats: lr: 0.000008  loss: 0.2886 (0.2898)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [95]  [  0/126]  eta: 0:10:52  lr: 0.000008  loss: 0.3103 (0.3103)  time: 5.1793  data: 3.5483  max mem: 6104\n",
      "Epoch: [95]  [ 20/126]  eta: 0:02:50  lr: 0.000007  loss: 0.2900 (0.2909)  time: 1.4267  data: 0.0004  max mem: 6104\n",
      "Epoch: [95]  [ 40/126]  eta: 0:02:08  lr: 0.000007  loss: 0.2946 (0.2937)  time: 1.3875  data: 0.0003  max mem: 6104\n",
      "Epoch: [95]  [ 60/126]  eta: 0:01:36  lr: 0.000006  loss: 0.2867 (0.2925)  time: 1.3809  data: 0.0003  max mem: 6104\n",
      "Epoch: [95]  [ 80/126]  eta: 0:01:06  lr: 0.000006  loss: 0.2804 (0.2907)  time: 1.3906  data: 0.0003  max mem: 6104\n",
      "Epoch: [95]  [100/126]  eta: 0:00:37  lr: 0.000005  loss: 0.2868 (0.2904)  time: 1.4010  data: 0.0003  max mem: 6104\n",
      "Epoch: [95]  [120/126]  eta: 0:00:08  lr: 0.000005  loss: 0.2828 (0.2895)  time: 1.3776  data: 0.0004  max mem: 6104\n",
      "Epoch: [95]  [125/126]  eta: 0:00:01  lr: 0.000005  loss: 0.2852 (0.2897)  time: 1.3694  data: 0.0004  max mem: 6104\n",
      "Epoch: [95] Total time: 0:02:59 (1.4234 s / it)\n",
      "Averaged stats: lr: 0.000005  loss: 0.2852 (0.2897)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [96]  [  0/126]  eta: 0:10:52  lr: 0.000005  loss: 0.2908 (0.2908)  time: 5.1748  data: 3.6123  max mem: 6104\n",
      "Epoch: [96]  [ 20/126]  eta: 0:02:48  lr: 0.000004  loss: 0.2911 (0.2903)  time: 1.4141  data: 0.0004  max mem: 6104\n",
      "Epoch: [96]  [ 40/126]  eta: 0:02:08  lr: 0.000004  loss: 0.2909 (0.2910)  time: 1.3916  data: 0.0002  max mem: 6104\n",
      "Epoch: [96]  [ 60/126]  eta: 0:01:36  lr: 0.000004  loss: 0.2886 (0.2893)  time: 1.3772  data: 0.0003  max mem: 6104\n",
      "Epoch: [96]  [ 80/126]  eta: 0:01:06  lr: 0.000003  loss: 0.2839 (0.2889)  time: 1.3923  data: 0.0003  max mem: 6104\n",
      "Epoch: [96]  [100/126]  eta: 0:00:37  lr: 0.000003  loss: 0.2905 (0.2888)  time: 1.3819  data: 0.0002  max mem: 6104\n",
      "Epoch: [96]  [120/126]  eta: 0:00:08  lr: 0.000003  loss: 0.2893 (0.2886)  time: 1.3787  data: 0.0003  max mem: 6104\n",
      "Epoch: [96]  [125/126]  eta: 0:00:01  lr: 0.000003  loss: 0.2910 (0.2887)  time: 1.3738  data: 0.0003  max mem: 6104\n",
      "Epoch: [96] Total time: 0:02:58 (1.4189 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.2910 (0.2887)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [97]  [  0/126]  eta: 0:09:53  lr: 0.000003  loss: 0.2989 (0.2989)  time: 4.7074  data: 3.1592  max mem: 6104\n",
      "Epoch: [97]  [ 20/126]  eta: 0:02:47  lr: 0.000002  loss: 0.2878 (0.2882)  time: 1.4245  data: 0.0006  max mem: 6104\n",
      "Epoch: [97]  [ 40/126]  eta: 0:02:09  lr: 0.000002  loss: 0.2898 (0.2889)  time: 1.4209  data: 0.0002  max mem: 6104\n",
      "Epoch: [97]  [ 60/126]  eta: 0:01:36  lr: 0.000002  loss: 0.2850 (0.2892)  time: 1.3811  data: 0.0003  max mem: 6104\n",
      "Epoch: [97]  [ 80/126]  eta: 0:01:06  lr: 0.000002  loss: 0.2938 (0.2902)  time: 1.3864  data: 0.0003  max mem: 6104\n",
      "Epoch: [97]  [100/126]  eta: 0:00:37  lr: 0.000001  loss: 0.2886 (0.2902)  time: 1.3852  data: 0.0003  max mem: 6104\n",
      "Epoch: [97]  [120/126]  eta: 0:00:08  lr: 0.000001  loss: 0.2883 (0.2902)  time: 1.3768  data: 0.0004  max mem: 6104\n",
      "Epoch: [97]  [125/126]  eta: 0:00:01  lr: 0.000001  loss: 0.2869 (0.2899)  time: 1.3685  data: 0.0003  max mem: 6104\n",
      "Epoch: [97] Total time: 0:02:59 (1.4215 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.2869 (0.2899)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [98]  [  0/126]  eta: 0:09:54  lr: 0.000001  loss: 0.3021 (0.3021)  time: 4.7168  data: 3.0894  max mem: 6104\n",
      "Epoch: [98]  [ 20/126]  eta: 0:02:47  lr: 0.000001  loss: 0.2895 (0.2899)  time: 1.4193  data: 0.0003  max mem: 6104\n",
      "Epoch: [98]  [ 40/126]  eta: 0:02:07  lr: 0.000001  loss: 0.2922 (0.2915)  time: 1.3838  data: 0.0003  max mem: 6104\n",
      "Epoch: [98]  [ 60/126]  eta: 0:01:35  lr: 0.000001  loss: 0.2927 (0.2914)  time: 1.3803  data: 0.0003  max mem: 6104\n",
      "Epoch: [98]  [ 80/126]  eta: 0:01:05  lr: 0.000001  loss: 0.2877 (0.2907)  time: 1.3818  data: 0.0003  max mem: 6104\n",
      "Epoch: [98]  [100/126]  eta: 0:00:37  lr: 0.000000  loss: 0.2922 (0.2904)  time: 1.4201  data: 0.0002  max mem: 6104\n",
      "Epoch: [98]  [120/126]  eta: 0:00:08  lr: 0.000000  loss: 0.2917 (0.2906)  time: 1.3750  data: 0.0003  max mem: 6104\n",
      "Epoch: [98]  [125/126]  eta: 0:00:01  lr: 0.000000  loss: 0.2914 (0.2904)  time: 1.3688  data: 0.0003  max mem: 6104\n",
      "Epoch: [98] Total time: 0:02:58 (1.4192 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.2914 (0.2904)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [99]  [  0/126]  eta: 0:09:52  lr: 0.000000  loss: 0.2859 (0.2859)  time: 4.6988  data: 3.0968  max mem: 6104\n",
      "Epoch: [99]  [ 20/126]  eta: 0:02:46  lr: 0.000000  loss: 0.2819 (0.2853)  time: 1.4144  data: 0.0007  max mem: 6104\n",
      "Epoch: [99]  [ 40/126]  eta: 0:02:06  lr: 0.000000  loss: 0.2942 (0.2896)  time: 1.3767  data: 0.0002  max mem: 6104\n",
      "Epoch: [99]  [ 60/126]  eta: 0:01:35  lr: 0.000000  loss: 0.2853 (0.2879)  time: 1.3790  data: 0.0003  max mem: 6104\n",
      "Epoch: [99]  [ 80/126]  eta: 0:01:05  lr: 0.000000  loss: 0.2892 (0.2890)  time: 1.3814  data: 0.0003  max mem: 6104\n",
      "Epoch: [99]  [100/126]  eta: 0:00:36  lr: 0.000000  loss: 0.2854 (0.2883)  time: 1.3792  data: 0.0002  max mem: 6104\n",
      "Epoch: [99]  [120/126]  eta: 0:00:08  lr: 0.000000  loss: 0.2950 (0.2896)  time: 1.3772  data: 0.0004  max mem: 6104\n",
      "Epoch: [99]  [125/126]  eta: 0:00:01  lr: 0.000000  loss: 0.2944 (0.2895)  time: 1.3868  data: 0.0004  max mem: 6104\n",
      "Epoch: [99] Total time: 0:02:58 (1.4129 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.2944 (0.2895)\n",
      "Training time 4:57:08\n"
     ]
    }
   ],
   "source": [
    "# main_pretrain.py\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import timm\n",
    "\n",
    "# assert timm.__version__ == \"0.3.2\"  # version check\n",
    "from timm.optim import param_groups_weight_decay  \n",
    "\n",
    "# import util.misc as misc\n",
    "# from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "# import models_mae\n",
    "\n",
    "# from engine_pretrain import train_one_epoch\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=256, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--epochs', default=100, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='mae_vit_small_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "    parser.add_argument('--mask_ratio', default=0.75, type=float,\n",
    "                        help='Masking ratio (percentage of removed patches).')\n",
    "\n",
    "    parser.add_argument('--norm_pix_loss', action='store_true',\n",
    "                        help='Use (per-patch) normalized pixels as targets for computing loss')\n",
    "    parser.set_defaults(norm_pix_loss=False)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--blr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=10, metavar='N', # 10 same as SimCLR\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', default='/kaggle/input/ssl-dataset/ssl_dataset/train.X1', type=str,\n",
    "                        help='dataset path')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='/kaggle/working/checkpoints',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default='/kaggle/working/logs',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--num_workers', default=4, type=int) # Kaggle Recommendation\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print(\"Running with args:\\n\", json.dumps(vars(args), indent=2))\n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    dataset_train = datasets.ImageFolder(os.path.join(args.data_path), transform=transform_train)\n",
    "    print(dataset_train)\n",
    "\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "\n",
    "    log_writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir else None\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    # define the model\n",
    "    model = available_models[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using DataParallel with {torch.cuda.device_count()} GPUs.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    print(\"Model = %s\" % str(model))\n",
    "\n",
    "    eff_batch_size = args.batch_size * args.accum_iter * get_world_size()\n",
    "    \n",
    "    if args.lr is None:  # only base_lr is specified\n",
    "        args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "    print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "    print(\"actual lr: %.2e\" % args.lr)\n",
    "    print(\"effective batch size: %d\" % eff_batch_size)\n",
    "    \n",
    "    param_groups = param_groups_weight_decay(model, args.weight_decay)\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))\n",
    "    print(optimizer)\n",
    "    loss_scaler = NativeScalerWithGradNormCount()\n",
    "\n",
    "    load_model(args=args, model=model, optimizer=optimizer, loss_scaler=loss_scaler)\n",
    "\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "        train_stats = train_one_epoch(\n",
    "            model, data_loader_train,\n",
    "            optimizer, device, epoch, loss_scaler,\n",
    "            log_writer=log_writer,\n",
    "            args=args\n",
    "        )\n",
    "        if args.output_dir and (epoch % 5 == 0 or epoch + 1 == args.epochs):\n",
    "            save_model(\n",
    "                args=args, model=model, optimizer=optimizer,\n",
    "                loss_scaler=loss_scaler, epoch=epoch)\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        'epoch': epoch,}\n",
    "\n",
    "        if args.output_dir and is_main_process():\n",
    "            if log_writer is not None:\n",
    "                log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_args_parser()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d53318",
   "metadata": {
    "papermill": {
     "duration": 0.033547,
     "end_time": "2025-05-28T00:44:02.699945",
     "exception": false,
     "start_time": "2025-05-28T00:44:02.666398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7502292,
     "sourceId": 11932960,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17896.429575,
   "end_time": "2025-05-28T00:44:06.466428",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-27T19:45:50.036853",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
