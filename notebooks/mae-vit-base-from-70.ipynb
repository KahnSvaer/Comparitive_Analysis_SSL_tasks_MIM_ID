{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65238d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:10.781437Z",
     "iopub.status.busy": "2025-05-28T13:50:10.780791Z",
     "iopub.status.idle": "2025-05-28T13:50:10.784667Z",
     "shell.execute_reply": "2025-05-28T13:50:10.784063Z"
    },
    "papermill": {
     "duration": 0.010057,
     "end_time": "2025-05-28T13:50:10.785881",
     "exception": false,
     "start_time": "2025-05-28T13:50:10.775824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing MAE model over the dataset\n",
    "\n",
    "# Reference for code : https://github.com/facebookresearch/mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6645059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:10.792173Z",
     "iopub.status.busy": "2025-05-28T13:50:10.791987Z",
     "iopub.status.idle": "2025-05-28T13:50:15.991426Z",
     "shell.execute_reply": "2025-05-28T13:50:15.990625Z"
    },
    "papermill": {
     "duration": 5.204154,
     "end_time": "2025-05-28T13:50:15.993069",
     "exception": false,
     "start_time": "2025-05-28T13:50:10.788915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/': Device or resource busy\r\n"
     ]
    }
   ],
   "source": [
    "# Code for clearing memory\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "!rm -rf /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa7a029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:16.000965Z",
     "iopub.status.busy": "2025-05-28T13:50:16.000394Z",
     "iopub.status.idle": "2025-05-28T13:50:16.033209Z",
     "shell.execute_reply": "2025-05-28T13:50:16.032396Z"
    },
    "papermill": {
     "duration": 0.03845,
     "end_time": "2025-05-28T13:50:16.034485",
     "exception": false,
     "start_time": "2025-05-28T13:50:15.996035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util/misc.py\n",
    "\n",
    "import builtins\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        setup_for_distributed(is_master=True)  # hack\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == float('inf'):\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def save_model(args, epoch, model, optimizer, loss_scaler):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    if loss_scaler is not None:\n",
    "        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            to_save = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'scaler': loss_scaler.state_dict(),\n",
    "                'args': args,\n",
    "            }\n",
    "\n",
    "            save_on_master(to_save, checkpoint_path)\n",
    "    else:\n",
    "        client_state = {'epoch': epoch}\n",
    "        model.save_checkpoint(save_dir=args.output_dir, tag=\"checkpoint-%s\" % epoch_name, client_state=client_state)\n",
    "\n",
    "\n",
    "def load_model(args, model, optimizer, loss_scaler):\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu', weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint and not (hasattr(args, 'eval') and args.eval):\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            args.start_epoch = checkpoint['epoch'] + 1\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "\n",
    "def all_reduce_mean(x):\n",
    "    world_size = get_world_size()\n",
    "    if world_size > 1:\n",
    "        x_reduce = torch.tensor(x).cuda()\n",
    "        dist.all_reduce(x_reduce)\n",
    "        x_reduce /= world_size\n",
    "        return x_reduce.item()\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73947ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:16.041120Z",
     "iopub.status.busy": "2025-05-28T13:50:16.040859Z",
     "iopub.status.idle": "2025-05-28T13:50:16.045589Z",
     "shell.execute_reply": "2025-05-28T13:50:16.045024Z"
    },
    "papermill": {
     "duration": 0.009408,
     "end_time": "2025-05-28T13:50:16.046801",
     "exception": false,
     "start_time": "2025-05-28T13:50:16.037393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util/lr_sched.py\n",
    "\n",
    "import math\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < args.warmup_epochs:\n",
    "        lr = args.lr * epoch / args.warmup_epochs \n",
    "    else:\n",
    "        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50d28ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:16.053270Z",
     "iopub.status.busy": "2025-05-28T13:50:16.053058Z",
     "iopub.status.idle": "2025-05-28T13:50:16.062261Z",
     "shell.execute_reply": "2025-05-28T13:50:16.061725Z"
    },
    "papermill": {
     "duration": 0.0137,
     "end_time": "2025-05-28T13:50:16.063304",
     "exception": false,
     "start_time": "2025-05-28T13:50:16.049604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util/pos_embed.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bd7da0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:16.069777Z",
     "iopub.status.busy": "2025-05-28T13:50:16.069580Z",
     "iopub.status.idle": "2025-05-28T13:50:25.728804Z",
     "shell.execute_reply": "2025-05-28T13:50:25.728138Z"
    },
    "papermill": {
     "duration": 9.66421,
     "end_time": "2025-05-28T13:50:25.730234",
     "exception": false,
     "start_time": "2025-05-28T13:50:16.066024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models_mae.py\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "# from util.pos_embed import get_2d_sincos_pos_embed\n",
    "\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss # Adding mean to balance out DataParallel\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "\n",
    "def mae_vit_small_patch16_dec512d8b(**kwargs):  # Main Test Done in this\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6,  # corrected for small model\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# set recommended archs\n",
    "\n",
    "available_models = {\n",
    "    'mae_vit_small_patch16' : mae_vit_small_patch16_dec512d8b, # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_base_patch16' : mae_vit_base_patch16_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_large_patch16' : mae_vit_large_patch16_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_huge_patch14' : mae_vit_huge_patch14_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa5914f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:25.737211Z",
     "iopub.status.busy": "2025-05-28T13:50:25.736859Z",
     "iopub.status.idle": "2025-05-28T13:50:25.746052Z",
     "shell.execute_reply": "2025-05-28T13:50:25.745288Z"
    },
    "papermill": {
     "duration": 0.013836,
     "end_time": "2025-05-28T13:50:25.747182",
     "exception": false,
     "start_time": "2025-05-28T13:50:25.733346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# engine_pretrain.py\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "\n",
    "# import util.misc as misc\n",
    "# import util.lr_sched as lr_sched\n",
    "\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler,\n",
    "                    log_writer=None,\n",
    "                    args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 20\n",
    "\n",
    "    accum_iter = args.accum_iter\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "\n",
    "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
    "\n",
    "        loss = loss.mean()\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
    "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=lr)\n",
    "\n",
    "        loss_value_reduce = all_reduce_mean(loss_value)\n",
    "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
    "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
    "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
    "\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7e6fb5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-28T13:50:25.753729Z",
     "iopub.status.busy": "2025-05-28T13:50:25.753482Z",
     "iopub.status.idle": "2025-05-28T16:05:02.296500Z",
     "shell.execute_reply": "2025-05-28T16:05:02.295617Z"
    },
    "papermill": {
     "duration": 8076.547869,
     "end_time": "2025-05-28T16:05:02.297858",
     "exception": false,
     "start_time": "2025-05-28T13:50:25.749989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:50:27.972279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748440228.195548      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748440228.262591      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with args:\n",
      " {\n",
      "  \"batch_size\": 256,\n",
      "  \"epochs\": 100,\n",
      "  \"accum_iter\": 1,\n",
      "  \"model\": \"mae_vit_base_patch16\",\n",
      "  \"input_size\": 224,\n",
      "  \"mask_ratio\": 0.75,\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"weight_decay\": 0.05,\n",
      "  \"lr\": null,\n",
      "  \"blr\": 0.001,\n",
      "  \"min_lr\": 0.0,\n",
      "  \"warmup_epochs\": 10,\n",
      "  \"data_path\": \"/kaggle/input/ssl-dataset/ssl_dataset/train.X1\",\n",
      "  \"output_dir\": \"/kaggle/working/checkpoints\",\n",
      "  \"log_dir\": \"/kaggle/working/logs\",\n",
      "  \"device\": \"cuda\",\n",
      "  \"seed\": 0,\n",
      "  \"resume\": \"/kaggle/input/mae-vit-base/checkpoints/checkpoint-70.pth\",\n",
      "  \"start_epoch\": 0,\n",
      "  \"num_workers\": 4,\n",
      "  \"pin_mem\": true,\n",
      "  \"world_size\": 1,\n",
      "  \"local_rank\": -1,\n",
      "  \"dist_on_itp\": false,\n",
      "  \"dist_url\": \"env://\"\n",
      "}\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 32500\n",
      "    Root location: /kaggle/input/ssl-dataset/ssl_dataset/train.X1\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Using DataParallel with 2 GPUs.\n",
      "Model = DataParallel(\n",
      "  (module): MaskedAutoencoderViT(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (decoder_blocks): ModuleList(\n",
      "      (0-7): 8 x Block(\n",
      "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      "base lr: 1.00e-03\n",
      "actual lr: 1.00e-03\n",
      "effective batch size: 256\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/589335751.py:229: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume checkpoint /kaggle/input/mae-vit-base/checkpoints/checkpoint-70.pth\n",
      "With optim & sched!\n",
      "Start training for 100 epochs\n",
      "log_dir: /kaggle/working/logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3798565144.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71]  [  0/126]  eta: 0:25:48  lr: 0.000235  loss: 0.2735 (0.2735)  time: 12.2859  data: 6.1135  max mem: 8045\n",
      "Epoch: [71]  [ 20/126]  eta: 0:04:25  lr: 0.000233  loss: 0.2967 (0.2971)  time: 2.0152  data: 0.0003  max mem: 8053\n",
      "Epoch: [71]  [ 40/126]  eta: 0:03:17  lr: 0.000230  loss: 0.2928 (0.2966)  time: 2.0805  data: 0.0004  max mem: 8053\n",
      "Epoch: [71]  [ 60/126]  eta: 0:02:28  lr: 0.000228  loss: 0.3025 (0.2974)  time: 2.1729  data: 0.0004  max mem: 8053\n",
      "Epoch: [71]  [ 80/126]  eta: 0:01:42  lr: 0.000226  loss: 0.2916 (0.2959)  time: 2.1423  data: 0.0004  max mem: 8053\n",
      "Epoch: [71]  [100/126]  eta: 0:00:57  lr: 0.000223  loss: 0.3023 (0.2966)  time: 2.1481  data: 0.0004  max mem: 8053\n",
      "Epoch: [71]  [120/126]  eta: 0:00:13  lr: 0.000221  loss: 0.2944 (0.2964)  time: 2.1432  data: 0.0004  max mem: 8053\n",
      "Epoch: [71]  [125/126]  eta: 0:00:02  lr: 0.000221  loss: 0.2952 (0.2964)  time: 2.1414  data: 0.0004  max mem: 8053\n",
      "Epoch: [71] Total time: 0:04:37 (2.1995 s / it)\n",
      "Averaged stats: lr: 0.000221  loss: 0.2952 (0.2964)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [72]  [  0/126]  eta: 0:12:37  lr: 0.000220  loss: 0.3038 (0.3038)  time: 6.0125  data: 3.7790  max mem: 8053\n",
      "Epoch: [72]  [ 20/126]  eta: 0:04:07  lr: 0.000218  loss: 0.2961 (0.2987)  time: 2.1528  data: 0.0003  max mem: 8053\n",
      "Epoch: [72]  [ 40/126]  eta: 0:03:12  lr: 0.000216  loss: 0.2967 (0.2972)  time: 2.1335  data: 0.0003  max mem: 8053\n",
      "Epoch: [72]  [ 60/126]  eta: 0:02:25  lr: 0.000214  loss: 0.2967 (0.2985)  time: 2.1376  data: 0.0003  max mem: 8053\n",
      "Epoch: [72]  [ 80/126]  eta: 0:01:41  lr: 0.000211  loss: 0.2890 (0.2967)  time: 2.1728  data: 0.0003  max mem: 8053\n",
      "Epoch: [72]  [100/126]  eta: 0:00:56  lr: 0.000209  loss: 0.2934 (0.2963)  time: 2.1391  data: 0.0003  max mem: 8053\n",
      "Epoch: [72]  [120/126]  eta: 0:00:13  lr: 0.000207  loss: 0.2960 (0.2964)  time: 2.1320  data: 0.0004  max mem: 8053\n",
      "Epoch: [72]  [125/126]  eta: 0:00:02  lr: 0.000206  loss: 0.2947 (0.2965)  time: 2.1319  data: 0.0004  max mem: 8053\n",
      "Epoch: [72] Total time: 0:04:34 (2.1759 s / it)\n",
      "Averaged stats: lr: 0.000206  loss: 0.2947 (0.2965)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [73]  [  0/126]  eta: 0:12:40  lr: 0.000206  loss: 0.3060 (0.3060)  time: 6.0353  data: 3.6668  max mem: 8053\n",
      "Epoch: [73]  [ 20/126]  eta: 0:04:07  lr: 0.000204  loss: 0.2932 (0.2961)  time: 2.1502  data: 0.0004  max mem: 8053\n",
      "Epoch: [73]  [ 40/126]  eta: 0:03:12  lr: 0.000202  loss: 0.2954 (0.2958)  time: 2.1315  data: 0.0003  max mem: 8053\n",
      "Epoch: [73]  [ 60/126]  eta: 0:02:25  lr: 0.000199  loss: 0.2921 (0.2953)  time: 2.1397  data: 0.0003  max mem: 8053\n",
      "Epoch: [73]  [ 80/126]  eta: 0:01:40  lr: 0.000197  loss: 0.2972 (0.2957)  time: 2.1321  data: 0.0003  max mem: 8053\n",
      "Epoch: [73]  [100/126]  eta: 0:00:56  lr: 0.000195  loss: 0.2927 (0.2957)  time: 2.1383  data: 0.0003  max mem: 8053\n",
      "Epoch: [73]  [120/126]  eta: 0:00:13  lr: 0.000193  loss: 0.2886 (0.2952)  time: 2.1324  data: 0.0004  max mem: 8053\n",
      "Epoch: [73]  [125/126]  eta: 0:00:02  lr: 0.000192  loss: 0.2914 (0.2952)  time: 2.1316  data: 0.0003  max mem: 8053\n",
      "Epoch: [73] Total time: 0:04:33 (2.1691 s / it)\n",
      "Averaged stats: lr: 0.000192  loss: 0.2914 (0.2952)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [74]  [  0/126]  eta: 0:12:13  lr: 0.000192  loss: 0.3190 (0.3190)  time: 5.8254  data: 3.5828  max mem: 8053\n",
      "Epoch: [74]  [ 20/126]  eta: 0:04:11  lr: 0.000190  loss: 0.3010 (0.3041)  time: 2.1981  data: 0.0003  max mem: 8053\n",
      "Epoch: [74]  [ 40/126]  eta: 0:03:13  lr: 0.000188  loss: 0.2934 (0.2990)  time: 2.1330  data: 0.0004  max mem: 8053\n",
      "Epoch: [74]  [ 60/126]  eta: 0:02:26  lr: 0.000186  loss: 0.2853 (0.2962)  time: 2.1361  data: 0.0004  max mem: 8053\n",
      "Epoch: [74]  [ 80/126]  eta: 0:01:41  lr: 0.000184  loss: 0.2922 (0.2960)  time: 2.1361  data: 0.0003  max mem: 8053\n",
      "Epoch: [74]  [100/126]  eta: 0:00:56  lr: 0.000181  loss: 0.2920 (0.2953)  time: 2.1403  data: 0.0003  max mem: 8053\n",
      "Epoch: [74]  [120/126]  eta: 0:00:13  lr: 0.000179  loss: 0.2968 (0.2954)  time: 2.1339  data: 0.0004  max mem: 8053\n",
      "Epoch: [74]  [125/126]  eta: 0:00:02  lr: 0.000179  loss: 0.2931 (0.2951)  time: 2.1332  data: 0.0004  max mem: 8053\n",
      "Epoch: [74] Total time: 0:04:34 (2.1759 s / it)\n",
      "Averaged stats: lr: 0.000179  loss: 0.2931 (0.2951)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [75]  [  0/126]  eta: 0:13:40  lr: 0.000179  loss: 0.2944 (0.2944)  time: 6.5146  data: 4.1418  max mem: 8053\n",
      "Epoch: [75]  [ 20/126]  eta: 0:04:10  lr: 0.000176  loss: 0.2886 (0.2912)  time: 2.1529  data: 0.0004  max mem: 8053\n",
      "Epoch: [75]  [ 40/126]  eta: 0:03:13  lr: 0.000174  loss: 0.2957 (0.2930)  time: 2.1277  data: 0.0003  max mem: 8053\n",
      "Epoch: [75]  [ 60/126]  eta: 0:02:25  lr: 0.000172  loss: 0.2981 (0.2953)  time: 2.1404  data: 0.0003  max mem: 8053\n",
      "Epoch: [75]  [ 80/126]  eta: 0:01:41  lr: 0.000170  loss: 0.2987 (0.2963)  time: 2.1701  data: 0.0003  max mem: 8053\n",
      "Epoch: [75]  [100/126]  eta: 0:00:56  lr: 0.000168  loss: 0.2931 (0.2963)  time: 2.1372  data: 0.0004  max mem: 8053\n",
      "Epoch: [75]  [120/126]  eta: 0:00:13  lr: 0.000166  loss: 0.2884 (0.2950)  time: 2.1323  data: 0.0004  max mem: 8053\n",
      "Epoch: [75]  [125/126]  eta: 0:00:02  lr: 0.000166  loss: 0.2884 (0.2948)  time: 2.1328  data: 0.0003  max mem: 8053\n",
      "Epoch: [75] Total time: 0:04:34 (2.1789 s / it)\n",
      "Averaged stats: lr: 0.000166  loss: 0.2884 (0.2948)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [76]  [  0/126]  eta: 0:12:04  lr: 0.000165  loss: 0.3241 (0.3241)  time: 5.7509  data: 3.4320  max mem: 8053\n",
      "Epoch: [76]  [ 20/126]  eta: 0:04:06  lr: 0.000163  loss: 0.2895 (0.2928)  time: 2.1547  data: 0.0003  max mem: 8053\n",
      "Epoch: [76]  [ 40/126]  eta: 0:03:11  lr: 0.000161  loss: 0.2958 (0.2952)  time: 2.1324  data: 0.0003  max mem: 8053\n",
      "Epoch: [76]  [ 60/126]  eta: 0:02:25  lr: 0.000159  loss: 0.2866 (0.2941)  time: 2.1364  data: 0.0003  max mem: 8053\n",
      "Epoch: [76]  [ 80/126]  eta: 0:01:40  lr: 0.000157  loss: 0.2990 (0.2952)  time: 2.1343  data: 0.0003  max mem: 8053\n",
      "Epoch: [76]  [100/126]  eta: 0:00:56  lr: 0.000155  loss: 0.2874 (0.2943)  time: 2.1348  data: 0.0003  max mem: 8053\n",
      "Epoch: [76]  [120/126]  eta: 0:00:13  lr: 0.000153  loss: 0.2960 (0.2947)  time: 2.1710  data: 0.0004  max mem: 8053\n",
      "Epoch: [76]  [125/126]  eta: 0:00:02  lr: 0.000153  loss: 0.2939 (0.2943)  time: 2.1706  data: 0.0003  max mem: 8053\n",
      "Epoch: [76] Total time: 0:04:33 (2.1733 s / it)\n",
      "Averaged stats: lr: 0.000153  loss: 0.2939 (0.2943)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [77]  [  0/126]  eta: 0:14:52  lr: 0.000153  loss: 0.2947 (0.2947)  time: 7.0843  data: 4.8165  max mem: 8053\n",
      "Epoch: [77]  [ 20/126]  eta: 0:04:13  lr: 0.000151  loss: 0.2929 (0.2929)  time: 2.1525  data: 0.0003  max mem: 8053\n",
      "Epoch: [77]  [ 40/126]  eta: 0:03:14  lr: 0.000149  loss: 0.2934 (0.2943)  time: 2.1290  data: 0.0004  max mem: 8053\n",
      "Epoch: [77]  [ 60/126]  eta: 0:02:26  lr: 0.000147  loss: 0.2923 (0.2940)  time: 2.1392  data: 0.0003  max mem: 8053\n",
      "Epoch: [77]  [ 80/126]  eta: 0:01:41  lr: 0.000145  loss: 0.2888 (0.2938)  time: 2.1330  data: 0.0003  max mem: 8053\n",
      "Epoch: [77]  [100/126]  eta: 0:00:56  lr: 0.000143  loss: 0.2949 (0.2938)  time: 2.1305  data: 0.0003  max mem: 8053\n",
      "Epoch: [77]  [120/126]  eta: 0:00:13  lr: 0.000141  loss: 0.2958 (0.2942)  time: 2.1355  data: 0.0004  max mem: 8053\n",
      "Epoch: [77]  [125/126]  eta: 0:00:02  lr: 0.000140  loss: 0.2947 (0.2940)  time: 2.1342  data: 0.0003  max mem: 8053\n",
      "Epoch: [77] Total time: 0:04:34 (2.1766 s / it)\n",
      "Averaged stats: lr: 0.000140  loss: 0.2947 (0.2940)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [78]  [  0/126]  eta: 0:13:30  lr: 0.000140  loss: 0.2999 (0.2999)  time: 6.4327  data: 4.1859  max mem: 8053\n",
      "Epoch: [78]  [ 20/126]  eta: 0:04:12  lr: 0.000138  loss: 0.2931 (0.2952)  time: 2.1843  data: 0.0005  max mem: 8053\n",
      "Epoch: [78]  [ 40/126]  eta: 0:03:14  lr: 0.000137  loss: 0.2929 (0.2950)  time: 2.1330  data: 0.0003  max mem: 8053\n",
      "Epoch: [78]  [ 60/126]  eta: 0:02:26  lr: 0.000135  loss: 0.2950 (0.2952)  time: 2.1344  data: 0.0003  max mem: 8053\n",
      "Epoch: [78]  [ 80/126]  eta: 0:01:41  lr: 0.000133  loss: 0.2930 (0.2951)  time: 2.1347  data: 0.0003  max mem: 8053\n",
      "Epoch: [78]  [100/126]  eta: 0:00:56  lr: 0.000131  loss: 0.2926 (0.2950)  time: 2.1369  data: 0.0003  max mem: 8053\n",
      "Epoch: [78]  [120/126]  eta: 0:00:13  lr: 0.000129  loss: 0.2920 (0.2946)  time: 2.1348  data: 0.0004  max mem: 8053\n",
      "Epoch: [78]  [125/126]  eta: 0:00:02  lr: 0.000129  loss: 0.2917 (0.2946)  time: 2.1345  data: 0.0003  max mem: 8053\n",
      "Epoch: [78] Total time: 0:04:34 (2.1778 s / it)\n",
      "Averaged stats: lr: 0.000129  loss: 0.2917 (0.2946)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [79]  [  0/126]  eta: 0:12:58  lr: 0.000128  loss: 0.2766 (0.2766)  time: 6.1795  data: 3.7963  max mem: 8053\n",
      "Epoch: [79]  [ 20/126]  eta: 0:04:08  lr: 0.000127  loss: 0.2966 (0.2943)  time: 2.1566  data: 0.0003  max mem: 8053\n",
      "Epoch: [79]  [ 40/126]  eta: 0:03:12  lr: 0.000125  loss: 0.2926 (0.2942)  time: 2.1297  data: 0.0003  max mem: 8053\n",
      "Epoch: [79]  [ 60/126]  eta: 0:02:26  lr: 0.000123  loss: 0.2945 (0.2940)  time: 2.1734  data: 0.0003  max mem: 8053\n",
      "Epoch: [79]  [ 80/126]  eta: 0:01:41  lr: 0.000121  loss: 0.2935 (0.2940)  time: 2.1376  data: 0.0003  max mem: 8053\n",
      "Epoch: [79]  [100/126]  eta: 0:00:56  lr: 0.000119  loss: 0.2914 (0.2934)  time: 2.1346  data: 0.0004  max mem: 8053\n",
      "Epoch: [79]  [120/126]  eta: 0:00:13  lr: 0.000118  loss: 0.2912 (0.2933)  time: 2.1393  data: 0.0003  max mem: 8053\n",
      "Epoch: [79]  [125/126]  eta: 0:00:02  lr: 0.000117  loss: 0.2901 (0.2931)  time: 2.1389  data: 0.0003  max mem: 8053\n",
      "Epoch: [79] Total time: 0:04:34 (2.1778 s / it)\n",
      "Averaged stats: lr: 0.000117  loss: 0.2901 (0.2931)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [80]  [  0/126]  eta: 0:13:22  lr: 0.000117  loss: 0.2916 (0.2916)  time: 6.3664  data: 4.1024  max mem: 8053\n",
      "Epoch: [80]  [ 20/126]  eta: 0:04:09  lr: 0.000115  loss: 0.2970 (0.2937)  time: 2.1553  data: 0.0004  max mem: 8053\n",
      "Epoch: [80]  [ 40/126]  eta: 0:03:13  lr: 0.000113  loss: 0.2942 (0.2953)  time: 2.1357  data: 0.0003  max mem: 8053\n",
      "Epoch: [80]  [ 60/126]  eta: 0:02:25  lr: 0.000112  loss: 0.2903 (0.2943)  time: 2.1351  data: 0.0004  max mem: 8053\n",
      "Epoch: [80]  [ 80/126]  eta: 0:01:40  lr: 0.000110  loss: 0.2920 (0.2936)  time: 2.1369  data: 0.0003  max mem: 8053\n",
      "Epoch: [80]  [100/126]  eta: 0:00:56  lr: 0.000108  loss: 0.2860 (0.2925)  time: 2.1763  data: 0.0003  max mem: 8053\n",
      "Epoch: [80]  [120/126]  eta: 0:00:13  lr: 0.000107  loss: 0.2875 (0.2924)  time: 2.1379  data: 0.0004  max mem: 8053\n",
      "Epoch: [80]  [125/126]  eta: 0:00:02  lr: 0.000106  loss: 0.2907 (0.2924)  time: 2.1350  data: 0.0004  max mem: 8053\n",
      "Epoch: [80] Total time: 0:04:34 (2.1801 s / it)\n",
      "Averaged stats: lr: 0.000106  loss: 0.2907 (0.2924)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [81]  [  0/126]  eta: 0:12:47  lr: 0.000106  loss: 0.2736 (0.2736)  time: 6.0925  data: 3.7694  max mem: 8053\n",
      "Epoch: [81]  [ 20/126]  eta: 0:04:09  lr: 0.000104  loss: 0.2915 (0.2906)  time: 2.1680  data: 0.0006  max mem: 8053\n",
      "Epoch: [81]  [ 40/126]  eta: 0:03:13  lr: 0.000103  loss: 0.2896 (0.2913)  time: 2.1307  data: 0.0003  max mem: 8053\n",
      "Epoch: [81]  [ 60/126]  eta: 0:02:26  lr: 0.000101  loss: 0.2913 (0.2912)  time: 2.1438  data: 0.0003  max mem: 8053\n",
      "Epoch: [81]  [ 80/126]  eta: 0:01:40  lr: 0.000099  loss: 0.2931 (0.2916)  time: 2.1394  data: 0.0004  max mem: 8053\n",
      "Epoch: [81]  [100/126]  eta: 0:00:56  lr: 0.000098  loss: 0.2930 (0.2922)  time: 2.1389  data: 0.0003  max mem: 8053\n",
      "Epoch: [81]  [120/126]  eta: 0:00:13  lr: 0.000096  loss: 0.2973 (0.2928)  time: 2.1353  data: 0.0005  max mem: 8053\n",
      "Epoch: [81]  [125/126]  eta: 0:00:02  lr: 0.000096  loss: 0.2973 (0.2929)  time: 2.1661  data: 0.0005  max mem: 8053\n",
      "Epoch: [81] Total time: 0:04:34 (2.1797 s / it)\n",
      "Averaged stats: lr: 0.000096  loss: 0.2973 (0.2929)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [82]  [  0/126]  eta: 0:12:19  lr: 0.000095  loss: 0.2903 (0.2903)  time: 5.8654  data: 3.4924  max mem: 8053\n",
      "Epoch: [82]  [ 20/126]  eta: 0:04:07  lr: 0.000094  loss: 0.2881 (0.2885)  time: 2.1581  data: 0.0004  max mem: 8053\n",
      "Epoch: [82]  [ 40/126]  eta: 0:03:12  lr: 0.000092  loss: 0.2944 (0.2921)  time: 2.1344  data: 0.0004  max mem: 8053\n",
      "Epoch: [82]  [ 60/126]  eta: 0:02:25  lr: 0.000091  loss: 0.2983 (0.2936)  time: 2.1404  data: 0.0003  max mem: 8053\n",
      "Epoch: [82]  [ 80/126]  eta: 0:01:40  lr: 0.000089  loss: 0.2964 (0.2940)  time: 2.1340  data: 0.0004  max mem: 8053\n",
      "Epoch: [82]  [100/126]  eta: 0:00:56  lr: 0.000088  loss: 0.2924 (0.2938)  time: 2.1374  data: 0.0003  max mem: 8053\n",
      "Epoch: [82]  [120/126]  eta: 0:00:13  lr: 0.000086  loss: 0.2923 (0.2934)  time: 2.1358  data: 0.0004  max mem: 8053\n",
      "Epoch: [82]  [125/126]  eta: 0:00:02  lr: 0.000086  loss: 0.2933 (0.2936)  time: 2.1345  data: 0.0003  max mem: 8053\n",
      "Epoch: [82] Total time: 0:04:33 (2.1703 s / it)\n",
      "Averaged stats: lr: 0.000086  loss: 0.2933 (0.2936)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [83]  [  0/126]  eta: 0:12:21  lr: 0.000085  loss: 0.2826 (0.2826)  time: 5.8862  data: 3.5366  max mem: 8053\n",
      "Epoch: [83]  [ 20/126]  eta: 0:04:06  lr: 0.000084  loss: 0.2946 (0.2946)  time: 2.1521  data: 0.0003  max mem: 8053\n",
      "Epoch: [83]  [ 40/126]  eta: 0:03:13  lr: 0.000082  loss: 0.2919 (0.2942)  time: 2.1590  data: 0.0003  max mem: 8053\n",
      "Epoch: [83]  [ 60/126]  eta: 0:02:25  lr: 0.000081  loss: 0.2948 (0.2940)  time: 2.1366  data: 0.0003  max mem: 8053\n",
      "Epoch: [83]  [ 80/126]  eta: 0:01:40  lr: 0.000079  loss: 0.2913 (0.2936)  time: 2.1302  data: 0.0002  max mem: 8053\n",
      "Epoch: [83]  [100/126]  eta: 0:00:56  lr: 0.000078  loss: 0.2928 (0.2941)  time: 2.1362  data: 0.0003  max mem: 8053\n",
      "Epoch: [83]  [120/126]  eta: 0:00:13  lr: 0.000076  loss: 0.2892 (0.2932)  time: 2.1359  data: 0.0004  max mem: 8053\n",
      "Epoch: [83]  [125/126]  eta: 0:00:02  lr: 0.000076  loss: 0.2879 (0.2931)  time: 2.1368  data: 0.0003  max mem: 8053\n",
      "Epoch: [83] Total time: 0:04:33 (2.1722 s / it)\n",
      "Averaged stats: lr: 0.000076  loss: 0.2879 (0.2931)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [84]  [  0/126]  eta: 0:12:18  lr: 0.000076  loss: 0.2874 (0.2874)  time: 5.8627  data: 3.4923  max mem: 8053\n",
      "Epoch: [84]  [ 20/126]  eta: 0:04:07  lr: 0.000075  loss: 0.2944 (0.2898)  time: 2.1559  data: 0.0004  max mem: 8053\n",
      "Epoch: [84]  [ 40/126]  eta: 0:03:12  lr: 0.000073  loss: 0.2832 (0.2881)  time: 2.1293  data: 0.0003  max mem: 8053\n",
      "Epoch: [84]  [ 60/126]  eta: 0:02:25  lr: 0.000072  loss: 0.2894 (0.2895)  time: 2.1412  data: 0.0003  max mem: 8053\n",
      "Epoch: [84]  [ 80/126]  eta: 0:01:40  lr: 0.000070  loss: 0.2937 (0.2917)  time: 2.1701  data: 0.0003  max mem: 8053\n",
      "Epoch: [84]  [100/126]  eta: 0:00:56  lr: 0.000069  loss: 0.2940 (0.2921)  time: 2.1377  data: 0.0003  max mem: 8053\n",
      "Epoch: [84]  [120/126]  eta: 0:00:13  lr: 0.000067  loss: 0.2904 (0.2924)  time: 2.1329  data: 0.0004  max mem: 8053\n",
      "Epoch: [84]  [125/126]  eta: 0:00:02  lr: 0.000067  loss: 0.2948 (0.2927)  time: 2.1330  data: 0.0004  max mem: 8053\n",
      "Epoch: [84] Total time: 0:04:34 (2.1747 s / it)\n",
      "Averaged stats: lr: 0.000067  loss: 0.2948 (0.2927)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [85]  [  0/126]  eta: 0:12:40  lr: 0.000067  loss: 0.2838 (0.2838)  time: 6.0392  data: 3.7997  max mem: 8053\n",
      "Epoch: [85]  [ 20/126]  eta: 0:04:08  lr: 0.000066  loss: 0.2911 (0.2929)  time: 2.1547  data: 0.0004  max mem: 8053\n",
      "Epoch: [85]  [ 40/126]  eta: 0:03:12  lr: 0.000064  loss: 0.2925 (0.2919)  time: 2.1333  data: 0.0003  max mem: 8053\n",
      "Epoch: [85]  [ 60/126]  eta: 0:02:25  lr: 0.000063  loss: 0.2939 (0.2925)  time: 2.1352  data: 0.0002  max mem: 8053\n",
      "Epoch: [85]  [ 80/126]  eta: 0:01:40  lr: 0.000062  loss: 0.2903 (0.2926)  time: 2.1393  data: 0.0003  max mem: 8053\n",
      "Epoch: [85]  [100/126]  eta: 0:00:56  lr: 0.000060  loss: 0.2882 (0.2924)  time: 2.1364  data: 0.0003  max mem: 8053\n",
      "Epoch: [85]  [120/126]  eta: 0:00:13  lr: 0.000059  loss: 0.2903 (0.2921)  time: 2.1756  data: 0.0003  max mem: 8053\n",
      "Epoch: [85]  [125/126]  eta: 0:00:02  lr: 0.000059  loss: 0.2932 (0.2924)  time: 2.1737  data: 0.0003  max mem: 8053\n",
      "Epoch: [85] Total time: 0:04:34 (2.1776 s / it)\n",
      "Averaged stats: lr: 0.000059  loss: 0.2932 (0.2924)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [86]  [  0/126]  eta: 0:12:09  lr: 0.000059  loss: 0.2706 (0.2706)  time: 5.7877  data: 3.4594  max mem: 8053\n",
      "Epoch: [86]  [ 20/126]  eta: 0:04:07  lr: 0.000057  loss: 0.2946 (0.2917)  time: 2.1615  data: 0.0003  max mem: 8053\n",
      "Epoch: [86]  [ 40/126]  eta: 0:03:12  lr: 0.000056  loss: 0.2876 (0.2903)  time: 2.1305  data: 0.0002  max mem: 8053\n",
      "Epoch: [86]  [ 60/126]  eta: 0:02:25  lr: 0.000055  loss: 0.2932 (0.2917)  time: 2.1362  data: 0.0004  max mem: 8053\n",
      "Epoch: [86]  [ 80/126]  eta: 0:01:40  lr: 0.000053  loss: 0.2874 (0.2907)  time: 2.1333  data: 0.0002  max mem: 8053\n",
      "Epoch: [86]  [100/126]  eta: 0:00:56  lr: 0.000052  loss: 0.2907 (0.2909)  time: 2.1334  data: 0.0003  max mem: 8053\n",
      "Epoch: [86]  [120/126]  eta: 0:00:13  lr: 0.000051  loss: 0.2941 (0.2912)  time: 2.1320  data: 0.0007  max mem: 8053\n",
      "Epoch: [86]  [125/126]  eta: 0:00:02  lr: 0.000051  loss: 0.2867 (0.2908)  time: 2.1306  data: 0.0007  max mem: 8053\n",
      "Epoch: [86] Total time: 0:04:33 (2.1675 s / it)\n",
      "Averaged stats: lr: 0.000051  loss: 0.2867 (0.2908)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [87]  [  0/126]  eta: 0:13:05  lr: 0.000051  loss: 0.3100 (0.3100)  time: 6.2349  data: 3.8898  max mem: 8053\n",
      "Epoch: [87]  [ 20/126]  eta: 0:04:12  lr: 0.000049  loss: 0.2932 (0.2935)  time: 2.1916  data: 0.0003  max mem: 8053\n",
      "Epoch: [87]  [ 40/126]  eta: 0:03:14  lr: 0.000048  loss: 0.2928 (0.2929)  time: 2.1327  data: 0.0003  max mem: 8053\n",
      "Epoch: [87]  [ 60/126]  eta: 0:02:26  lr: 0.000047  loss: 0.2892 (0.2918)  time: 2.1371  data: 0.0003  max mem: 8053\n",
      "Epoch: [87]  [ 80/126]  eta: 0:01:41  lr: 0.000046  loss: 0.2970 (0.2929)  time: 2.1377  data: 0.0003  max mem: 8053\n",
      "Epoch: [87]  [100/126]  eta: 0:00:56  lr: 0.000045  loss: 0.2883 (0.2923)  time: 2.1332  data: 0.0003  max mem: 8053\n",
      "Epoch: [87]  [120/126]  eta: 0:00:13  lr: 0.000044  loss: 0.2896 (0.2919)  time: 2.1371  data: 0.0004  max mem: 8053\n",
      "Epoch: [87]  [125/126]  eta: 0:00:02  lr: 0.000043  loss: 0.2896 (0.2919)  time: 2.1364  data: 0.0003  max mem: 8053\n",
      "Epoch: [87] Total time: 0:04:34 (2.1781 s / it)\n",
      "Averaged stats: lr: 0.000043  loss: 0.2896 (0.2919)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [88]  [  0/126]  eta: 0:13:50  lr: 0.000043  loss: 0.2951 (0.2951)  time: 6.5922  data: 4.2343  max mem: 8053\n",
      "Epoch: [88]  [ 20/126]  eta: 0:04:10  lr: 0.000042  loss: 0.2901 (0.2940)  time: 2.1544  data: 0.0003  max mem: 8053\n",
      "Epoch: [88]  [ 40/126]  eta: 0:03:13  lr: 0.000041  loss: 0.2958 (0.2934)  time: 2.1320  data: 0.0003  max mem: 8053\n",
      "Epoch: [88]  [ 60/126]  eta: 0:02:26  lr: 0.000040  loss: 0.2876 (0.2926)  time: 2.1718  data: 0.0003  max mem: 8053\n",
      "Epoch: [88]  [ 80/126]  eta: 0:01:41  lr: 0.000039  loss: 0.2906 (0.2923)  time: 2.1351  data: 0.0004  max mem: 8053\n",
      "Epoch: [88]  [100/126]  eta: 0:00:56  lr: 0.000038  loss: 0.2907 (0.2918)  time: 2.1378  data: 0.0004  max mem: 8053\n",
      "Epoch: [88]  [120/126]  eta: 0:00:13  lr: 0.000037  loss: 0.2881 (0.2908)  time: 2.1352  data: 0.0004  max mem: 8053\n",
      "Epoch: [88]  [125/126]  eta: 0:00:02  lr: 0.000036  loss: 0.2899 (0.2909)  time: 2.1328  data: 0.0004  max mem: 8053\n",
      "Epoch: [88] Total time: 0:04:34 (2.1802 s / it)\n",
      "Averaged stats: lr: 0.000036  loss: 0.2899 (0.2909)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [89]  [  0/126]  eta: 0:13:07  lr: 0.000036  loss: 0.2892 (0.2892)  time: 6.2506  data: 3.9692  max mem: 8053\n",
      "Epoch: [89]  [ 20/126]  eta: 0:04:09  lr: 0.000035  loss: 0.2856 (0.2904)  time: 2.1601  data: 0.0003  max mem: 8053\n",
      "Epoch: [89]  [ 40/126]  eta: 0:03:13  lr: 0.000034  loss: 0.2894 (0.2915)  time: 2.1338  data: 0.0003  max mem: 8053\n",
      "Epoch: [89]  [ 60/126]  eta: 0:02:25  lr: 0.000033  loss: 0.2984 (0.2923)  time: 2.1378  data: 0.0003  max mem: 8053\n",
      "Epoch: [89]  [ 80/126]  eta: 0:01:40  lr: 0.000032  loss: 0.2914 (0.2924)  time: 2.1337  data: 0.0003  max mem: 8053\n",
      "Epoch: [89]  [100/126]  eta: 0:00:56  lr: 0.000031  loss: 0.2897 (0.2921)  time: 2.1805  data: 0.0003  max mem: 8053\n",
      "Epoch: [89]  [120/126]  eta: 0:00:13  lr: 0.000030  loss: 0.2838 (0.2917)  time: 2.1345  data: 0.0005  max mem: 8053\n",
      "Epoch: [89]  [125/126]  eta: 0:00:02  lr: 0.000030  loss: 0.2837 (0.2916)  time: 2.1338  data: 0.0004  max mem: 8053\n",
      "Epoch: [89] Total time: 0:04:34 (2.1799 s / it)\n",
      "Averaged stats: lr: 0.000030  loss: 0.2837 (0.2916)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [90]  [  0/126]  eta: 0:12:56  lr: 0.000030  loss: 0.3059 (0.3059)  time: 6.1627  data: 3.7541  max mem: 8053\n",
      "Epoch: [90]  [ 20/126]  eta: 0:04:08  lr: 0.000029  loss: 0.2941 (0.2952)  time: 2.1582  data: 0.0004  max mem: 8053\n",
      "Epoch: [90]  [ 40/126]  eta: 0:03:12  lr: 0.000028  loss: 0.2931 (0.2932)  time: 2.1338  data: 0.0003  max mem: 8053\n",
      "Epoch: [90]  [ 60/126]  eta: 0:02:25  lr: 0.000027  loss: 0.2936 (0.2926)  time: 2.1364  data: 0.0003  max mem: 8053\n",
      "Epoch: [90]  [ 80/126]  eta: 0:01:40  lr: 0.000026  loss: 0.2884 (0.2919)  time: 2.1356  data: 0.0003  max mem: 8053\n",
      "Epoch: [90]  [100/126]  eta: 0:00:56  lr: 0.000026  loss: 0.2896 (0.2914)  time: 2.1344  data: 0.0003  max mem: 8053\n",
      "Epoch: [90]  [120/126]  eta: 0:00:13  lr: 0.000025  loss: 0.2846 (0.2910)  time: 2.1344  data: 0.0004  max mem: 8053\n",
      "Epoch: [90]  [125/126]  eta: 0:00:02  lr: 0.000025  loss: 0.2900 (0.2912)  time: 2.1342  data: 0.0004  max mem: 8053\n",
      "Epoch: [90] Total time: 0:04:33 (2.1714 s / it)\n",
      "Averaged stats: lr: 0.000025  loss: 0.2900 (0.2912)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [91]  [  0/126]  eta: 0:12:54  lr: 0.000024  loss: 0.3032 (0.3032)  time: 6.1507  data: 3.8614  max mem: 8053\n",
      "Epoch: [91]  [ 20/126]  eta: 0:04:12  lr: 0.000024  loss: 0.2968 (0.2941)  time: 2.1955  data: 0.0004  max mem: 8053\n",
      "Epoch: [91]  [ 40/126]  eta: 0:03:14  lr: 0.000023  loss: 0.2869 (0.2911)  time: 2.1308  data: 0.0003  max mem: 8053\n",
      "Epoch: [91]  [ 60/126]  eta: 0:02:26  lr: 0.000022  loss: 0.2934 (0.2917)  time: 2.1384  data: 0.0002  max mem: 8053\n",
      "Epoch: [91]  [ 80/126]  eta: 0:01:41  lr: 0.000021  loss: 0.2913 (0.2919)  time: 2.1355  data: 0.0003  max mem: 8053\n",
      "Epoch: [91]  [100/126]  eta: 0:00:56  lr: 0.000020  loss: 0.2920 (0.2917)  time: 2.1353  data: 0.0003  max mem: 8053\n",
      "Epoch: [91]  [120/126]  eta: 0:00:13  lr: 0.000020  loss: 0.2929 (0.2916)  time: 2.1352  data: 0.0005  max mem: 8053\n",
      "Epoch: [91]  [125/126]  eta: 0:00:02  lr: 0.000019  loss: 0.2856 (0.2915)  time: 2.1325  data: 0.0004  max mem: 8053\n",
      "Epoch: [91] Total time: 0:04:34 (2.1775 s / it)\n",
      "Averaged stats: lr: 0.000019  loss: 0.2856 (0.2915)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [92]  [  0/126]  eta: 0:12:23  lr: 0.000019  loss: 0.3121 (0.3121)  time: 5.9025  data: 3.6272  max mem: 8053\n",
      "Epoch: [92]  [ 20/126]  eta: 0:04:07  lr: 0.000019  loss: 0.2944 (0.2957)  time: 2.1534  data: 0.0004  max mem: 8053\n",
      "Epoch: [92]  [ 40/126]  eta: 0:03:12  lr: 0.000018  loss: 0.2881 (0.2936)  time: 2.1325  data: 0.0003  max mem: 8053\n",
      "Epoch: [92]  [ 60/126]  eta: 0:02:25  lr: 0.000017  loss: 0.2935 (0.2934)  time: 2.1380  data: 0.0003  max mem: 8053\n",
      "Epoch: [92]  [ 80/126]  eta: 0:01:40  lr: 0.000016  loss: 0.2841 (0.2921)  time: 2.1482  data: 0.0003  max mem: 8053\n",
      "Epoch: [92]  [100/126]  eta: 0:00:56  lr: 0.000016  loss: 0.2891 (0.2916)  time: 2.1402  data: 0.0005  max mem: 8053\n",
      "Epoch: [92]  [120/126]  eta: 0:00:13  lr: 0.000015  loss: 0.2890 (0.2910)  time: 2.1375  data: 0.0004  max mem: 8053\n",
      "Epoch: [92]  [125/126]  eta: 0:00:02  lr: 0.000015  loss: 0.2944 (0.2910)  time: 2.1354  data: 0.0004  max mem: 8053\n",
      "Epoch: [92] Total time: 0:04:33 (2.1725 s / it)\n",
      "Averaged stats: lr: 0.000015  loss: 0.2944 (0.2910)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [93]  [  0/126]  eta: 0:14:09  lr: 0.000015  loss: 0.3108 (0.3108)  time: 6.7445  data: 4.4743  max mem: 8053\n",
      "Epoch: [93]  [ 20/126]  eta: 0:04:12  lr: 0.000014  loss: 0.2894 (0.2918)  time: 2.1640  data: 0.0004  max mem: 8053\n",
      "Epoch: [93]  [ 40/126]  eta: 0:03:14  lr: 0.000014  loss: 0.2932 (0.2922)  time: 2.1328  data: 0.0003  max mem: 8053\n",
      "Epoch: [93]  [ 60/126]  eta: 0:02:26  lr: 0.000013  loss: 0.2878 (0.2915)  time: 2.1417  data: 0.0002  max mem: 8053\n",
      "Epoch: [93]  [ 80/126]  eta: 0:01:41  lr: 0.000012  loss: 0.2875 (0.2910)  time: 2.1357  data: 0.0003  max mem: 8053\n",
      "Epoch: [93]  [100/126]  eta: 0:00:56  lr: 0.000012  loss: 0.2906 (0.2911)  time: 2.1392  data: 0.0003  max mem: 8053\n",
      "Epoch: [93]  [120/126]  eta: 0:00:13  lr: 0.000011  loss: 0.2934 (0.2915)  time: 2.1670  data: 0.0004  max mem: 8053\n",
      "Epoch: [93]  [125/126]  eta: 0:00:02  lr: 0.000011  loss: 0.2935 (0.2918)  time: 2.1664  data: 0.0003  max mem: 8053\n",
      "Epoch: [93] Total time: 0:04:35 (2.1838 s / it)\n",
      "Averaged stats: lr: 0.000011  loss: 0.2935 (0.2918)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [94]  [  0/126]  eta: 0:12:44  lr: 0.000011  loss: 0.3057 (0.3057)  time: 6.0707  data: 3.7553  max mem: 8053\n",
      "Epoch: [94]  [ 20/126]  eta: 0:04:08  lr: 0.000010  loss: 0.2936 (0.2963)  time: 2.1560  data: 0.0005  max mem: 8053\n",
      "Epoch: [94]  [ 40/126]  eta: 0:03:12  lr: 0.000010  loss: 0.2850 (0.2920)  time: 2.1315  data: 0.0003  max mem: 8053\n",
      "Epoch: [94]  [ 60/126]  eta: 0:02:25  lr: 0.000009  loss: 0.2935 (0.2920)  time: 2.1394  data: 0.0003  max mem: 8053\n",
      "Epoch: [94]  [ 80/126]  eta: 0:01:40  lr: 0.000009  loss: 0.2899 (0.2918)  time: 2.1389  data: 0.0004  max mem: 8053\n",
      "Epoch: [94]  [100/126]  eta: 0:00:56  lr: 0.000008  loss: 0.2867 (0.2910)  time: 2.1367  data: 0.0003  max mem: 8053\n",
      "Epoch: [94]  [120/126]  eta: 0:00:13  lr: 0.000008  loss: 0.2932 (0.2916)  time: 2.1397  data: 0.0004  max mem: 8053\n",
      "Epoch: [94]  [125/126]  eta: 0:00:02  lr: 0.000008  loss: 0.2932 (0.2917)  time: 2.1376  data: 0.0004  max mem: 8053\n",
      "Epoch: [94] Total time: 0:04:33 (2.1723 s / it)\n",
      "Averaged stats: lr: 0.000008  loss: 0.2932 (0.2917)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [95]  [  0/126]  eta: 0:14:11  lr: 0.000008  loss: 0.3088 (0.3088)  time: 6.7572  data: 4.3965  max mem: 8053\n",
      "Epoch: [95]  [ 20/126]  eta: 0:04:14  lr: 0.000007  loss: 0.2913 (0.2936)  time: 2.1851  data: 0.0004  max mem: 8053\n",
      "Epoch: [95]  [ 40/126]  eta: 0:03:15  lr: 0.000007  loss: 0.2879 (0.2911)  time: 2.1319  data: 0.0003  max mem: 8053\n",
      "Epoch: [95]  [ 60/126]  eta: 0:02:26  lr: 0.000006  loss: 0.2892 (0.2908)  time: 2.1366  data: 0.0003  max mem: 8053\n",
      "Epoch: [95]  [ 80/126]  eta: 0:01:41  lr: 0.000006  loss: 0.2938 (0.2920)  time: 2.1349  data: 0.0002  max mem: 8053\n",
      "Epoch: [95]  [100/126]  eta: 0:00:56  lr: 0.000005  loss: 0.2897 (0.2916)  time: 2.1382  data: 0.0003  max mem: 8053\n",
      "Epoch: [95]  [120/126]  eta: 0:00:13  lr: 0.000005  loss: 0.2910 (0.2910)  time: 2.1370  data: 0.0004  max mem: 8053\n",
      "Epoch: [95]  [125/126]  eta: 0:00:02  lr: 0.000005  loss: 0.2910 (0.2909)  time: 2.1349  data: 0.0004  max mem: 8053\n",
      "Epoch: [95] Total time: 0:04:34 (2.1813 s / it)\n",
      "Averaged stats: lr: 0.000005  loss: 0.2910 (0.2909)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [96]  [  0/126]  eta: 0:12:06  lr: 0.000005  loss: 0.3104 (0.3104)  time: 5.7675  data: 3.3853  max mem: 8053\n",
      "Epoch: [96]  [ 20/126]  eta: 0:04:08  lr: 0.000004  loss: 0.2938 (0.2937)  time: 2.1742  data: 0.0004  max mem: 8053\n",
      "Epoch: [96]  [ 40/126]  eta: 0:03:12  lr: 0.000004  loss: 0.2901 (0.2926)  time: 2.1357  data: 0.0003  max mem: 8053\n",
      "Epoch: [96]  [ 60/126]  eta: 0:02:25  lr: 0.000004  loss: 0.2848 (0.2905)  time: 2.1368  data: 0.0003  max mem: 8053\n",
      "Epoch: [96]  [ 80/126]  eta: 0:01:41  lr: 0.000003  loss: 0.2904 (0.2909)  time: 2.1743  data: 0.0003  max mem: 8053\n",
      "Epoch: [96]  [100/126]  eta: 0:00:56  lr: 0.000003  loss: 0.2892 (0.2911)  time: 2.1358  data: 0.0003  max mem: 8053\n",
      "Epoch: [96]  [120/126]  eta: 0:00:13  lr: 0.000003  loss: 0.2925 (0.2913)  time: 2.1366  data: 0.0003  max mem: 8053\n",
      "Epoch: [96]  [125/126]  eta: 0:00:02  lr: 0.000003  loss: 0.2925 (0.2912)  time: 2.1369  data: 0.0003  max mem: 8053\n",
      "Epoch: [96] Total time: 0:04:34 (2.1782 s / it)\n",
      "Averaged stats: lr: 0.000003  loss: 0.2925 (0.2912)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [97]  [  0/126]  eta: 0:13:38  lr: 0.000003  loss: 0.2777 (0.2777)  time: 6.4947  data: 4.2069  max mem: 8053\n",
      "Epoch: [97]  [ 20/126]  eta: 0:04:10  lr: 0.000002  loss: 0.2951 (0.2940)  time: 2.1528  data: 0.0003  max mem: 8053\n",
      "Epoch: [97]  [ 40/126]  eta: 0:03:13  lr: 0.000002  loss: 0.2881 (0.2916)  time: 2.1335  data: 0.0003  max mem: 8053\n",
      "Epoch: [97]  [ 60/126]  eta: 0:02:26  lr: 0.000002  loss: 0.2914 (0.2920)  time: 2.1446  data: 0.0003  max mem: 8053\n",
      "Epoch: [97]  [ 80/126]  eta: 0:01:41  lr: 0.000002  loss: 0.2873 (0.2920)  time: 2.1379  data: 0.0003  max mem: 8053\n",
      "Epoch: [97]  [100/126]  eta: 0:00:56  lr: 0.000001  loss: 0.2867 (0.2909)  time: 2.1375  data: 0.0003  max mem: 8053\n",
      "Epoch: [97]  [120/126]  eta: 0:00:13  lr: 0.000001  loss: 0.2890 (0.2909)  time: 2.1493  data: 0.0003  max mem: 8053\n",
      "Epoch: [97]  [125/126]  eta: 0:00:02  lr: 0.000001  loss: 0.2887 (0.2907)  time: 2.1359  data: 0.0003  max mem: 8053\n",
      "Epoch: [97] Total time: 0:04:34 (2.1780 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 0.2887 (0.2907)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [98]  [  0/126]  eta: 0:11:46  lr: 0.000001  loss: 0.2707 (0.2707)  time: 5.6109  data: 3.3541  max mem: 8053\n",
      "Epoch: [98]  [ 20/126]  eta: 0:04:05  lr: 0.000001  loss: 0.2888 (0.2922)  time: 2.1518  data: 0.0004  max mem: 8053\n",
      "Epoch: [98]  [ 40/126]  eta: 0:03:11  lr: 0.000001  loss: 0.2913 (0.2918)  time: 2.1351  data: 0.0002  max mem: 8053\n",
      "Epoch: [98]  [ 60/126]  eta: 0:02:25  lr: 0.000001  loss: 0.2936 (0.2914)  time: 2.1353  data: 0.0003  max mem: 8053\n",
      "Epoch: [98]  [ 80/126]  eta: 0:01:40  lr: 0.000001  loss: 0.2911 (0.2910)  time: 2.1408  data: 0.0003  max mem: 8053\n",
      "Epoch: [98]  [100/126]  eta: 0:00:56  lr: 0.000000  loss: 0.2848 (0.2903)  time: 2.1357  data: 0.0003  max mem: 8053\n",
      "Epoch: [98]  [120/126]  eta: 0:00:13  lr: 0.000000  loss: 0.2945 (0.2907)  time: 2.1393  data: 0.0004  max mem: 8053\n",
      "Epoch: [98]  [125/126]  eta: 0:00:02  lr: 0.000000  loss: 0.2945 (0.2908)  time: 2.1357  data: 0.0003  max mem: 8053\n",
      "Epoch: [98] Total time: 0:04:33 (2.1677 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.2945 (0.2908)\n",
      "log_dir: /kaggle/working/logs\n",
      "Epoch: [99]  [  0/126]  eta: 0:13:11  lr: 0.000000  loss: 0.2543 (0.2543)  time: 6.2810  data: 3.8214  max mem: 8053\n",
      "Epoch: [99]  [ 20/126]  eta: 0:04:13  lr: 0.000000  loss: 0.2859 (0.2877)  time: 2.1935  data: 0.0003  max mem: 8053\n",
      "Epoch: [99]  [ 40/126]  eta: 0:03:14  lr: 0.000000  loss: 0.2901 (0.2887)  time: 2.1337  data: 0.0003  max mem: 8053\n",
      "Epoch: [99]  [ 60/126]  eta: 0:02:26  lr: 0.000000  loss: 0.2868 (0.2891)  time: 2.1408  data: 0.0003  max mem: 8053\n",
      "Epoch: [99]  [ 80/126]  eta: 0:01:41  lr: 0.000000  loss: 0.2882 (0.2891)  time: 2.1367  data: 0.0003  max mem: 8053\n",
      "Epoch: [99]  [100/126]  eta: 0:00:56  lr: 0.000000  loss: 0.2937 (0.2900)  time: 2.1389  data: 0.0003  max mem: 8053\n",
      "Epoch: [99]  [120/126]  eta: 0:00:13  lr: 0.000000  loss: 0.2871 (0.2898)  time: 2.1358  data: 0.0003  max mem: 8053\n",
      "Epoch: [99]  [125/126]  eta: 0:00:02  lr: 0.000000  loss: 0.2878 (0.2900)  time: 2.1336  data: 0.0003  max mem: 8053\n",
      "Epoch: [99] Total time: 0:04:34 (2.1800 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.2878 (0.2900)\n",
      "Training time 2:12:44\n"
     ]
    }
   ],
   "source": [
    "# main_pretrain.py\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import timm\n",
    "\n",
    "# assert timm.__version__ == \"0.3.2\"  # version check\n",
    "from timm.optim import param_groups_weight_decay  \n",
    "\n",
    "# import util.misc as misc\n",
    "# from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "# import models_mae\n",
    "\n",
    "# from engine_pretrain import train_one_epoch\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=256, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--epochs', default=100, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='mae_vit_base_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "    parser.add_argument('--mask_ratio', default=0.75, type=float,\n",
    "                        help='Masking ratio (percentage of removed patches).')\n",
    "\n",
    "    parser.add_argument('--norm_pix_loss', action='store_true',\n",
    "                        help='Use (per-patch) normalized pixels as targets for computing loss')\n",
    "    parser.set_defaults(norm_pix_loss=False)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--blr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=10, metavar='N', # 10 same as SimCLR\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', default='/kaggle/input/ssl-dataset/ssl_dataset/train.X1', type=str,\n",
    "                        help='dataset path')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='/kaggle/working/checkpoints',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default='/kaggle/working/logs',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='/kaggle/input/mae-vit-base/checkpoints/checkpoint-70.pth',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--num_workers', default=4, type=int) # Kaggle Recommendation\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print(\"Running with args:\\n\", json.dumps(vars(args), indent=2))\n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    dataset_train = datasets.ImageFolder(os.path.join(args.data_path), transform=transform_train)\n",
    "    print(dataset_train)\n",
    "\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "\n",
    "    log_writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir else None\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    # define the model\n",
    "    model = available_models[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using DataParallel with {torch.cuda.device_count()} GPUs.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    print(\"Model = %s\" % str(model))\n",
    "\n",
    "    eff_batch_size = args.batch_size * args.accum_iter * get_world_size()\n",
    "    \n",
    "    if args.lr is None:  # only base_lr is specified\n",
    "        args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "    print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "    print(\"actual lr: %.2e\" % args.lr)\n",
    "    print(\"effective batch size: %d\" % eff_batch_size)\n",
    "    \n",
    "    param_groups = param_groups_weight_decay(model, args.weight_decay)\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))\n",
    "    print(optimizer)\n",
    "    loss_scaler = NativeScalerWithGradNormCount()\n",
    "\n",
    "    load_model(args=args, model=model, optimizer=optimizer, loss_scaler=loss_scaler)\n",
    "\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "        train_stats = train_one_epoch(\n",
    "            model, data_loader_train,\n",
    "            optimizer, device, epoch, loss_scaler,\n",
    "            log_writer=log_writer,\n",
    "            args=args\n",
    "        )\n",
    "        if args.output_dir and (epoch % 5 == 0 or epoch + 1 == args.epochs):\n",
    "            save_model(\n",
    "                args=args, model=model, optimizer=optimizer,\n",
    "                loss_scaler=loss_scaler, epoch=epoch)\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        'epoch': epoch,}\n",
    "\n",
    "        if args.output_dir and is_main_process():\n",
    "            if log_writer is not None:\n",
    "                log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_args_parser()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d88d9",
   "metadata": {
    "papermill": {
     "duration": 0.012985,
     "end_time": "2025-05-28T16:05:02.324628",
     "exception": false,
     "start_time": "2025-05-28T16:05:02.311643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7502292,
     "sourceId": 11932960,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242301588,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8100.297926,
   "end_time": "2025-05-28T16:05:06.284605",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T13:50:05.986679",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
