{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93534f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-29T04:26:23.759844Z",
     "iopub.status.busy": "2025-05-29T04:26:23.759586Z",
     "iopub.status.idle": "2025-05-29T04:26:31.248601Z",
     "shell.execute_reply": "2025-05-29T04:26:31.247979Z"
    },
    "papermill": {
     "duration": 7.494339,
     "end_time": "2025-05-29T04:26:31.250054",
     "exception": false,
     "start_time": "2025-05-29T04:26:23.755715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for clearing memory\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816f36fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:26:31.256330Z",
     "iopub.status.busy": "2025-05-29T04:26:31.255675Z",
     "iopub.status.idle": "2025-05-29T04:26:31.273791Z",
     "shell.execute_reply": "2025-05-29T04:26:31.272997Z"
    },
    "papermill": {
     "duration": 0.022007,
     "end_time": "2025-05-29T04:26:31.274907",
     "exception": false,
     "start_time": "2025-05-29T04:26:31.252900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mae-vit-base-from-70/checkpoints/checkpoint-75.pth\n",
      "/kaggle/input/mae-vit-base-from-70/checkpoints/checkpoint-80.pth\n",
      "/kaggle/input/mae-vit-base-from-70/checkpoints/checkpoint-85.pth\n",
      "/kaggle/input/mae-vit-base-from-70/checkpoints/checkpoint-90.pth\n",
      "/kaggle/input/mae-vit-base-from-70/checkpoints/checkpoint-95.pth\n",
      "/kaggle/input/mae-vit-base-from-70/checkpoints/checkpoint-99.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIRECTORY = [\"/kaggle/input/mae-vit-base-from-70/checkpoints\"]\n",
    "\n",
    "checkpoint_paths_list = []\n",
    "for dirs in MODEL_DIRECTORY:\n",
    "    checkpoint_paths_list += [os.path.join(dirs, f) for f in os.listdir(dirs) if f.endswith('.pth')]\n",
    "\n",
    "checkpoint_paths_list = sorted(checkpoint_paths_list, key=lambda x: ((\"0\"+os.path.basename(x).split(\"-\")[-1].split(\".\")[0])[-2:]))\n",
    "\n",
    "for path in checkpoint_paths_list:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59469740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:26:31.280010Z",
     "iopub.status.busy": "2025-05-29T04:26:31.279641Z",
     "iopub.status.idle": "2025-05-29T04:27:03.559401Z",
     "shell.execute_reply": "2025-05-29T04:27:03.558655Z"
    },
    "papermill": {
     "duration": 32.285135,
     "end_time": "2025-05-29T04:27:03.562207",
     "exception": false,
     "start_time": "2025-05-29T04:26:31.277072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 04:26:34.905379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748492795.327781      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748492795.443704      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "# \n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f529392e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:27:03.568757Z",
     "iopub.status.busy": "2025-05-29T04:27:03.567834Z",
     "iopub.status.idle": "2025-05-29T04:32:06.921699Z",
     "shell.execute_reply": "2025-05-29T04:32:06.920916Z"
    },
    "papermill": {
     "duration": 303.360572,
     "end_time": "2025-05-29T04:32:06.925343",
     "exception": false,
     "start_time": "2025-05-29T04:27:03.564771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [05:03<00:00, 75.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = \"/kaggle/input/ssl-dataset/ssl_dataset\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "all_class_names = set()\n",
    "for i in range(1, 5):\n",
    "    split_path = os.path.join(base_path, f\"train.X{i}\")\n",
    "    all_class_names.update(os.listdir(split_path))\n",
    "\n",
    "all_class_names = sorted(all_class_names)\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(all_class_names)}\n",
    "\n",
    "train_datasets = []\n",
    "for i in tqdm(range(1, 5)):\n",
    "    folder = f\"train.X{i}\"\n",
    "    split_path = os.path.join(base_path, folder)\n",
    "    dataset = datasets.ImageFolder(split_path, transform=transform)\n",
    "    dataset.class_to_idx = class_to_idx\n",
    "    dataset.samples = [(path, class_to_idx[os.path.basename(os.path.dirname(path))]) \n",
    "                       for path, _ in dataset.samples]\n",
    "    train_datasets.append(dataset)\n",
    "\n",
    "full_train_dataset = ConcatDataset(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c48c8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:32:06.931364Z",
     "iopub.status.busy": "2025-05-29T04:32:06.931139Z",
     "iopub.status.idle": "2025-05-29T04:32:06.940712Z",
     "shell.execute_reply": "2025-05-29T04:32:06.940222Z"
    },
    "papermill": {
     "duration": 0.013868,
     "end_time": "2025-05-29T04:32:06.941800",
     "exception": false,
     "start_time": "2025-05-29T04:32:06.927932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dae3ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:32:06.948052Z",
     "iopub.status.busy": "2025-05-29T04:32:06.947819Z",
     "iopub.status.idle": "2025-05-29T04:32:12.096955Z",
     "shell.execute_reply": "2025-05-29T04:32:12.096202Z"
    },
    "papermill": {
     "duration": 5.153844,
     "end_time": "2025-05-29T04:32:12.098440",
     "exception": false,
     "start_time": "2025-05-29T04:32:06.944596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAE Complete Model\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "# from util.pos_embed import get_2d_sincos_pos_embed\n",
    "\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss # Adding mean to balance out DataParallel\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "\n",
    "def mae_vit_small_patch16_dec512d8b(**kwargs):  # Main Test Done in this\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6,  # corrected for small model\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# set recommended archs\n",
    "\n",
    "available_models = {\n",
    "    'mae_vit_small_patch16' : mae_vit_small_patch16_dec512d8b, # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_base_patch16' : mae_vit_base_patch16_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_large_patch16' : mae_vit_large_patch16_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "    'mae_vit_huge_patch14' : mae_vit_huge_patch14_dec512d8b,  # decoder: 512 dim, 8 blocks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4312f953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:32:12.105494Z",
     "iopub.status.busy": "2025-05-29T04:32:12.105274Z",
     "iopub.status.idle": "2025-05-29T04:32:12.114032Z",
     "shell.execute_reply": "2025-05-29T04:32:12.113205Z"
    },
    "papermill": {
     "duration": 0.013873,
     "end_time": "2025-05-29T04:32:12.115231",
     "exception": false,
     "start_time": "2025-05-29T04:32:12.101358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder Model\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import timm.models.vision_transformer\n",
    "\n",
    "\n",
    "class VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n",
    "    \"\"\" Vision Transformer with support for global average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, global_pool=False, **kwargs):\n",
    "        super(VisionTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "        if self.global_pool:\n",
    "            norm_layer = kwargs['norm_layer']\n",
    "            embed_dim = kwargs['embed_dim']\n",
    "            self.fc_norm = norm_layer(embed_dim)\n",
    "\n",
    "            del self.norm  # remove the original norm\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.global_pool:\n",
    "            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n",
    "            outcome = self.fc_norm(x)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "            outcome = x[:, 0]\n",
    "\n",
    "        return outcome\n",
    "\n",
    "\n",
    "def vit_small_patch16(**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base_patch16(**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large_patch16(**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge_patch14(**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "available_models_vit = {\n",
    "    \"vit_small_patch16\": vit_small_patch16,\n",
    "    \"vit_base_patch16\": vit_base_patch16,\n",
    "    \"vit_large_patch16\": vit_large_patch16,\n",
    "    \"vit_huge_patch14\": vit_huge_patch14,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac45269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:32:12.120829Z",
     "iopub.status.busy": "2025-05-29T04:32:12.120645Z",
     "iopub.status.idle": "2025-05-29T05:50:39.346076Z",
     "shell.execute_reply": "2025-05-29T05:50:39.345098Z"
    },
    "papermill": {
     "duration": 4707.353031,
     "end_time": "2025-05-29T05:50:39.470650",
     "exception": false,
     "start_time": "2025-05-29T04:32:12.117619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [12:26<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/train_data_checkpoint_76.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [12:30<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/train_data_checkpoint_81.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [12:36<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/train_data_checkpoint_86.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [12:32<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/train_data_checkpoint_91.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [12:30<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/train_data_checkpoint_96.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [12:30<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/train_data_checkpoint_100.npz\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_path in checkpoint_paths_list:\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    checkpoint_num = checkpoint['epoch']\n",
    "    train_loader = DataLoader(full_train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "    # ====Loading model from memory and converting to encoder====\n",
    "    encoder = available_models_vit['vit_base_patch16'](\n",
    "            num_classes=100,\n",
    "            global_pool=False,\n",
    "        )\n",
    "    \n",
    "    model = available_models['mae_vit_base_patch16'](norm_pix_loss=False)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    loaded_state_dict = model.module.state_dict()\n",
    "    encoder_state_dict = encoder.state_dict()\n",
    "    for k in ['head.weight', 'head.bias']:\n",
    "        if k in loaded_state_dict and loaded_state_dict[k].shape != encoder_state_dict[k].shape:\n",
    "            print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "            del loaded_state_dict[k]\n",
    "    \n",
    "    interpolate_pos_embed(encoder, loaded_state_dict)\n",
    "    msg = encoder.load_state_dict(loaded_state_dict, strict=False)\n",
    "    assert set(msg.missing_keys) == {'head.weight', 'head.bias'}\n",
    "    \n",
    "    interpolate_pos_embed(encoder, checkpoint)\n",
    "    encoder.head = nn.Identity()\n",
    "    \n",
    "    encoder.to(DEVICE)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        encoder = nn.DataParallel(encoder)\n",
    "    #=============================================================\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            features = encoder(inputs)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    features_array = np.concatenate(all_features, axis=0)\n",
    "    labels_array = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    os.makedirs('/kaggle/working/extracted_features', exist_ok=True)\n",
    "    np.savez_compressed(f'/kaggle/working/extracted_features/train_data_checkpoint_{checkpoint_num+1}.npz', features=features_array, labels=labels_array)\n",
    "    \n",
    "    print(f\"Saved features and labels together in /kaggle/working/train_data_checkpoint_{checkpoint_num+1}.npz\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7502292,
     "sourceId": 11932960,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242301588,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242369845,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5065.937177,
   "end_time": "2025-05-29T05:50:43.693432",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-29T04:26:17.756255",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
