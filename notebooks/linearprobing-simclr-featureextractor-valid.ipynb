{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c2f83f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:20.795051Z",
     "iopub.status.busy": "2025-05-29T05:29:20.794832Z",
     "iopub.status.idle": "2025-05-29T05:29:20.820427Z",
     "shell.execute_reply": "2025-05-29T05:29:20.819646Z"
    },
    "papermill": {
     "duration": 0.030192,
     "end_time": "2025-05-29T05:29:20.821720",
     "exception": false,
     "start_time": "2025-05-29T05:29:20.791528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0005.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0010.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0015.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0020.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0025.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0030.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0035.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0040.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0045.pth.tar\n",
      "/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/checkpoint_epoch_0050.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0055.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0060.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0065.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0070.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0075.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0080.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0085.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0090.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0095.pth.tar\n",
      "/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/checkpoint_epoch_0100.pth.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIRECTORY_1 = \"/kaggle/input/simclr-model/runs/May26_22-27-46_ca41811d77bd/\"\n",
    "MODEL_DIRECTORY_2 = \"/kaggle/input/simclr-model-100/runs/May27_10-38-58_a9089d95001b/\"\n",
    "\n",
    "checkpoint_files = []\n",
    "for ckpt_path in (MODEL_DIRECTORY_1, MODEL_DIRECTORY_2):\n",
    "    checkpoint_files += [(f,ckpt_path) for f in os.listdir(ckpt_path) if f.endswith('.pth.tar')]\n",
    "\n",
    "checkpoint_paths_list = [os.path.join(ckpt_path, f) for f,ckpt_path in checkpoint_files]\n",
    "checkpoint_paths_list = sorted(checkpoint_paths_list, key=lambda x: os.path.basename(x))\n",
    "\n",
    "for path in checkpoint_paths_list:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb7a476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:20.826812Z",
     "iopub.status.busy": "2025-05-29T05:29:20.826631Z",
     "iopub.status.idle": "2025-05-29T05:29:25.082602Z",
     "shell.execute_reply": "2025-05-29T05:29:25.081870Z"
    },
    "papermill": {
     "duration": 4.259977,
     "end_time": "2025-05-29T05:29:25.084092",
     "exception": false,
     "start_time": "2025-05-29T05:29:20.824115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for clearing memory\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f365cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:25.089631Z",
     "iopub.status.busy": "2025-05-29T05:29:25.089302Z",
     "iopub.status.idle": "2025-05-29T05:29:46.453014Z",
     "shell.execute_reply": "2025-05-29T05:29:46.452055Z"
    },
    "papermill": {
     "duration": 21.367866,
     "end_time": "2025-05-29T05:29:46.454342",
     "exception": false,
     "start_time": "2025-05-29T05:29:25.086476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 05:29:27.047608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748496567.242200      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748496567.297580      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "# \n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3b0a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:46.461023Z",
     "iopub.status.busy": "2025-05-29T05:29:46.459958Z",
     "iopub.status.idle": "2025-05-29T05:29:51.270729Z",
     "shell.execute_reply": "2025-05-29T05:29:51.270110Z"
    },
    "papermill": {
     "duration": 4.815189,
     "end_time": "2025-05-29T05:29:51.272057",
     "exception": false,
     "start_time": "2025-05-29T05:29:46.456868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = \"/kaggle/input/ssl-dataset/ssl_dataset\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "all_class_names = set()\n",
    "for i in range(1, 5):\n",
    "    split_path = os.path.join(base_path, f\"train.X{i}\")\n",
    "    all_class_names.update(os.listdir(split_path))\n",
    "\n",
    "all_class_names = sorted(all_class_names)\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(all_class_names)}\n",
    "\n",
    "\n",
    "# train_datasets = []\n",
    "# for i in tqdm(range(1, 5)):\n",
    "#     folder = f\"train.X{i}\"\n",
    "#     split_path = os.path.join(base_path, folder)\n",
    "#     dataset = datasets.ImageFolder(split_path, transform=transform)\n",
    "#     dataset.class_to_idx = class_to_idx\n",
    "#     dataset.samples = [(path, class_to_idx[os.path.basename(os.path.dirname(path))]) \n",
    "#                        for path, _ in dataset.samples]\n",
    "#     train_datasets.append(dataset)\n",
    "\n",
    "# full_train_dataset = ConcatDataset(train_datasets)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\"/kaggle/input/ssl-dataset/ssl_dataset/val.X\", transform=transform)\n",
    "val_dataset.class_to_idx = class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a32c3c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:51.277645Z",
     "iopub.status.busy": "2025-05-29T05:29:51.277409Z",
     "iopub.status.idle": "2025-05-29T05:29:51.282036Z",
     "shell.execute_reply": "2025-05-29T05:29:51.281416Z"
    },
    "papermill": {
     "duration": 0.00859,
     "end_time": "2025-05-29T05:29:51.283101",
     "exception": false,
     "start_time": "2025-05-29T05:29:51.274511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved class_to_idx to /kaggle/working/class_to_idx.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_path = '/kaggle/working/class_to_idx.json'\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(class_to_idx, f)\n",
    "\n",
    "print(f\"Saved class_to_idx to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7903e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:51.290305Z",
     "iopub.status.busy": "2025-05-29T05:29:51.289885Z",
     "iopub.status.idle": "2025-05-29T05:29:51.293114Z",
     "shell.execute_reply": "2025-05-29T05:29:51.292642Z"
    },
    "papermill": {
     "duration": 0.006798,
     "end_time": "2025-05-29T05:29:51.294164",
     "exception": false,
     "start_time": "2025-05-29T05:29:51.287366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exceptions/exceptions.py\n",
    "\n",
    "class BaseSimCLRException(Exception):\n",
    "    \"\"\"Base exception\"\"\"\n",
    "\n",
    "\n",
    "class InvalidBackboneError(BaseSimCLRException):\n",
    "    \"\"\"Raised when the choice of backbone Convnet is invalid.\"\"\"\n",
    "\n",
    "\n",
    "class InvalidDatasetSelection(BaseSimCLRException):\n",
    "    \"\"\"Raised when the choice of dataset is invalid.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b83c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:51.299534Z",
     "iopub.status.busy": "2025-05-29T05:29:51.299019Z",
     "iopub.status.idle": "2025-05-29T05:29:51.304216Z",
     "shell.execute_reply": "2025-05-29T05:29:51.303714Z"
    },
    "papermill": {
     "duration": 0.008727,
     "end_time": "2025-05-29T05:29:51.305140",
     "exception": false,
     "start_time": "2025-05-29T05:29:51.296413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNetSimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        self.resnet_dict = {\"resnet18\": models.resnet18(pretrained=False, num_classes=out_dim),\n",
    "                            \"resnet50\": models.resnet50(pretrained=False, num_classes=out_dim)}\n",
    "\n",
    "        self.backbone = self._get_basemodel(base_model)\n",
    "        dim_mlp = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.backbone.fc)\n",
    "\n",
    "    def _get_basemodel(self, model_name = 'resnet50'):\n",
    "        try:\n",
    "            model = self.resnet_dict[model_name]\n",
    "        except KeyError:\n",
    "            raise InvalidBackboneError(\n",
    "                \"Invalid backbone architecture. Check the config file and pass one of: resnet18 or resnet50\")\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2290c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:29:51.309979Z",
     "iopub.status.busy": "2025-05-29T05:29:51.309733Z",
     "iopub.status.idle": "2025-05-29T05:37:43.164826Z",
     "shell.execute_reply": "2025-05-29T05:37:43.163924Z"
    },
    "papermill": {
     "duration": 471.859211,
     "end_time": "2025-05-29T05:37:43.166368",
     "exception": false,
     "start_time": "2025-05-29T05:29:51.307157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:19<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_005.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_010.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_015.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_020.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_025.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_030.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_035.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_040.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_045.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_050.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_055.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_060.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_065.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_070.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_075.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_080.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_085.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_090.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_095.npz\n",
      "Using 2 GPUs.\n",
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels together in /kaggle/working/val_data_checkpoint_100.npz\n"
     ]
    }
   ],
   "source": [
    "# Loading model and converting it to feature extractor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "for checkpoint_path in checkpoint_paths_list:\n",
    "    file_name = os.path.basename(checkpoint_path)\n",
    "    checkpoint_num = os.path.splitext(os.path.splitext(file_name)[0])[0][-3:]\n",
    "\n",
    "    # train_loader = DataLoader(full_train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "    model = ResNetSimCLR(base_model=\"resnet50\", out_dim=128)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
    "            model = torch.nn.DataParallel(model) # Since model was stored as data parallel\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    num_classes = len(all_class_names)\n",
    "    feature_dim = model.module.backbone.fc[0].in_features\n",
    "    encoder = model.module.backbone\n",
    "    encoder.fc = nn.Identity()\n",
    "    encoder.to(DEVICE)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
    "            encoder = torch.nn.DataParallel(encoder)\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            features = encoder(inputs)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    features_array = np.concatenate(all_features, axis=0)\n",
    "    labels_array = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    os.makedirs('/kaggle/working/extracted_features', exist_ok=True)\n",
    "    np.savez_compressed(f'/kaggle/working/extracted_features/val_data_checkpoint_{checkpoint_num}.npz', features=features_array, labels=labels_array)\n",
    "    \n",
    "    print(f\"Saved features and labels together in /kaggle/working/val_data_checkpoint_{checkpoint_num}.npz\")\n",
    "\n",
    "    del model\n",
    "    del encoder\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b08dc",
   "metadata": {
    "papermill": {
     "duration": 0.020296,
     "end_time": "2025-05-29T05:37:43.208339",
     "exception": false,
     "start_time": "2025-05-29T05:37:43.188043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e00629",
   "metadata": {
    "papermill": {
     "duration": 0.020205,
     "end_time": "2025-05-29T05:37:43.248702",
     "exception": false,
     "start_time": "2025-05-29T05:37:43.228497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7502292,
     "sourceId": 11932960,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242040653,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242137132,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 509.71568,
   "end_time": "2025-05-29T05:37:46.316448",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-29T05:29:16.600768",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
